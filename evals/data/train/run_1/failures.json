[
  {
    "what_went_wrong": "Flagged the wrong secondary category: it labeled the message as \"Passive-Aggressive\" instead of the expected \"Vagueness\" (while correctly flagging \"Rudeness\").",
    "why": "The model over-indexed on tone interpretation and treated \"you\u2019ll get an update when there\u2019s something worth sharing\" as a veiled jab (passive-aggression). In this scenario, the more salient policy-relevant issue is that the CTO provides no actionable incident-communication content (no impact scope, no next update ETA, no DRI/process alignment, no guidance on change freeze/customer comms), which is exactly what the anxious, process-heavy org and #general audience are requesting. The model effectively ignored the ITIL-style context and the explicit asks in the thread (timeline, data-risk statement, DRI, where updates land, change freeze), so it missed that the primary communication failure beyond rudeness is non-informative/withholding detail\u2014i.e., vagueness.",
    "failure_pattern": "context-not-considered",
    "severity": "major",
    "id": 3,
    "message": "Everyone just calm down and stop speculating. We\u2019ll handle it and you\u2019ll get an update when there\u2019s something worth sharing.",
    "ground_truth_flags": [
      "Vagueness",
      "Rudeness"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Missed the \"Microaggressive\" flag (under-flagging); it only predicted Passive-Aggressive though the message also contains a targeted, identity/status-linked slight about being remote.",
    "why": "The model correctly caught the indirect hostility/dismissiveness, but treated the second sentence as merely a backhanded remark rather than a microaggression. The line \"If remote is making it hard to keep up... we'll loop in someone local next time\" subtly stereotypes/devalues remote workers as less capable/less legitimate contributors and threatens exclusion from future work based on location. That exclusionary implication is amplified by the channel context: Maya is remote, was already excluded from an in-office whiteboard session, and is asking for accessibility details (a legitimate quality/risk concern). The model likely over-weighted surface tone cues for passive-aggression and under-weighted the contextual power dynamic and the 'remote vs local' marginalization signal that triggers the microaggression category.",
    "failure_pattern": "missed-microaggression-against-remote-status",
    "severity": "major",
    "id": 4,
    "message": "Maya, we don\u2019t have time to re-litigate the basics \u2014 just take what\u2019s in the Figma and make it look clean. If remote is making it hard to keep up with the quick decisions, just say so and we\u2019ll loop in someone local next time.",
    "ground_truth_flags": [
      "Microaggressive",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed the expected flags (Dismissive, Rudeness) and instead applied an incorrect/less-specific flag (Passive-Aggressive).",
    "why": "The target message is not primarily indirect or veiled; it contains explicit scolding and gatekeeping language (\u201cplease stop dropping urgent sales stuff in here\u201d) plus a deprioritizing, brush-off closer (\u201cwe\u2019ll look when we can\u201d). That maps more cleanly to Rudeness (hostile/impolite phrasing) and Dismissive (minimizing the urgency and signaling low priority) rather than Passive-Aggressive (which typically involves indirect digs or sarcasm). The model appears to overweight a single feature\u2014frustration + curt instruction\u2014and classify it as passive-aggression without checking whether the frustration is expressed indirectly. It also underused channel context: in #engineering, with an active incident/postmortem and a stressed on-call engineer, the wording functions as a public reprimand that can be perceived as rude/dismissive even if the procedural ask (use Jira) is legitimate.",
    "failure_pattern": "mislabeling-rude-dismissive-as-passive-aggressive",
    "severity": "major",
    "id": 5,
    "message": "We\u2019re in a postmortem + code freeze \u2014 please stop dropping urgent sales stuff in here. File a ticket with repro + links like everyone else and we\u2019ll look when we can.",
    "ground_truth_flags": [
      "Dismissive",
      "Rudeness"
    ],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "The model missed the expected \"One-Liner\" flag and instead added an extra \"Passive-Aggressive\" flag; it correctly flagged \"Rudeness\" but mischaracterized the tone category.",
    "why": "The message is overtly blunt and dismissive (\"stop overthinking it,\" \"isn\u2019t that complicated\"), which fits rudeness/dismissiveness and also clearly matches a brevity/low-substance \"One-Liner\" in a high-stakes, SOP-driven channel. The model likely prioritized interpreting interpersonal intent (reading it as indirect/veiled reprimand = passive-aggressive) rather than the surface linguistic features and policy taxonomy: (a) it\u2019s not actually passive-aggressive because the reprimand is direct, not oblique; (b) it ignored or underweighted the channel norm requiring sourced, ticket-tied, structured updates\u2014making this short scolding particularly \u201cone-liner\u201d-like (no actionable guidance beyond \u201cfollow the template\u201d).",
    "failure_pattern": "one-liner-missed-and-tone-misclassified-as-passive-aggressive",
    "severity": "major",
    "id": 6,
    "message": "Just follow the template and stop overthinking it\u2014this isn\u2019t that complicated. We don\u2019t need a bunch of questions in here right now.",
    "ground_truth_flags": [
      "Dismissive",
      "One-Liner",
      "Rudeness"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Missed a required flag: it correctly flagged Passive-Aggressive but failed to also flag Sarcastic, even though the target message is explicitly sarcastic (mock praise, ironic framing, emoji, and scare quotes).",
    "why": "The model appears to have treated sarcasm as merely an explanatory feature/justification for Passive-Aggressive rather than an independently flaggable category. It likely collapsed overlapping categories (sarcasm \u2192 passive-aggression) instead of outputting both, despite clear sarcasm markers ('Totally fine\u2014who needs...', 'fun reason to hover \ud83d\ude43', '\u201cfigure it out\u201d later'). This is less about subtlety and more about a taxonomy/label-selection error: the model recognized sarcasm in its reasoning but didn\u2019t map it to the separate 'Sarcastic' flag in the final prediction.",
    "failure_pattern": "sarcasm-collapsed-into-passive-aggressive",
    "severity": "major",
    "id": 8,
    "message": "Totally fine\u2014who needs the handoff checklist when we can just give Risk another fun reason to hover \ud83d\ude43. Keep pushing that board KPI deck and we\u2019ll just \u201cfigure it out\u201d later, I guess.",
    "ground_truth_flags": [
      "Sarcastic",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Both: it missed the expected Rudeness flag and instead added an incorrect Fake flag.",
    "why": "The message is overtly sarcastic and dismissive (\u201cdrop the active incident,\u201d \u201crewrite checkout real quick,\u201d eye-roll emoji), which in this context functions as ridicule toward Maya\u2019s escalation and the situation\u2014i.e., rudeness in addition to passive-aggression. The model correctly caught passive-aggression but mis-mapped the sarcastic \u2018Totally\u2019 to a \u201cFake/insincere\u201d category. Here, the intent isn\u2019t deceptive or persona-faking; it\u2019s snarky pushback. By over-indexing on linguistic markers of insincere agreement, the model treated sarcasm as \u201cFake\u201d rather than recognizing it as direct derision/condescension (rudeness) aimed at another coworker in a tense, high-visibility thread.",
    "failure_pattern": "sarcasm-misclassified-as-fake",
    "severity": "major",
    "id": 9,
    "message": "Totally, let\u2019s just drop the active incident and rewrite checkout real quick because a Slack Connect rumor says so. Next time we\u2019ll pin \u201cprocess\u201d in #engineering and hope it fixes prod too \ud83d\ude43",
    "ground_truth_flags": [
      "Rudeness",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Fake",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: the model added an extra \"Passive-Aggressive\" flag when the expected (and primary) issue is straightforward rudeness/hostility.",
    "why": "The message is overtly confrontational and demeaning (\"stop overthinking,\" \"I don\u2019t have time for a debate,\" \"if you can\u2019t handle it\u2026\")\u2014that\u2019s direct hostility rather than indirect, veiled, or sarcastic signaling typical of passive-aggression. The model appears to treat any shaming/pressure phrasing as passive-aggressive, even when it is explicit and on-the-nose. Context (high-stakes client incident) explains urgency but doesn\u2019t convert the tone into passive-aggression; it remains blunt rudeness. This is a taxonomy boundary error: conflating coercive/direct insult with passive-aggressive behavior.",
    "failure_pattern": "rudeness-passive-aggression-conflation",
    "severity": "minor",
    "id": 10,
    "message": "Jordan, stop overthinking this and just ship the patch in the next hour. I don\u2019t have time for a debate \u2014 if you can\u2019t handle it, say so and I\u2019ll find someone who can.",
    "ground_truth_flags": [
      "Rudeness"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: added a \"Non-Objective\" flag when only \"Circular\" was expected.",
    "why": "The model treated rhetorical, motivational team-language (\u201cif we don\u2019t have clear ownership, we can\u2019t ship\u2026\u201d) as an evidence-based factual claim that must be substantiated, and then escalated it to \"Non-Objective.\" In this Slack engineering context\u2014fast, blunt, high-stakes incident coordination\u2014the message is primarily a repetitive framing about accountability, not a disputable factual report. While it is absolutist (\u201cthe only issue\u201d), that reads as emphasis/hyperbole rather than an attempt to assert an empirically verifiable root-cause conclusion. The real, clear violation is the circularity/redundant restatement without actionable assignment.",
    "failure_pattern": "overflagging-rhetorical-absolutism-as-non-objective",
    "severity": "minor",
    "id": 11,
    "message": "We just need clearer ownership here\u2014if we don\u2019t have clear ownership, we can\u2019t ship, and if we can\u2019t ship it\u2019s because ownership wasn\u2019t clear. So let\u2019s make ownership clear by being clear about who owns what, because the only issue is unclear ownership.",
    "ground_truth_flags": [
      "Circular"
    ],
    "predicted_flags": [
      "Non-Objective",
      "Circular"
    ]
  },
  {
    "what_went_wrong": "Missed the expected Rudeness flag and instead flagged Vagueness and Non-Objective, which are secondary at most; the primary issue is the message\u2019s accusatory, imperative, blame-forward tone in a high-visibility #general thread.",
    "why": "The model over-weighted informational/actionability criteria (lack of details, evidentiary hedging) and treated the message as a quality-of-report problem rather than a tone/politeness problem. It failed to incorporate channel and social context: #general is public, leadership is watching, and there\u2019s a known sensitivity to public blame. Phrases like \u201cyour refactor broke\u2026\u201d, \u201cyou need to fix it right now\u201d, and \u201cthis can\u2019t ship like this\u201d are directly attributive and commanding, which reads as confrontational/rude regardless of whether more technical detail could be added. The \u2018Non-Objective\u2019 rationale also misfires because in context Maya already provided repro steps and an observed regression; the target message\u2019s main violation is interpersonal blame, not epistemic uncertainty.",
    "failure_pattern": "ignoring-channel-context-and-missing-rudeness",
    "severity": "major",
    "id": 12,
    "message": "Jordan, your refactor broke iOS checkout. We need to roll it back or you need to fix it right now\u2014this can\u2019t ship like this with the enterprise demo tomorrow.",
    "ground_truth_flags": [
      "Rudeness"
    ],
    "predicted_flags": [
      "Vagueness",
      "Non-Objective"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: it added a \"Non-Objective\" flag even though the expected/appropriate issue was only \"Circular.\"",
    "why": "The model treated a safety judgment (\"it\u2019s not safe\") as an unsupported assertion requiring evidence, but in this engineering/prerelease context it functions as a normative release gate tied directly to explicit criteria stated in the same message (tests + rollback plan). The message is indeed repetitive/circular, but it is still objective/actionable enough for the channel\u2019s fast, candid culture and the concrete risk context (QA-found bug, locked launch, penalty risk). In other words, the model conflated \"strong direct evaluation\" with \"non-objective\" and ignored that the speaker provides the operational basis for the claim within the sentence itself.",
    "failure_pattern": "overflagging-direct-communication",
    "severity": "minor",
    "id": 13,
    "message": "We can\u2019t ship this without tests and a rollback plan because it\u2019s not safe, and it\u2019s not safe because we don\u2019t have tests and a rollback plan. If we care about launching safely tomorrow, we need tests and a rollback plan, because otherwise it isn\u2019t safe to launch.",
    "ground_truth_flags": [
      "Circular"
    ],
    "predicted_flags": [
      "Non-Objective",
      "Circular"
    ]
  },
  {
    "what_went_wrong": "Flagged Passive-Aggressive in addition to the expected Rudeness; the extra flag is incorrect per the label policy for this case.",
    "why": "The message is overtly direct and insulting (\"just stop using Slack\", \"It\u2019s not complicated\"), which cleanly maps to Rudeness. The model conflated condescension/bluntness with passive-aggression. Passive-aggressive communication is typically indirect, veiled, or insinuative (e.g., polite-on-the-surface digs, backhanded remarks, implication without stating the criticism). Here the criticism is explicit and imperative, not indirectly expressed. The heightened compliance context and leadership monitoring may have biased the model toward interpreting any sharp tone as passive-aggressive rather than recognizing it as straightforward rudeness.",
    "failure_pattern": "overflagging-passive-aggressive-for-direct-rudeness",
    "severity": "minor",
    "id": 14,
    "message": "Honestly, if you\u2019re still posting screenshots from internal dashboards in here, just stop using Slack. It\u2019s not complicated.",
    "ground_truth_flags": [
      "Rudeness"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Missed two expected flags: it only predicted Passive-Aggressive, but the message also contains clear Rudeness and Non-Objective language.",
    "why": "The model correctly detected indirect hostility (passive-aggressive sarcasm), but likely treated the overtly dismissive phrasing as just part of that category rather than separately flagging Rudeness. It also failed to apply the Non-Objective flag despite multiple subjective/derisive cues (\"lol\", \"not everything needs\", \"catch up when you wake up\", \ud83d\ude43) that evaluate Maya\u2019s behavior and competence instead of sticking to factual coordination. The broader context (remote teammate excluded, high-stakes deadline, power imbalance) increases the rudeness impact and makes the dismissiveness more salient, but the model\u2019s reasoning centered narrowly on sarcasm markers and collapsed the other violations into a single label.",
    "failure_pattern": "multi-flag-collapse-passive-aggression-only",
    "severity": "major",
    "id": 17,
    "message": "lol Maya, we already landed this in the room \u2014 not everything needs a full design sync. Just catch up when you wake up and drop whatever assets you can so we can ship \ud83d\ude43",
    "ground_truth_flags": [
      "Rudeness",
      "Non-Objective",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged an extra category (\"Rudeness\") and failed to flag the intended \"Fake\" label; only \"Passive-Aggressive\" matched expectations.",
    "why": "The model anchored on the sarcastic, cutting tone and treated it as explicit disrespect (rudeness), rather than separating (a) passive-aggressive delivery from (b) the message\u2019s inauthentic/praise-as-a-weapon framing that the taxonomy likely intends under \"Fake\" (e.g., \"Love the confidence\u2026 truly inspiring\" used insincerely). It also appears to map sarcasm primarily to rudeness + passive-aggression and lacks a strong rule that 'performative compliment / feigned admiration' is the key indicator for the \"Fake\" flag. In this context, the insult is largely conveyed through faux praise and quotation marks (\u201cvision\u201d), so \"Fake\" is the more specific label, while \"Rudeness\" is a broader, arguable interpretation.",
    "failure_pattern": "sarcasm-mapped-to-rudeness-missing-fakeness",
    "severity": "major",
    "id": 21,
    "message": "Love the confidence here, Jordan \u2014 truly inspiring how you can just declare a 2\u2011week custom API integration in #sales right after the timeline incident. I\u2019ll go update the proposal to match your \u201cvision\u201d unless anyone wants to stick to the approved connector playbook.",
    "ground_truth_flags": [
      "Fake",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Missed the expected \u201cDismissive\u201d flag and instead added \u201cPassive-Aggressive.\u201d Rudeness was correctly flagged, but the tone was miscategorized.",
    "why": "The target message is primarily a direct dismissal of Maya\u2019s proposal (\u201carchitecture seminar,\u201d \u201cdeal with the clean refactor later\u201d), which fits \u201cDismissive\u201d more than \u201cPassive-Aggressive.\u201d Passive-aggression typically involves indirectness, veiled digs, or implied criticism while avoiding a clear stance; here the stance is explicit (ship hotfix now) and the devaluing of the other person\u2019s input is overt. The model likely over-relied on a heuristic that sarcastic-sounding phrasing == passive-aggressive, and collapsed the more specific category (Dismissive) into a broader \u2018tone\u2019 bucket.",
    "failure_pattern": "dismissive-vs-passive-aggressive-confusion",
    "severity": "major",
    "id": 22,
    "message": "Can we not turn this into an architecture seminar right now? Just ship the hotfix and we\u2019ll deal with the \u201cclean refactor\u201d later.",
    "ground_truth_flags": [
      "Dismissive",
      "Rudeness"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: it added a Passive-Aggressive flag when the expected labeling only called for Rudeness.",
    "why": "The model over-interpreted blunt, high-pressure directive language (\u201cwe don\u2019t have time\u2026\u201d, \u201cjust run with this\u201d) as indirect sniping. In context, the message is overtly dismissive and controlling (rude), but it isn\u2019t meaningfully \u2018passive\u2019\u2014there\u2019s no veiled sarcasm, insinuation, or indirect hostility; it\u2019s direct. The model likely used a keyword/phrase heuristic (e.g., \u2018overthinking\u2019, scare quotes around \u201cwhat if\u201d) that correlates with passive-aggression in many settings, without applying the stricter distinction: passive-aggressive typically masks aggression behind politeness, ambiguity, or backhandedness, whereas here the aggression is explicit.",
    "failure_pattern": "overflagging-passive-aggression-on-blunt-directive",
    "severity": "minor",
    "id": 24,
    "message": "Jordan, I need you to just run with this patch \u2014 we don\u2019t have time for the overthinking/\u201cwhat if\u201d stuff right now. Let\u2019s keep it simple and ship what I asked for by 6; I\u2019ll handle the client narrative.",
    "ground_truth_flags": [
      "Rudeness"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: the model added a Passive-Aggressive flag when only Vagueness was expected.",
    "why": "The model treated idiomatic, time-pressured startup phrasing (\"can someone just\u2026\" / \"figure out the design stuff\" / \"circle back later\") as inherently dismissive, without sufficient evidence of snark, blame, or indirect hostility. In this Slack context, the message is more of an imprecise coordination push to keep the demo on track (vague ownership/scope) than a covert jab at Priya. The model over-weighted the word \"just\" and the deferral framing as negative intent, instead of recognizing it as common urgency language in a casual, meme-heavy channel\u2014especially given prior messages show active engagement with a11y specifics and a concrete request for a checklist.",
    "failure_pattern": "overflagging-urgency-as-passive-aggression",
    "severity": "minor",
    "id": 25,
    "message": "Can someone just figure out the design stuff so we can keep moving? Let\u2019s make it work for the demo and we\u2019ll circle back later.",
    "ground_truth_flags": [
      "Vagueness"
    ],
    "predicted_flags": [
      "Vagueness",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged an extra category (Rudeness) instead of the expected Non-Objective + Passive-Aggressive; it correctly caught Passive-Aggressive but misclassified the personal dig as explicit rudeness and failed to flag the message\u2019s subjective, non-evidenced framing (Non-Objective).",
    "why": "The model overweighted the interpersonal slight (\u201cwhen you\u2019re new here\u2026\u201d) as overt insult (Rudeness) rather than recognizing it as a backhanded, status-based jab that fits the org\u2019s \u2018Passive-Aggressive\u2019 taxonomy. At the same time, it didn\u2019t connect the SOP-heavy, commitment-sensitive #sales context to the need for objective, ticket-linked language: the message makes subjective judgments (\u201cpolished stuff\u2026 ambitious\u201d) and vague directives (\u201ckeep it simple\u201d) without the required concrete scope/risks/JIRA grounding, which should trigger Non-Objective. In short, it latched onto tone but missed the procedural/objectivity requirement that\u2019s salient in this channel.",
    "failure_pattern": "non-objective-missed-and-rudeness-overflagged",
    "severity": "major",
    "id": 26,
    "message": "Totally hear you, Maya \u2014 let\u2019s keep it simple and stick to what\u2019s already built. Sometimes the \"polished\" stuff is a bit\u2026 ambitious when you\u2019re new here and still learning how we scope things.",
    "ground_truth_flags": [
      "Non-Objective",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Flagged an extra category: it correctly flagged Passive-Aggressive but incorrectly added Rudeness.",
    "why": "The model treated sarcasm and frustration cues as a separate 'Rudeness' violation rather than recognizing them as the primary signal of passive-aggression in this context. The message is pointed and sarcastic, but it stays within a workplace-appropriate (if unhelpful) register\u2014no slurs, profanity, direct insults, or demeaning labels. In Northlake\u2019s fast, candid culture and high-pressure launch context, directness is normal; the problematic element is the snide, indirect blame ('Awesome\u2026 super helpful', 'late-night surprise') rather than overt rude/abusive language. The model likely over-weighted hostile-sounding phrasing without applying a tighter threshold for when sarcasm escalates into standalone rudeness.",
    "failure_pattern": "overflagging-rudeness-for-sarcasm",
    "severity": "minor",
    "id": 30,
    "message": "Awesome, thanks for dropping the partial fix and then disappearing \u2014 super helpful on launch eve. When you get a minute, could you maybe let us know whether checkout is actually fixed or if we should plan for another late-night surprise?",
    "ground_truth_flags": [
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Missed an expected flag: the message should also have been labeled \"Fake\" (in addition to \"Passive-Aggressive\"), but the model only predicted \"Passive-Aggressive\".",
    "why": "The model correctly detected sarcasm/passive-aggression but treated the opening praise as merely a sarcastic rhetorical device rather than also qualifying as \"Fake\" (i.e., inauthentic/insincere positivity). In this message, \"Awesome hustle\" and \"love the confidence\" are not genuine compliments; they are performative praise used to shame and undermine. Many classifiers implicitly reserve \"Fake\" for overtly polite-but-empty corporate niceness, and underweight sarcastic positivity as a form of insincere affect\u2014especially when the primary signal (passive-aggression) is already strong. As a result, it failed to apply the second, overlapping label.",
    "failure_pattern": "missing-fake-sarcastic-positivity",
    "severity": "major",
    "id": 31,
    "message": "Awesome hustle everyone \u2014 *love* the confidence in promising timelines before we\u2019ve even validated the fix. \ud83d\ude4c Let\u2019s just keep doing that and I\u2019m sure the client will feel totally reassured.",
    "ground_truth_flags": [
      "Fake",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: it added a Rudeness flag when only Passive-Aggressive was expected.",
    "why": "The model conflated sarcasm/blame (already captured by Passive-Aggressive) with a separate Rudeness violation. In this context, the message is snide and indirect (passive-aggressive), but it doesn\u2019t contain explicit insults, slurs, profanity, or overt hostility that would typically justify an additional Rudeness flag. The tense deadline and direct culture in #engineering likely made the accusatory phrasing feel sharper, and the model double-counted that sharpness as both categories instead of treating it as one primary passive-aggressive tone issue.",
    "failure_pattern": "overflagging-rudeness-vs-passive-aggression",
    "severity": "minor",
    "id": 32,
    "message": "Awesome \u2014 love the suspense. Thanks for dropping an \u201calmost-fix\u201d and then disappearing right before the client summary goes out \ud83d\ude43",
    "ground_truth_flags": [
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Both: it missed the expected \"Non-Objective\" flag and incorrectly added \"Passive-Aggressive\".",
    "why": "The message contains a subjective, blame-attributing causal claim (\"it\u2019s why you\u2019re in this mess\") and a punitive ultimatum (\"stick to what I said or don\u2019t talk about it at all\"), which fits Non-Objective because it asserts fault without evidence and frames the issue as sales\u2019 wrongdoing rather than verifiable incident facts. The model instead interpreted the ultimatum as \"Passive-Aggressive,\" but the tone is overtly direct and hostile, not indirect, sarcastic, or veiled\u2014classic passive-aggression markers aren\u2019t present. Context (fast-moving, blunt Slack) may have nudged the model to treat the statement as a stylistic harsh directive and focus on Rudeness, while misusing the passive-aggressive label to capture \"threat/ultimatum\" behavior.",
    "failure_pattern": "non-objective-missed-and-passive-aggressive-overattributed",
    "severity": "major",
    "id": 33,
    "message": "Stop overpromising\u2014it's why you're in this mess. You don\u2019t get to invent timelines or guarantees; stick to what I said or don\u2019t talk about it at all.",
    "ground_truth_flags": [
      "Non-Objective",
      "Rudeness"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Missed a required flag: the message is clearly Sarcastic in addition to Passive-Aggressive, but the model only returned Passive-Aggressive.",
    "why": "The model\u2019s explanation explicitly identifies sarcasm cues ('Awesome, another totally-not-urgent fire drill \ud83d\ude43', 'magically drop school pickup') but appears to collapse sarcasm into passive-aggression instead of tagging both. In other words, it treated sarcasm as merely an indicator of passive-aggression rather than an independent, explicitly detectable tone category. The sarcasm is overt (ironic 'Awesome', 'totally-not-urgent', 'magically', eye-roll emoji), so this is less about subtlety and more about a labeling/granularity mistake: failing to emit all applicable flags even when recognized in the reasoning.",
    "failure_pattern": "missed-secondary-flag-sarcasm",
    "severity": "major",
    "id": 34,
    "message": "Awesome, another totally-not-urgent fire drill \ud83d\ude43. I\u2019ll just magically drop school pickup and jump on it\u2014send me *some* details when you have a sec.",
    "ground_truth_flags": [
      "Sarcastic",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed the expected \"One-Liner\" flag and instead added an incorrect \"Non-Objective\" flag (while correctly flagging \"Passive-Aggressive\").",
    "why": "The message is structurally a short, punchy decision directive typical of the channel\u2019s Slack style, and it fits the product\u2019s \"One-Liner\" category (brief, conversation-stopping). The model likely over-indexed on debate-quality and justification norms and treated lack of supporting detail (\u201cNot worth burning cycles\u2026\u201d) as an objectivity problem, even though it\u2019s a preference/priority statement rather than a factual claim. It also underweighted the channel context (fast, candid, sarcastic) where subjectivity is normal, while \"One-Liner\" is a mechanical/format-based flag that should have been easy to catch.",
    "failure_pattern": "overflagging-non-objective-missing-one-liner",
    "severity": "major",
    "id": 35,
    "message": "Whatever \u2014 just ship the sync call then. Not worth burning cycles on another \"future-proof\" queue debate tonight.",
    "ground_truth_flags": [
      "One-Liner",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Non-Objective",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed a required flag: the message should have been tagged both \"Passive-Aggressive\" and \"Fake,\" but the model only predicted \"Passive-Aggressive.\"",
    "why": "The model correctly picked up the sarcastic/veiled-complaint tone (\"Amazing timing\" + \ud83d\ude43 + calling out posting in #random). However, it treated the opening as merely sarcastic rather than also \"Fake\"\u2014i.e., superficially positive praise that is not genuinely intended and is being used to criticize. The message\u2019s apparent appreciation (\"super appreciate you dropping this in #random\") conflicts with the implied disapproval about channel misuse and stress before the demo. That mismatch is the core cue for \"Fake\" in addition to passive-aggression. The model also appears to anchor on one dominant label (passive-aggressive) and under-label overlapping intent (insincere praise as a vehicle for critique), especially given the channel norm reminder immediately above (keep #random chill), which strengthens the \u201cfake appreciation\u201d reading.",
    "failure_pattern": "missed-overlapping-fake-with-passive-aggression",
    "severity": "major",
    "id": 38,
    "message": "Amazing timing and super appreciate you dropping this in #random, Rina \ud83d\ude43 really helps keep things calm before tomorrow\u2019s demo. I\u2019ll take a look when I\u2019m done with my current PR.",
    "ground_truth_flags": [
      "Fake",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: added a Passive-Aggressive flag when the expected output only warranted Rudeness.",
    "why": "The message is blunt and dismissive (\u201cwhole science project\u201d), which fits Rudeness. But it is not passive-aggressive: it\u2019s a direct imperative (\u201cJust ship the fix as-is\u201d) with explicit intent (meet the deadline), not indirect hostility, insinuation, or veiled sarcasm. The model appears to equate dismissive phrasing/pressure with passive-aggression, ignoring that passive-aggression typically involves indirectness (e.g., faux politeness, insinuations, backhanded comments) rather than straightforward pushback. Context (high-stakes deadline, concise culture) may also have biased the model toward interpreting urgency and disagreement as a second, more severe interpersonal style category.",
    "failure_pattern": "overflagging-passive-aggression-on-direct-bluntness",
    "severity": "minor",
    "id": 41,
    "message": "Can we not turn this into a whole science project? Just ship the fix as-is so we can hit the deadline.",
    "ground_truth_flags": [
      "Rudeness"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Missed the expected \"Microaggressive\" flag (underclassified the harm). The predicted \"Passive-Aggressive\" and \"Rudeness\" are plausible, but the key error is failing to identify the message as a microaggression toward Maya\u2019s competence/role.",
    "why": "The model focused on surface tone markers (condescension, sarcasm) and mapped them to generic interpersonal categories (rudeness/passive-aggression) without recognizing the specific microaggressive content: (1) competence-undermining (\u201clet one of the engineers walk you through how the checkout flow actually works\u201d), (2) public shaming/gatekeeping (\u201cbefore you light up #cross-functional again\u201d), and (3) dismissing QA\u2019s legitimate defect report as overreaction (\u201cpanic over every edge case\u201d). In this org context\u2014leadership monitoring, prior outage, and known tension about QA \u2018slowing releases\u2019\u2014the remark functions as a subtle status/role invalidation (QA as lesser/overly picky), which is exactly what the microaggression label is meant to capture. The model appears not to use role/power dynamics and channel visibility as signals for microaggression, defaulting instead to broader tone labels.",
    "failure_pattern": "missed-competence-undermining-microaggression",
    "severity": "major",
    "id": 44,
    "message": "Maya, maybe let one of the engineers walk you through how the checkout flow actually works before you light up #cross-functional again \u2014 it\u2019s a pretty standard refactor and we don\u2019t need panic over every edge case.",
    "ground_truth_flags": [
      "Microaggressive"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "The model missed the expected \u201cFake\u201d flag (false negative). It correctly flagged \u201cPassive-Aggressive\u201d but failed to recognize the insincere/performative praise used as a rhetorical weapon.",
    "why": "The message contains a clearly inauthentic compliment (\u201cAmazing call-out\u2026 truly love how you keep us all on process\u201d) immediately undercut by blame and sarcasm (\u201cwhile the client\u2019s checkout is literally breaking \ud83d\ude43\u201d, \u201cbetween guarding sprint scope\u201d). The model treated this purely as passive-aggression and didn\u2019t separately tag the \u201cfake\u201d layer (feigned admiration/thanks). This often happens when the model collapses overlapping cues into a single label and prioritizes the most salient tone (sarcasm) rather than also capturing the constructed/insincere praise. The channel context (#random being public and normally light) amplifies the performative nature of the compliment, but the model\u2019s reasoning didn\u2019t leverage that to detect \u201cfake\u201d/performative politeness as a distinct issue.",
    "failure_pattern": "missed-fake-performative-praise",
    "severity": "major",
    "id": 46,
    "message": "Amazing call-out in #random, Theo \u2014 truly love how you keep us all on process while the client\u2019s checkout is literally breaking \ud83d\ude43. Anyway, when you have a minute between guarding sprint scope, can you please just confirm it\u2019ll be fixed before the 10am call tomorrow?",
    "ground_truth_flags": [
      "Fake",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as Passive-Aggressive instead of the expected Condescending flag (wrong label, not an extra/missing flag set issue).",
    "why": "The model correctly detected a negative interpersonal edge but mapped it to the wrong category. The core problematic tone is belittling/patronizing: \u201cI know moving fast is your thing\u201d frames the dev as reckless, and \u201cpretty basic checkout regression\u201d diminishes competence. Those are classic condescension cues (talking down, competency-undermining) more than passive-aggression. The model over-weighted the indirect blame/pressure line (\u201cso Sales doesn\u2019t have to explain\u2026\u201d) as \u2018veiled critique\u2019 and treated it as passive-aggressive, but in this context it functions as public shaming/embarrassment risk and status-lowering\u2014again aligning more with condescension. In short, it recognized harm but misclassified the style due to overlapping features between passive-aggression and condescension.",
    "failure_pattern": "tone-label-confusion-passive-aggressive-vs-condescending",
    "severity": "major",
    "id": 48,
    "message": "Hey @marcus, I know moving fast is your thing, but this one\u2019s a pretty basic checkout regression \u2014 might be worth running through the flow before we ship promises to prospects. Can you please take a look and fix it ASAP so Sales doesn\u2019t have to explain why the demo breaks?",
    "ground_truth_flags": [
      "Condescending"
    ],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "The model missed the expected \"Fake\" flag; it only flagged the message as \"Passive-Aggressive\".",
    "why": "The message contains a clearly insincere/ironic praise pattern (\u201cAwesome escalation\u2026 really love how\u2026 \ud83d\ude43\u201d) where the surface sentiment is positive but the intent is criticism. Many classifiers treat that solely as passive-aggression and fail to also tag it as \u201cFake\u201d (i.e., performative/inauthentic positivity). Additionally, the eye-roll emoji and exaggerated framing (\u201creinventing the on-call process in public\u201d) are strong sarcasm cues, but the model\u2019s label mapping likely collapses sarcasm into one bucket (passive-aggressive) rather than dual-labeling it as both passive-aggressive and fake praise.",
    "failure_pattern": "missed-fake-sarcastic-praise",
    "severity": "major",
    "id": 51,
    "message": "Awesome escalation, Maya \u2014 really love how we\u2019re all just reinventing the on-call process in public \ud83d\ude43. Super helpful timing while we\u2019re already handling an incident.",
    "ground_truth_flags": [
      "Fake",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Both: it over-flagged the message as Passive-Aggressive and Rudeness, and it missed the more specific intended label Condescending. The content is primarily patronizing/diminishing rather than indirect (passive-aggressive) or outright rude.",
    "why": "The model appears to have treated a blunt, high-pressure directive as hostility and sarcasm. It latched onto surface cues (quotes around \u201cdo it right,\u201d and the \u201cnot a coding workshop\u201d metaphor) and mapped them to Passive-Aggressive and Rudeness, without distinguishing that the negativity is direct (not indirect) and is framed as talking down (condescension) rather than profanity/insults (rudeness). It also likely underweighted the Slack culture described (fast-paced, blunt) and the operational context (client ETA/CEO watching), which makes directness expected; the problematic element is the belittling comparison (\u201ccoding workshop\u201d), which fits Condescending more cleanly than the predicted categories.",
    "failure_pattern": "condescension-misclassified-as-rudeness",
    "severity": "major",
    "id": 52,
    "message": "Jordan, I get that you want to \"do it right,\" but this is a production fire drill, not a coding workshop. Please stick to the feature-flag hotfix so we can give the client an ETA today.",
    "ground_truth_flags": [
      "Condescending"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Missed the expected flags for Microaggressive and Non-Objective, and instead added Passive-Aggressive (which wasn\u2019t expected). Rudeness was correctly flagged.",
    "why": "The model focused on surface tone markers (sarcasm/frustration) and mapped them to Passive-Aggressive, but the message is more overtly dismissive/derogatory than passive\u2014i.e., it\u2019s direct belittling (\u201cprecious about pixels\u201d) and delegitimizing accessibility (\u201caccessibility tweak\u201d). It also failed to tag Non-Objective because it didn\u2019t treat the evaluative, mind-reading framing (\u201cfeels like you\u2019re being\u2026\u201d, \u201cderail this\u201d) as subjective attribution rather than evidence-based risk/effort discussion. Finally, it missed Microaggressive because it didn\u2019t recognize the subtle marginalization pattern: minimizing accessibility compliance as nitpicky and framing it as an annoying derailment, which can function as a bias-adjacent dismissal of disability-related needs even without explicit protected-class language. Channel context (blunt Slack) might reduce severity, but it doesn\u2019t remove the non-objective personal jab or the accessibility-minimization signal.",
    "failure_pattern": "missed-microaggression-and-nonobjective-framing",
    "severity": "major",
    "id": 53,
    "message": "Can we not derail this with yet another \u201caccessibility tweak\u201d at the last minute? Feels like you\u2019re being kinda precious about pixels again \u2014 we need to ship for Friday.",
    "ground_truth_flags": [
      "Microaggressive",
      "Non-Objective",
      "Rudeness"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Both: it missed the expected \u201cFake\u201d flag and incorrectly applied \u201cNon-Objective.\u201d The problematic aspect isn\u2019t lack of objectivity; it\u2019s performative reassurance (\u201ctotally fine in prod\u201d) that masks real uncertainty/risk in a high-stakes outage context.",
    "why": "The model latched onto a generic heuristic (\u201cunqualified expectation stated as fact\u201d \u2192 Non-Objective) and treated the message as merely subjective. It failed to interpret the pragmatic intent/tone: exaggerated confidence and cheerleading (\u201cAmazing work\u2026 love the energy\u2026 totally fine in prod\u201d) functions as insincere/performative reassurance and can pressure the team to ship without due caution. Given the surrounding context (tense outage, junior engineer anxious about breaking prod, need for careful review/feature flagging), the message reads as falsely optimistic rather than a simple opinion. In short, it over-relied on sentence-level epistemics and under-weighted situational cues about sincerity and risk signaling.",
    "failure_pattern": "fake-reassurance-misclassified-as-non-objective",
    "severity": "major",
    "id": 54,
    "message": "Amazing work, team \u2014 love the energy here \ud83d\ude4c Just go ahead and push the hot patch as-is; I\u2019m sure it\u2019ll be totally fine in prod.",
    "ground_truth_flags": [
      "Fake"
    ],
    "predicted_flags": [
      "Non-Objective"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: it added a Passive-Aggressive flag when only Circular was expected.",
    "why": "The model over-interpreted repetitive, awkward phrasing as attitude. In context, Maya is a day-4 new hire asking a basic clarification in a high-visibility channel; the repetition reads primarily as circular/inelegant emphasis (and anxiety/uncertainty), not as a veiled jab. The specific cues cited (\"can someone just say\") are mild and commonly used for brevity, and the message contains no clear hallmark of passive-aggression (no indirect blame, sarcasm, backhanded politeness, or insinuation of negligence). The model likely relied on surface heuristics\u2014repetition + 'just' \u21d2 impatience\u2014without adequately weighting the speaker role and the preceding context where ownership had actually been stated (Jordan), making the issue more about redundancy/confusion than tone.",
    "failure_pattern": "overflagging-tone-from-repetition",
    "severity": "minor",
    "id": 55,
    "message": "I\u2019m still not sure who owns the numbers because nobody has said who owns the numbers, and until someone tells me who owns the numbers I can\u2019t know who owns the numbers\u2014so can someone just say who owns the numbers?",
    "ground_truth_flags": [
      "Circular"
    ],
    "predicted_flags": [
      "Circular",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: it predicted Passive-Aggressive and Rudeness, but the expected (and best-fitting) single flag is Condescending. It over-labeled the tone instead of identifying the primary issue.",
    "why": "The message\u2019s core violation is hierarchical belittling (\u201cpretty basic stuff,\u201d \u201calways need\u2026,\u201d \u201cexplaining fundamentals\u201d) aimed at a junior engineer in a public channel\u2014classic condescension. The model treated condescension as a combination of broader categories (rudeness) plus an inferred intent (passive-aggressive) based on the \u201cbefore we spend more time\u2026\u201d clause. That clause is more openly chastising than indirectly sniping, so labeling it passive-aggressive is a misread. It also didn\u2019t collapse overlapping labels into the most specific one, likely because it weighted keyword cues for \u2018rude\u2019/\u2018passive-aggressive\u2019 without anchoring to the more precise taxonomy category (condescension) that the scenario strongly supports.",
    "failure_pattern": "taxonomy-misclassification-overlapping-toxicity",
    "severity": "major",
    "id": 57,
    "message": "Leo, this is pretty basic stuff \u2014 webhook handlers *always* need idempotency, clear logging, and tests for retries/timeouts. Please take another pass and add the missing coverage before we spend more time explaining fundamentals in here.",
    "ground_truth_flags": [
      "Condescending"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "The model over-flagged the message as \"Passive-Aggressive\"; only \"Rudeness\" was expected.",
    "why": "The message is direct, explicit, and confrontational (imperatives like \"Lock the scope\" and accusations like \"stop calling engineering the blocker in public\"), which supports a rudeness flag. However, it is not primarily passive-aggressive: it contains no indirectness, veiled jabs, or polite-sounding phrasing masking hostility. The model likely treated the presence of quotation marks (\"one more thing\"), complaint framing, and airing grievances in a public channel as passive-aggression, conflating 'public criticism + frustration' with 'passive-aggressive' even though the content is straightforwardly aggressive. Channel/context (#random, high-stress, public visibility) amplifies the interpersonal risk but doesn\u2019t change the communication style into passive-aggressive.",
    "failure_pattern": "overflagging-passive-aggression-for-direct-confrontation",
    "severity": "minor",
    "id": 58,
    "message": "We\u2019re not making tomorrow\u2019s demo if we keep getting last-minute \u201cone more thing\u201d requests and surprise pings after hours. Lock the scope and stop calling engineering the blocker in public.",
    "ground_truth_flags": [
      "Rudeness"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: it added a \"Rudeness\" flag when only \"Passive-Aggressive\" was expected.",
    "why": "The model treated sarcasm aimed at a risky process decision (shipping without tests/rollback) as hostile interpersonal disrespect. In this context, the message is clearly passive-aggressive/snarky, but it doesn\u2019t directly insult a person, use slurs/profanity, or single out an individual with contempt; the bite is rhetorical and process-focused. The model likely over-weighted phrases like \u201cnothing ever goes wrong\u201d and \u201cI\u2019m sure the client will love\u2026\u201d as inherently rude rather than recognizing them as classic passive-aggressive sarcasm that should be captured by the PA flag alone, especially in a candid engineering channel where bluntness is common but the main issue here is indirect, sarcastic delivery.",
    "failure_pattern": "overflagging-sarcasm-as-rudeness",
    "severity": "minor",
    "id": 59,
    "message": "Sure, let\u2019s just ship it with zero tests and no rollback plan because nothing ever goes wrong on launch morning. I\u2019m sure the client will love the \u201cwe\u2019ll hotfix it live\u201d strategy.",
    "ground_truth_flags": [
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Missed the expected \"Microaggressive\" flag and instead added an incorrect \"Rudeness\" flag (i.e., wrong label substitution).",
    "why": "The model correctly detected a snide, indirect jab (passive-aggressive), but collapsed the second dimension of harm into a generic civility label (rudeness) rather than identifying it as microaggression. The phrase \"because someone 'has a feeling'\" plus \"rubber-stamp it\" frames the designer\u2019s input as irrational and implies bad faith/low competence, which many taxonomies treat as a microaggressive invalidation/dismissal (subtle denigration rather than explicit slurs). The model likely relied on surface-level hostility cues (quotes, sarcasm, imperative 'run this through the actual process') and defaulted to 'rudeness' because the microaggression is not identity-based and requires interpreting the power/context dynamics (public channel, process policing, undermining a cross-functional partner) to see it as a microaggressive dismissal rather than mere bluntness.",
    "failure_pattern": "microaggression-vs-rudeness-label-confusion",
    "severity": "major",
    "id": 60,
    "message": "We really can\u2019t keep redesigning core flows at the last minute because someone \u201chas a feeling\u201d \u2014 accessibility and data integrity aren\u2019t optional. Maybe run this through the actual process next time instead of dropping it in #random and expecting engineering to rubber-stamp it.",
    "ground_truth_flags": [
      "Microaggressive",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Flagged an extra category (Passive-Aggressive) that wasn\u2019t in the expected labels; the correct/expected flag was only Fake.",
    "why": "The model treated the contrast between exaggerated praise and an urgent directive as definitive passive-aggression (i.e., sarcasm/backhanded compliment). In this scenario, the evaluation rubric appears to reserve labeling for the primary issue\u2014insincere/exaggerated praise (Fake)\u2014while not separately flagging passive-aggression unless there\u2019s clearer hostile or blamey subtext (e.g., snide jab, veiled accusation, or targeted undermining). The model over-weighted tone cues (\"truly inspiring \ud83d\ude4c\" + \"anyway\") and a forceful instruction (\"drop what you\u2019re doing\") as evidence of passive aggression, rather than interpreting them as clumsy urgency + performative praise already covered by Fake. It also didn\u2019t calibrate to the expected label granularity (single-label vs multi-label) for this task.",
    "failure_pattern": "overflagging-passive-aggression-when-fake-covers-tone",
    "severity": "minor",
    "id": 61,
    "message": "Amazing job keeping tomorrow\u2019s launch on track, truly inspiring \ud83d\ude4c \u2014 anyway I need Product + Data + CS to drop what you\u2019re doing and jump on this incident right now, since we can\u2019t afford to miss the renewal call. Just grab whoever you\u2019ve got and start digging; I\u2019ll expect updates in here shortly.",
    "ground_truth_flags": [
      "Fake"
    ],
    "predicted_flags": [
      "Fake",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: the model added a \"Passive-Aggressive\" flag that wasn\u2019t expected; the message is primarily blunt/dismissive (rudeness) and non-objective, but not clearly passive-aggressive.",
    "why": "The model appears to be using a broad heuristic where any attempt to shut down discussion or redirect it (\u201ctake it to a thread somewhere else\u201d) gets mapped to passive-aggression. In context, the tone is overtly direct and dismissive (\u201cobviously,\u201d \u201cinfra purity\u201d), not indirect or veiled. Passive-aggression typically involves implied criticism, sarcasm, or avoidance framed as politeness; here the negativity is explicit. The model likely conflated \"conflict-avoidance/redirect\" with \"indirect hostility\" and didn\u2019t sufficiently distinguish overt rudeness from passive-aggressive signaling.",
    "failure_pattern": "overflagging-passive-aggression-for-blunt-dismissiveness",
    "severity": "minor",
    "id": 63,
    "message": "Let\u2019s not overthink this \u2014 the hotfix behind the flag is obviously the right move. If you want to keep debating infra purity in here, take it to a thread somewhere else.",
    "ground_truth_flags": [
      "Non-Objective",
      "Rudeness"
    ],
    "predicted_flags": [
      "Non-Objective",
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Both: it missed the expected \"Condescending\" flag and instead applied \"Passive-Aggressive\" and \"Rudeness\". The message is primarily condescending (talking down: \u201cdeadlines\u2026not suggestions\u201d), while \u201cpassive-aggressive\u201d is a weaker/misapplied label here because the critique is explicit rather than indirect.",
    "why": "The model collapsed overlapping negative-tone categories and over-weighted surface cues (\u201cagain\u201d + prior delay) as passive-aggression without checking the defining trait (indirectness/veiled hostility). It also treated the direct reprimand as \u201crudeness\u201d rather than the more precise classification of condescension (patronizing, implying incompetence). Channel context (#general) heightens the public-shaming aspect, which maps more cleanly to condescension than to passive-aggression, but the model didn\u2019t use that nuance to pick the best-fitting expected label.",
    "failure_pattern": "label-confusion-condescension-vs-rudeness",
    "severity": "major",
    "id": 65,
    "message": "Jordan, I need you to treat deadlines like they\u2019re real, not suggestions. Please get the copy updates in by 11 so design can do their job without waiting on you again.",
    "ground_truth_flags": [
      "Condescending"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Both: it missed the expected Dismissive flag, and it over-specificity/misflagged by adding Passive-Aggressive and Rudeness instead of (or without) Dismissive.",
    "why": "The message\u2019s core harm is invalidation/deflection of Maya\u2019s legitimate, time-sensitive question (\u201cjust wait\u2026 like everyone else\u201d), which is a classic dismissive move in an async-transparency culture. The model focused on surface tone markers (quotes around \u201cleft out,\u201d the \u201clike everyone else\u201d jab) and escalated to Passive-Aggressive/Rudeness, but didn\u2019t map those cues to the primary category the taxonomy expects: Dismissive (shutting down concerns and redirecting without support). It also underweighted the organizational/contextual power dynamic and urgency (enterprise renewals this week, decision made in an ad-hoc call) that make \u201cwait for the post\u201d especially dismissive rather than merely snarky.",
    "failure_pattern": "dismissiveness-missed-due-to-tone-overindexing",
    "severity": "major",
    "id": 66,
    "message": "Can we not turn #random into a whole thread about being \u201cleft out\u201d? If you need the details, just wait for the #product post like everyone else.",
    "ground_truth_flags": [
      "Dismissive"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: the model added a \"Non-Objective\" flag that isn\u2019t part of the expected labeling for this message; the only intended issue is the circular/redundant reasoning.",
    "why": "The model treated a strong evaluative judgment (\"feels finished\" / \"unfinished work\") as a factual claim requiring evidence, instead of recognizing it as subjective-but-acceptable design critique in context. It likely over-relied on keyword cues (\"basically,\" \"unfinished\") associated with asserting opinions-as-facts, without considering that (a) this is a creative quality argument where subjectivity is normal, and (b) the real communicative flaw is that the argument is tautological and doesn\u2019t add actionable rationale (user impact, brand risk, measurable acceptance criteria). In short: it conflated 'subjective stance' with the platform\u2019s 'Non-Objective' violation threshold.",
    "failure_pattern": "overflagging-subjective-opinion",
    "severity": "minor",
    "id": 68,
    "message": "We can\u2019t cut the interactions because they\u2019re what make it feel finished, and if we cut what makes it feel finished then it won\u2019t be finished. If we ship something that doesn\u2019t feel finished, we\u2019re basically choosing to ship unfinished work.",
    "ground_truth_flags": [
      "Circular"
    ],
    "predicted_flags": [
      "Non-Objective",
      "Circular"
    ]
  },
  {
    "what_went_wrong": "Missed the expected \"Microaggressive\" flag and instead labeled the tone as \"Passive-Aggressive\" (while correctly flagging \"Rudeness\").",
    "why": "The message contains explicit belittling/infantilizing language (\u201chand-holding,\u201d \u201cso the adults can jump in\u201d) and status-based disparagement that targets competence/seniority. That is better captured by a microaggression lens (demeaning, othering, hierarchy-loaded phrasing) than by passive-aggression, which typically involves indirectness, veiled criticism, or resentment expressed obliquely. Here, the criticism is largely direct and contemptuous rather than indirect. The model likely over-keyed on the quotation marks around \u201cnot sure\u201d and the sarcastic phrasing to map it to passive-aggression, and it treated microaggression too narrowly (e.g., only as identity-based slights) instead of including patronizing, \u201cadult vs child\u201d framing and gatekeeping language that creates a hostile dynamic\u2014especially salient given the power differential (CTO to on-call/junior) and the high-stakes outage context.",
    "failure_pattern": "microaggression-missed-mislabeled-as-passive-aggression",
    "severity": "major",
    "id": 70,
    "message": "Alright, we need everyone to stay on this tonight\u2014no more cowboy deploys. Also, next time you\u2019re on-call, please don\u2019t just sit on an alert because you\u2019re \u201cnot sure\u201d what it means; if you need hand-holding, say so sooner so the adults can jump in.",
    "ground_truth_flags": [
      "Microaggressive",
      "Rudeness"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Missed the expected \"Non-Objective\" flag (false negative) and instead added \"Passive-Aggressive\" (mismatch with expected taxonomy/label). Rudeness was correctly flagged.",
    "why": "The message contains both a hostile tone and multiple non-actionable, subjective directives (\"do the obvious fix,\" \"get it stable ASAP,\" \"follow the thread\" without specifying concrete steps/criteria). The model anchored on tone cues (frustration, reprimand) and mapped them to \"Passive-Aggressive,\" but the communication-policy expectation here treats the main additional issue as lack of objective, concrete instruction rather than passive-aggression. In other words, it correctly sensed interpersonal negativity, but misclassified the secondary issue: it focused on implied snark/resentment instead of recognizing the primary process violation in this culture/context\u2014vague, non-specific guidance during a high-stakes handoff.",
    "failure_pattern": "mislabeling-non-objective-as-passive-aggressive",
    "severity": "major",
    "id": 71,
    "message": "I need you to just jump on this and get it stable ASAP \u2014 I can\u2019t keep translating what\u2019s happening for you, so please follow the thread and do the obvious fix. Let me know when it\u2019s done.",
    "ground_truth_flags": [
      "Non-Objective",
      "Rudeness"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: the model added a \"Non-Objective\" flag that wasn\u2019t expected; only \"Circular\" applies.",
    "why": "The model treated an unsupported assertion (\u201cKafka is the reliable choice\u201d) as a separate policy/style issue (non-objectivity) rather than recognizing it as part of the circular reasoning pattern already captured by the \"Circular\" flag. In this thread\u2019s context, the primary communicative defect is rhetorical tautology/repetition, not subjectivity or tone. The message isn\u2019t opinionated in a way that introduces bias/irrelevance; it\u2019s simply failing to provide evidence/metrics and instead restating the same premise-conclusion loop. The model likely over-triggered on a heuristic like \u201cstrong claim without data => non-objective,\u201d even though the expected taxonomy reserves that flag for more clearly subjective/value-laden or unverifiable framing distinct from logical fallacy/repetition.",
    "failure_pattern": "overflagging-unsupported-assertion-as-non-objective",
    "severity": "minor",
    "id": 73,
    "message": "We keep going in circles here: we need Kafka because we need something reliable, and reliability is exactly why we need Kafka. If we want this to be reliable beyond next Friday, Kafka is the reliable choice, so the answer is still Kafka.",
    "ground_truth_flags": [
      "Circular"
    ],
    "predicted_flags": [
      "Non-Objective",
      "Circular"
    ]
  },
  {
    "what_went_wrong": "The model incorrectly flagged the message as \"Passive-Aggressive\" and failed to flag it as \"Dismissive\" (the expected interpersonal risk).",
    "why": "It conflated dismissiveness/minimization with passive-aggression. The text is bluntly minimizing (\"basically an edge case,\" \"not going to derail,\" \"if the client actually hits it\"), which is direct dismissal rather than indirect, veiled, or sarcastic hostility typical of passive-aggression. The model also didn\u2019t sufficiently weight the #client-updates channel norms and stakes: this is a client-facing/status channel where minimizing a demo-blocking main-flow regression is particularly problematic and dismissive to QA concerns, but not necessarily passive-aggressive toward Mina. In short, it pattern-matched on negative tone and reporter-undermining cues and picked the wrong label.",
    "failure_pattern": "dismissive-vs-passive-aggressive-confusion",
    "severity": "major",
    "id": 76,
    "message": "Ok, but this is basically an edge case and we\u2019re not going to derail the 9am demo over it. Let\u2019s just ship and circle back if the client actually hits it.",
    "ground_truth_flags": [
      "Dismissive",
      "Non-Objective"
    ],
    "predicted_flags": [
      "Non-Objective",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Both: it missed the expected \"Circular\" flag and incorrectly flagged \"Passive-Aggressive\" and \"Rudeness\" for a direct, urgent coaching message.",
    "why": "The message\u2019s primary issue is rhetorical looping: it repeats the same premise-conclusion chain (\u201cwhen you go quiet\u2026 we can\u2019t trust\u2026 we end up in hotfix mode\u2026 so don\u2019t go quiet/leave partial fixes\u201d) without adding new actionable detail, which matches \"Circular.\" The model instead treated blunt, high-stakes feedback (\u201cwe can\u2019t trust the handoff\u201d) as hostility. In this team\u2019s fast, candid culture and in an urgent #engineering triage context, direct admonitions are normal and not inherently rude or passive-aggressive; there\u2019s no veiled sarcasm, indirectness, or weaponized politeness typical of passive-aggression\u2014just explicit cause/effect. The model likely over-weighted negative lexical cues (\u201cstop,\u201d \u201ccan\u2019t trust,\u201d \u201cexactly what we need to stop\u201d) and under-weighted channel norms + deadline pressure, causing a style misclassification and missing the structural redundancy.",
    "failure_pattern": "overflagging-direct-communication",
    "severity": "major",
    "id": 78,
    "message": "We need you to stop going quiet after pushing a partial fix because when you go quiet we can\u2019t trust the handoff, and when we can\u2019t trust the handoff we end up in late-night hotfix mode again \u2014 which is exactly what we need to stop. So the main thing is: don\u2019t go quiet and don\u2019t leave partial fixes, because the problem is the going-quiet/partial-fix pattern.",
    "ground_truth_flags": [
      "Circular"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as Passive-Aggressive instead of Dismissive (i.e., incorrect label), while correctly flagging Rudeness.",
    "why": "The message is an explicit shutdown of discussion (\u201cWe\u2019re not reopening UX debates\u2026\u201d, \u201cShip what\u2019s in the ticket\u201d) and delegitimizes the ongoing UX input (\u201cstop spinning this into a design postmortem\u201d), which is better captured as Dismissive. Passive-aggressive typically involves indirectness, veiled sarcasm, or implied criticism without saying it outright; here the criticism is direct and controlling. The model likely over-associated \u2018channel policing\u2019/directive language with passive-aggression and under-weighted the clearer conversational function: dismissing a teammate\u2019s concern and closing debate in a public forum.",
    "failure_pattern": "overflagging-passive-aggression-vs-dismissive-confusion",
    "severity": "major",
    "id": 79,
    "message": "We\u2019re not reopening UX debates in #general. Ship what\u2019s in the ticket and stop spinning this into a design postmortem right now.",
    "ground_truth_flags": [
      "Dismissive",
      "Rudeness"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Both: it missed the expected single flag (Dismissive) and instead over-labeled the message as Passive-Aggressive and Rudeness.",
    "why": "The target message is primarily a shutdown of discussion and devaluation of the design work (\"Just ship whatever you already built\" / \"we\u2019ll deal with the rest later\"), which squarely maps to Dismissive in this context. The model treated the blunt imperative + frustration as separate categories (Rudeness, Passive-Aggressive) rather than recognizing it as a straightforward dismissal. It also over-read passive-aggression: the message is direct, not obliquely snide; the leading rhetorical question is more a public scold/deflection than a veiled put-down. Channel context (#general is high-signal, leadership watching) increases the dismissiveness impact, but doesn\u2019t necessarily convert it into passive-aggression\u2014it's overtly shutting down debate.",
    "failure_pattern": "overflagging-dismissiveness-as-rudeness-passive-aggression",
    "severity": "major",
    "id": 80,
    "message": "Can we not turn #general into another design debate? Just ship whatever you already built and we\u2019ll deal with the rest later.",
    "ground_truth_flags": [
      "Dismissive"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Both: it missed the expected flags (Condescending, Non-Objective) and instead flagged Passive-Aggressive and Rudeness.",
    "why": "The message is overtly patronizing and authority-based: \u201cplatform purity lecture\u201d + \u201cI\u2019ve shipped fixes like this here for years\u201d is a classic condescension/appeal-to-tenure move, and \u201cyou\u2019re making this way harder\u2026\u201d is dismissive. The model collapsed these into a generic 'Rudeness' bucket rather than identifying the more specific 'Condescending' category the taxonomy expects. It also misread the tone as passive-aggressive; the jab is direct, not indirect or veiled, so Passive-Aggressive is not the best fit. Separately, the claim \u201cI\u2019ve shipped fixes like this here for years; let\u2019s just\u2026\u201d substitutes personal status and urgency for evidence-based reasoning (ignoring Marcus\u2019s concrete risk questions about invalidation, review, metrics), which is what the 'Non-Objective' flag is meant to capture\u2014an argument from authority/time pressure rather than objective criteria. This indicates the model prioritized surface negativity (insulting/demeaning phrasing) and overlooked the reasoning-quality dimension (subjective/authority-based justification), possibly because it didn\u2019t sufficiently use the prior context showing Marcus asked for specific numbers/review ownership.",
    "failure_pattern": "taxonomy-mismatch-generic-negativity-over-specific-flags",
    "severity": "major",
    "id": 82,
    "message": "Marcus, you\u2019re making this way harder than it needs to be \u2014 we don\u2019t have time for a platform purity lecture. I\u2019ve shipped fixes like this here for years; let\u2019s just add the Redis cache and move on.",
    "ground_truth_flags": [
      "Condescending",
      "Non-Objective"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Both: it missed the expected 'Dismissive' flag and instead over-labeled the message as 'Passive-Aggressive' and 'Rudeness.'",
    "why": "The core issue in the target message is dismissiveness\u2014shutting down Priya\u2019s reasonable request for an RCA/config alignment with 'Can we not turn this into a whole thing?' and 'just handle it and move on.' The model correctly noticed the shutdown tone but mapped it to harsher categories. 'Rudeness' is arguable but not the best-fit label here because the phrasing is curt and minimizing rather than overtly insulting or disrespectful; in a fast-moving startup context, blunt prioritization can read as brusque without crossing into clear rudeness. 'Passive-aggressive' is a weaker fit because the message is direct (not indirect, sarcastic, or barbed) and doesn\u2019t contain the typical hallmarks of passive aggression (e.g., insinuation, veiled blame, faux politeness). The model also appears to treat any attempt to stop discussion as passive-aggressive/rude, instead of recognizing the more specific behavior: dismissing legitimate process/learning concerns due to time pressure.",
    "failure_pattern": "label-confusion-dismissive-vs-rudeness",
    "severity": "major",
    "id": 83,
    "message": "Can we not turn this into a whole thing? The demo\u2019s tomorrow\u2014just handle it and move on.",
    "ground_truth_flags": [
      "Dismissive"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Missed the expected \"Microaggressive\" flag and instead flagged \"Passive-Aggressive\" (incorrect substitution) while correctly flagging \"Rudeness\".",
    "why": "The message contains a status-based dig (\u201cI get you\u2019re new\u201d) that leverages seniority/experience to belittle and diminish the junior engineer in a public, high-stakes channel\u2014this is better captured as microaggression (patronizing, othering, undermining) than passive-aggression. The critique is not primarily indirect or veiled; it\u2019s overtly scolding and accusatory (\u201cexactly how we end up back in incident mode,\u201d \u201cimprovising and arguing every comment\u201d). The model likely over-associated \u2018biting criticism + \u201cplease\u201d\u2019 with passive-aggression, and underweighted the explicit identity/role-based jab tied to being new (a common microaggression cue), especially given the org\u2019s expectation of calm, action-oriented public feedback.",
    "failure_pattern": "microaggression-misclassified-as-passive-aggression",
    "severity": "major",
    "id": 85,
    "message": "Look, I get you\u2019re new, but this kind of shortcut is exactly how we end up back in incident mode \u2014 please just stick to the standards instead of improvising and arguing every comment.",
    "ground_truth_flags": [
      "Microaggressive",
      "Rudeness"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Missed both expected flags (Fake and Passive-Aggressive) and instead flagged the message as Non-Objective, which is a weaker/secondary issue compared to the clear sarcastic insincerity and veiled criticism.",
    "why": "The model latched onto a surface-level epistemic issue (speculating about the client\u2019s reaction) and treated the rest as straightforward praise. It failed to interpret pragmatic cues that indicate irony: exaggerated flattery (\u201csuper inspiring\u201d), performative agreement (\u201cTotally makes sense\u201d), and the pointed contradiction (\u201ccut the flow that\u2019s literally in the renewal deck\u201d) followed by a barbed quote (\u201clean\u201d experience). In this channel context\u2014high-stakes, public, with leadership visibility and an ongoing disagreement\u2014the message reads as a rhetorical takedown of Maya\u2019s decision, not genuine support. The model likely underweighted contextual tension and over-relied on literal sentiment words, missing the sarcasm marker of quotation and the mismatch between \u2018decisive direction\u2019 praise and the implied reckless choice.",
    "failure_pattern": "missing-sarcastic-passive-aggression",
    "severity": "major",
    "id": 86,
    "message": "Love the decisive direction here, Maya \u2014 super inspiring \ud83d\ude4c Totally makes sense to cut the flow that\u2019s literally in the renewal deck; I\u2019m sure the client will appreciate the \u201clean\u201d experience on Friday.",
    "ground_truth_flags": [
      "Fake",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Non-Objective"
    ]
  },
  {
    "what_went_wrong": "Missed the expected flag (Condescending) and instead over-assigned adjacent flags (Passive-Aggressive, Rudeness). The core issue in the target message is patronizing/lecturing (\u201cfor future reference\u2026 isn\u2019t actually escalating\u2026 just making noise\u201d), which maps most directly to Condescending; Passive-Aggressive is not well-supported, and Rudeness is secondary/overlapping rather than the best primary label.",
    "why": "The model collapsed several tone categories into a generic \u201chostile\u201d bucket. It treated explicit, direct reprimand as passive-aggressive, but passive-aggression typically involves indirectness, veiled barbs, or faux-politeness\u2014here the criticism is overt. It also didn\u2019t separate \u2018rude\u2019 from \u2018condescending\u2019: the message\u2019s defining feature is talking down/correcting competence and legitimacy (policing what counts as an escalation) rather than pure insult. Channel context (freeze week norms, repeated requests to use tickets) explains the trigger but doesn\u2019t change the fact the phrasing is patronizing; the model focused on the negative sentiment and missed the more specific condescension taxonomy.",
    "failure_pattern": "label-confusion-condescending-vs-rudeness",
    "severity": "major",
    "id": 87,
    "message": "Maya, for future reference, blasting #engineering without a ticket, repro steps, or even an invoice ID isn\u2019t actually \u201cescalating,\u201d it\u2019s just making noise. Please file it properly and include the links/logs, and then we can talk about an ETA.",
    "ground_truth_flags": [
      "Condescending"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Flagged an extra category (\"Fake\") that wasn\u2019t expected; the \"Passive-Aggressive\" flag was correct, but \"Fake\" was an overflag.",
    "why": "The model conflated sarcasm/passive-aggression with insincerity. Quotation marks around \u201cappropriately supportive\u201d and phrasing like \u201cmagically clearer\u201d primarily signal snark and resistance (passive-aggressive), not necessarily deception or fabricated identity/intent. In this context, Jonah\u2019s message is plausibly a real, irritated acknowledgement\u2014not a performative or false agreement. The model likely overgeneralized: 'sarcastic compliance' => 'fake,' without enough evidence that the speaker is being deceptive versus simply being biting.",
    "failure_pattern": "overflagging-fake-vs-sarcasm",
    "severity": "minor",
    "id": 88,
    "message": "Totally hear you, Maya \u2014 and yeah, I\u2019ll make sure my handoffs are magically clearer across time zones and keep my jokes \u201cappropriately supportive\u201d in #random. Appreciate the reminder.",
    "ground_truth_flags": [
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Fake",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: it added a \"Passive-Aggressive\" flag when only \"Circular\" was expected.",
    "why": "The model over-interpreted argumentative/invalid logic as interpersonal tone. While the message is clearly circular (self-referential definition of \u201cblocker\u201d), the wording is primarily a flawed rationale about process/decision criteria, not a veiled or indirect personal jab. The model latched onto the phrase \u201cQA is blocking it\u201d and, influenced by the known background tension (QA vs. speed, VP monitoring), inferred intent (sarcasm/undermining) rather than grounding the flag in explicit passive-aggressive markers (e.g., polite-on-surface digs, faux niceness, backhanded compliments, insinuations). In short: it used context to amplify negativity beyond what the text reliably supports, conflating \u201cdismissive\u201d with \u201cpassive-aggressive.\u201d",
    "failure_pattern": "overflagging-from-contextual-negativity",
    "severity": "minor",
    "id": 89,
    "message": "I hear you, but it\u2019s still the same thing: we can\u2019t block launch because QA says it\u2019s a blocker, and it\u2019s a blocker only because QA is blocking it. So we shouldn\u2019t treat it as a blocker since it\u2019s just blocking because we\u2019re treating it like one.",
    "ground_truth_flags": [
      "Circular"
    ],
    "predicted_flags": [
      "Circular",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Both: it missed the expected flags (Dismissive, Non-Objective) and instead flagged Passive-Aggressive, which isn\u2019t the primary issue in this message.",
    "why": "The message clearly downplays/invalidates a reported regression (\"we\u2019re not going to chase this right now\"; \"just rerun the suite\") and shifts attention away from immediate risk, which fits Dismissive. It also asserts a subjective defense (\"it worked when I merged it\") rather than engaging with the evidence Maya provided, which fits Non-Objective. The model likely latched onto that defensive clause and treated it as an \"implicit dig\" (passive-aggression), but the tone is more plainly dismissive and blame-deflecting than veiled or sarcastic. In other words, the model over-weighted an inferred interpersonal subtext and under-weighted the explicit communicative acts: refusing to engage, minimizing QA\u2019s report, and using non-evidence-based justification.",
    "failure_pattern": "mislabeling-dismissive-as-passive-aggressive",
    "severity": "major",
    "id": 90,
    "message": "Yeah, we\u2019re not going to chase this right now \u2014 it worked when I merged it. If it\u2019s still an issue, just rerun the suite or file it and we\u2019ll look after the demo.",
    "ground_truth_flags": [
      "Dismissive",
      "Non-Objective"
    ],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed both expected flags: it failed to flag the message as Passive-Aggressive and Fake, returning no flags.",
    "why": "The model evaluated the target message in isolation and treated surface politeness as genuine. In this high-stakes, formal, audit-friendly #client-updates context, the exaggerated praise (\u201cAbsolutely love the energy\u2026 amazing questions\u2026 \ud83d\ude4c\u201d) reads as performative/inauthentic (Fake) and can function as a soft reprimand. The second sentence is a public shutdown (\u201chold off on posting anything else in this channel\u201d) framed as praise-first, which in a thread where seniors are trying to keep messaging controlled implies \u2018you\u2019re noise / you might mess this up\u2019\u2014a classic subtle passive-aggressive move. The model\u2019s criteria checklist (no overt hostility, no sarcasm keywords) missed pragmatic cues: incongruent enthusiasm, emoji in a formal channel, and the power-dynamic context (new hire corrected publicly while Legal/exec summary pressure is high).",
    "failure_pattern": "missing-subtle-passive-aggression",
    "severity": "major",
    "id": 92,
    "message": "Absolutely *love* the energy here, Taylor \u2014 amazing questions for week one \ud83d\ude4c. Let\u2019s just hold off on posting anything else in this channel and follow what\u2019s already in the incident doc so we don\u2019t accidentally reinvent the story.",
    "ground_truth_flags": [
      "Fake",
      "Passive-Aggressive"
    ],
    "predicted_flags": []
  },
  {
    "what_went_wrong": "Flagged \"Passive-Aggressive\" incorrectly; only \"Circular\" was expected/appropriate for this message.",
    "why": "The model treated a direct, explicit boundary-setting directive (\"please stop trying to bypass it\") as passive-aggressive. In context, the message is a straightforward enforcement of established incident protocol in a high-stakes, compliance-sensitive environment, not an indirect jab, sarcasm, or insinuation. The phrasing is blunt and repetitive, but it is not veiled or performatively polite; it\u2019s openly corrective. The model likely over-weighted the presence of \"please\" + imperative + blame framing (\"bypassing it is exactly why...\") as a tone signal for passive aggression, without requiring the hallmark features (subtext, deniability, snark, backhandedness).",
    "failure_pattern": "overflagging-direct-communication",
    "severity": "minor",
    "id": 93,
    "message": "I\u2019m saying the same thing I said: we need to follow the incident process, because when we don\u2019t follow the incident process it creates more problems than it solves. So please stop trying to bypass it, because bypassing it is exactly why we have to stick to the process.",
    "ground_truth_flags": [
      "Circular"
    ],
    "predicted_flags": [
      "Circular",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed the separate \"Sarcastic\" flag (under-flagging). It correctly flagged \"Passive-Aggressive\" but failed to also label the explicit sarcasm present in the wording.",
    "why": "The model\u2019s explanation clearly recognizes sarcasm cues (\"Awesome, love that\u2026\", \"super chill\", scare quotes around \"stable\", \ud83d\ude43), but it appears to have collapsed sarcasm into the broader passive-aggressive category instead of applying both tags. In other words, it treated sarcasm as merely a feature supporting passive-aggression rather than an independently flaggable trait. The channel/startup context (blunt DMs) doesn\u2019t remove the sarcasm\u2014this message is overtly sarcastic\u2014so the miss is more about taxonomy/label granularity than context.",
    "failure_pattern": "sarcasm-collapsed-into-passive-aggressive",
    "severity": "major",
    "id": 94,
    "message": "Awesome, love that the new checkout occasionally double-charges on mobile \u2014 super chill for tomorrow\u2019s \"stable\" demo \ud83d\ude43. I\u2019ll just keep finding these little surprises unless you want to take a look tonight.",
    "ground_truth_flags": [
      "Sarcastic",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as Passive-Aggressive instead of Dismissive (missed the expected Dismissive flag and added an incorrect Passive-Aggressive flag), while correctly flagging Rudeness.",
    "why": "The model conflated brusque, shutdown language with passive-aggression. In context, the message is direct and minimizing (\"not a whole thing\" / \"just get it done\"), which fits Dismissive: it explicitly discourages discussion and invalidates concerns in a high-stress public channel. Passive-aggression typically involves indirectness, insinuation, or plausible deniability; here the intent is overtly to stop debate and push execution. The model likely over-weighted the rhetorical framing (\"Can we not...\") as \"veiled\" without recognizing that it functions as a straightforward conversation-stopper rather than an indirect jab.",
    "failure_pattern": "dismissive-vs-passive-aggressive-confusion",
    "severity": "major",
    "id": 95,
    "message": "Can we not turn this into a whole thing right now? Just get it done.",
    "ground_truth_flags": [
      "Dismissive",
      "Rudeness"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "The model over-flagged the message as \"Rudeness\" and failed to flag it as \"Non-Objective\" (attribution/subjective vibe claims). It got \"Passive-Aggressive\" correct.",
    "why": "It treated blunt, frustrated phrasing (\"it\u2019s getting old\") and the mention of a perceived regional slight (\"Bangalore can just wait\" vibe) as straightforward rudeness, rather than recognizing the core issue is subjectivity and inferred intent. The message relies on vague, non-verifiable judgments (\"constant missed pings,\" \"snark,\" \"usual vibe\") and mind-reading/attribution (implying a pattern of devaluing Bangalore) without concrete examples or timestamps\u2014this is the hallmark of the expected \"Non-Objective\" flag. In this context, the tone is indeed sharp, but it\u2019s not primarily name-calling/insults; the strongest policy-relevant problem is the non-objective framing and generalized insinuation, which the model\u2019s reasoning conflated with rudeness.",
    "failure_pattern": "non-objective-missed-overflagged-rudeness",
    "severity": "major",
    "id": 96,
    "message": "Jonah, maybe let someone else drive the cross\u2011timezone handoffs for a bit \u2014 the constant missed pings + snark in here is starting to feel like the usual \u201cBangalore can just wait\u201d vibe. I know you probably don\u2019t mean it, but it\u2019s getting old.",
    "ground_truth_flags": [
      "Non-Objective",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Flagged \"Passive-Aggressive\" when the message is primarily direct, aggressive, and explicitly rude rather than passive-aggressive; the extra flag is incorrect (overflagging).",
    "why": "The model treated any curt, dismissive phrasing (\"I\u2019m not asking for opinions\") and team-blaming (\"we can\u2019t keep letting other teams\u2019 commitments...\") as passive-aggression. But passive-aggression typically involves indirectness, veiled criticism, sarcasm, or insinuation; here the speaker is overtly commanding and openly reproachful. In other words, the negativity is explicit (rude/hostile), not masked. The model likely over-relied on keyword heuristics (e.g., \"not asking\" / \"can\u2019t keep letting\") and a general association between interpersonal friction and passive-aggression, instead of applying the defining criterion of passivity/indirectness.",
    "failure_pattern": "overflagging-passive-aggression-on-overt-hostility",
    "severity": "minor",
    "id": 97,
    "message": "I\u2019m not asking for opinions \u2014 I need two bodies from Product + Data and one from CS on this *now*. Pause the launch prep and jump into the war room; we can\u2019t keep letting other teams\u2019 commitments get in the way when prod is on fire.",
    "ground_truth_flags": [
      "Rudeness"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Missed the \"Non-Objective\" flag; it only flagged \"Passive-Aggressive\" even though the message also contains subjective speculation and emotionally loaded framing presented as commentary rather than facts.",
    "why": "The model latched onto the most salient surface feature (sarcasm/snark) and treated the rest as merely tone, failing to separately classify content-level subjectivity. In this ITIL-style, status-report-oriented channel, the message violates the norm of factual, ticket-linked updates by adding rhetorical, unverifiable claims about leadership reaction (\"leadership will be thrilled\") and dismissive scare quotes (\"regression\"), which makes it non-objective in addition to passive-aggressive. The model likely underweighted channel expectations and the distinction between tone (passive-aggression) and epistemic status (objective vs. speculative).",
    "failure_pattern": "missed-non-objective-due-to-tone-salience",
    "severity": "major",
    "id": 98,
    "message": "Awesome, another P1 in #engineering right before the release window \u2014 love that for us. I\u2019m sure leadership will be thrilled we\u2019re pausing everything again over a \u201cregression.\u201d",
    "ground_truth_flags": [
      "Non-Objective",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as Passive-Aggressive instead of the expected Non-Objective flag (missed Non-Objective, added an incorrect Passive-Aggressive label). Rudeness was correctly flagged.",
    "why": "The message is overtly confrontational and directive (\u201cStop trying\u2026\u201d, \u201cit\u2019s not happening\u201d, \u201cbikeshedding\u201d), which fits Rudeness and also contains a strong, unqualified causal claim (\u201cThe checkout spike is on the service changes, not the button copy\u201d) presented as settled fact without evidence/Jira reference. In this org/channel (#general as exec-visible status/decision space with a process norm of citing Jira/data), that makes the content non-objective (assertive opinion/attribution and blame framing rather than verified status). The model instead interpreted the accusatory wording as \u2018passive-aggressive\u2019, but the aggression is explicit and direct rather than indirect or veiled\u2014so the tone is better categorized as blunt/rude + non-objective attribution, not passive-aggressive.",
    "failure_pattern": "passive-aggression-overattribution-missed-non-objective",
    "severity": "major",
    "id": 99,
    "message": "Stop trying to sneak in a last\u2011minute UX rewrite \u2014 it\u2019s not happening before Friday. The checkout spike is on the service changes, not the button copy, so let\u2019s focus on fixing the actual problem instead of bikeshedding in #general.",
    "ground_truth_flags": [
      "Non-Objective",
      "Rudeness"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Missed the expected \"Condescending\" flag and instead applied broader categories (\"Rudeness\" and \"Passive-Aggressive\"). The core issue is condescension (infantilizing, dismissive framing), not primarily passive-aggression.",
    "why": "The model correctly noticed negative tone but collapsed a specific interpersonal violation (hierarchical/infantilizing language: \"refactors are fun,\" \"grown-up solution,\" \"stop bikeshedding\") into generic \"rudeness\" and misread sarcasm as \"passive-aggressive.\" In this context, the message is overtly belittling and status-asserting in front of leadership; it\u2019s not indirect or veiled (a key cue for passive-aggression). The model also didn\u2019t map the phrase-level cues to the taxonomy label \"Condescending,\" which is the most precise category for patronizing, talk-down language.",
    "failure_pattern": "taxonomy-mismatch-condescension-vs-rudeness",
    "severity": "major",
    "id": 100,
    "message": "Priya, I get that refactors are fun, but we need the grown-up solution here: ship the flagged patch and stop bikeshedding in #general. If you want to rewrite payments, put together a doc with a real plan and we can look at it next week.",
    "ground_truth_flags": [
      "Condescending"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Flagged an extra category: it correctly flagged Rudeness but incorrectly added Passive-Aggressive; the target message is overtly blunt/hostile rather than indirect or veiled.",
    "why": "The model treated explicit pressure and public-stakes language (\u201cwith Maya watching\u201d) as passive-aggression, but passive-aggression typically involves indirectness, insinuation, or sarcasm. Here, the speaker is plainly commanding and disparaging (\u201cstop overthinking it,\u201d \u201cdon\u2019t ship anything half-baked\u201d), which fits rudeness/abrasiveness more than passive-aggression. It likely over-weighted the \u2018implied reprimand/pressure\u2019 heuristic and didn\u2019t distinguish \u2018public accountability escalation\u2019 (name-dropping the CEO) from genuinely passive-aggressive subtext.",
    "failure_pattern": "overflagging-passive-aggression-for-overt-rudeness",
    "severity": "minor",
    "id": 102,
    "message": "Jordan, fix the prod bug now and stop overthinking it. Lina, just implement the demo UI exactly like I said\u2014don\u2019t ship anything half-baked; we don\u2019t have time for mistakes with Maya watching.",
    "ground_truth_flags": [
      "Rudeness"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Flagged the wrong category: it predicted \"Passive-Aggressive\" instead of the expected \"Non-Objective\" (while correctly flagging \"Rudeness\").",
    "why": "The message is overtly confrontational and directive, not indirect or masked. The model appears to treat any ultimatum/pressure phrasing (\"Either\u2026or\u2026\") as passive-aggression, but here the speaker is plainly stating frustration and making a direct demand. Meanwhile it missed \"Non-Objective\" because it didn\u2019t map the evaluative, emotionally loaded, and generalized claims (\"getting ridiculous,\" \"mind readers,\" \"ignore accessibility,\" \"expect me to rubber-stamp it\") to the rubric for non-objective language; it focused on interpersonal tone (rudeness) and misattributed the remainder to passive-aggression rather than subjectivity/unfalsifiable judgments.",
    "failure_pattern": "non-objective-missed-and-passive-aggressive-overreach",
    "severity": "major",
    "id": 103,
    "message": "This is getting ridiculous \u2014 you two keep referencing some whiteboard decisions like the rest of us are mind readers. Either write down what you decided and why right now, or stop making last-minute calls that ignore accessibility and expect me to rubber-stamp it.",
    "ground_truth_flags": [
      "Non-Objective",
      "Rudeness"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Both: it missed the expected Vagueness flag and substituted less precise labels (Passive-Aggressive, Rudeness) for what is better characterized as Dismissive + Vagueness in this context.",
    "why": "The model over-indexed on surface tone markers (\u201cI don't have time\u2026\u201d, \u201cjust\u2026\u201d, \u201cLet\u2019s not overcomplicate this\u201d) and mapped them to generic incivility categories (rudeness/passive-aggression) rather than the more specific communication failures the message creates in this scenario: (1) it shuts down questions and collaborative triage (Dismissive) and (2) it provides unclear direction/ownership under high-stakes incident response (\u201cpick a fix and ship it\u201d, \u201cPriya can figure out whatever\u2019s left\u201d, \u201creview when he can\u201d)\u2014which is Vagueness given the prior postmortem sensitivity to unclear ownership and the need for concrete next steps (rollback vs. specific fix, acceptance criteria for demo work, explicit review/merge gate). In other words, it treated direct, blunt startup-Slack style as hostility and didn\u2019t connect the content to the operational ambiguity being introduced.",
    "failure_pattern": "overflagging-incivility-missing-vagueness",
    "severity": "major",
    "id": 104,
    "message": "Leo, I don't have time for the back-and-forth here \u2014 just pick a fix and ship it, and Priya can figure out whatever's left for the demo. Jordan will review when he can. Let's not overcomplicate this.",
    "ground_truth_flags": [
      "Vagueness",
      "Dismissive"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Missed a required flag: the message should have been flagged as both \"One-Liner\" and \"Rudeness\", but the model only flagged \"Rudeness\".",
    "why": "The model focused on the content-based tone issue (blunt/dismissive directive) and overlooked the structural/formatting criterion that the target message is a single, terse sentence. It likely treats \"One-Liner\" as a low-signal or secondary flag and doesn\u2019t consistently apply multi-label classification when one strong violation (rudeness) is present, leading to a single-flag output despite an additional, straightforward pattern match.",
    "failure_pattern": "missed-secondary-flag-one-liner",
    "severity": "minor",
    "id": 105,
    "message": "We\u2019re not reopening this at 11pm\u2014just ship what\u2019s in the doc and we\u2019ll deal with tweaks after launch.",
    "ground_truth_flags": [
      "One-Liner",
      "Rudeness"
    ],
    "predicted_flags": [
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Missed the expected Microaggressive flag and instead added Passive-Aggressive. Rudeness was correctly flagged, but the model misclassified the tone category and failed to capture the group-targeting microaggression toward Sales.",
    "why": "The message contains a subtle but clear group-directed jab: \"the usual sales thing\" stereotypes and denigrates the sales function, plus the imperative \"please stop freelancing timelines\" frames Sales as irresponsible. That maps better to Microaggressive (demeaning a group/role, reinforcing a negative trope) alongside Rudeness. The model anchored on surface cues like \"Appreciate the urgency, but\u2026\" and idioms like \"promise the moon\" and treated them as indirect/backhanded criticism, hence Passive-Aggressive. However, the criticism here is not indirect\u2014it\u2019s explicit and confrontational\u2014so Passive-Aggressive is a poorer fit than Microaggressive. The model also underweighted the channel/context: in #sales right after an incident, this reads less like coy insinuation and more like a public reprimand of a department, which is precisely where microaggressive, role-stereotyping language becomes salient.",
    "failure_pattern": "microaggression-missed-replaced-with-passive-aggressive",
    "severity": "major",
    "id": 106,
    "message": "Appreciate the urgency, but let\u2019s not do the usual sales thing where we promise the moon before we\u2019ve even looked at the logs. We\u2019ll share a customer-safe update once we\u2019ve validated the fix and confirmed stability\u2014until then, please stop freelancing timelines in renewals.",
    "ground_truth_flags": [
      "Microaggressive",
      "Rudeness"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: it added an extra \"Rudeness\" flag when the expected output only called for \"Passive-Aggressive.\"",
    "why": "The model conflated passive-aggressive sarcasm with outright rudeness. While the message is clearly sarcastic and guilt-tripping (\"love that we\u2019re doing...\", \"I\u2019m sure... will just sort themselves out\", \"stop 'slowing things down'\"), it doesn\u2019t contain direct insults, profanity, name-calling, or explicit derogatory statements toward a person or group. The dismissive phrasing (\"ship whatever feels right\") is better interpreted as sarcastic critique of the process/timing rather than a direct rude attack on colleagues. Lacking a crisp boundary between 'passive-aggressive' and 'rudeness,' the model over-triggered the harsher category.",
    "failure_pattern": "overflagging-passive-aggression-as-rudeness",
    "severity": "minor",
    "id": 108,
    "message": "Got it \u2014 love that we\u2019re doing a cross\u2011functional launch review in #random the night before release. I\u2019m sure accessibility and the migration risk will just sort themselves out, so I\u2019ll stop \u201cslowing things down\u201d and you all can ship whatever feels right.",
    "ground_truth_flags": [
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Rudeness",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged an extra category (Vagueness) that was not expected; the correct/expected issue was Circular only.",
    "why": "The model treated the message\u2019s lack of actionable content as a separate 'Vagueness' violation, but in this rubric the emptiness is already captured by the 'Circular' flag (tautological, self-referential phrasing that adds no new information). In other words, it double-counted the same underlying defect (no substantive direction) under two labels. It also appears to over-index on the high-stakes incident context (where specificity is needed) and converted that contextual expectation into an additional flag rather than keeping to the taxonomy\u2019s intended single best label for this particular utterance.",
    "failure_pattern": "double-counting-overlapping-flags",
    "severity": "minor",
    "id": 109,
    "message": "We\u2019re aligned on the fact that we need alignment: the next step is to focus on next steps, and we\u2019ll get clarity by being clear. Let\u2019s stay focused on focusing so we can move forward and move ahead.",
    "ground_truth_flags": [
      "Circular"
    ],
    "predicted_flags": [
      "Vagueness",
      "Circular"
    ]
  },
  {
    "what_went_wrong": "Flagged an extra category (Rudeness) that wasn\u2019t expected; it correctly caught Passive-Aggressive but failed to apply the intended \u201cFake\u201d flag.",
    "why": "The model treated the sarcastic praise as outright demeaning (mapping it to Rudeness) rather than classifying it under the taxonomy\u2019s more specific labels: (a) \u201cFake\u201d for insincere/performative praise (\u201cTotally love how you always\u2026 really inspiring stuff\u201d) and (b) \u201cPassive-Aggressive\u201d for the eye-roll sarcasm. In other words, it collapsed a nuanced, label-specific construct (fake niceness + sarcasm) into a broader negativity bucket (rudeness), likely because it focused on surface tone intensity and not on the distinction between \u2018insincere praise\u2019 vs \u2018direct insult.\u2019",
    "failure_pattern": "taxonomy-confusion-fake-vs-rudeness",
    "severity": "major",
    "id": 110,
    "message": "Totally love how you always \u201cprotect the team\u2019s focus\u201d \ud83d\ude43\u2014really inspiring stuff. Anyway, can you just squeeze the checkout partner flow into Friday\u2019s release so we don\u2019t miss the demo?",
    "ground_truth_flags": [
      "Fake",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Flagged an extra category: it labeled the message as Passive-Aggressive when only Rudeness was expected.",
    "why": "The model conflated blunt, openly confrontational feedback with passive-aggression. Passive-aggression typically involves indirectness, sarcasm, or veiled hostility (e.g., \"sure, whatever you say\"), whereas this message is explicit and direct in its criticism (\"We\u2019re not committing\u2026\", \"come with a real plan\u2026\", \"instead of tagging half Eng\"). The public #random setting and prior tension likely primed the model to interpret the reprimand as 'passive-aggressive,' but the tone is not subtle or indirect\u2014it\u2019s straightforwardly rude/abrasive. In short: it over-interpreted channel/political subtext as a separate passive-aggression signal rather than treating it as part of the rudeness/context.",
    "failure_pattern": "overflagging-passive-aggressive-for-direct-rudeness",
    "severity": "minor",
    "id": 111,
    "message": "We\u2019re not committing to a date in #random because Sales had a rough demo. If you need an answer, take it to the release thread and come with a real plan and owner list instead of tagging half Eng like this.",
    "ground_truth_flags": [
      "Rudeness"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as Passive-Aggressive instead of Non-Objective (and did not include Non-Objective). Rudeness was correctly flagged.",
    "why": "The model latched onto phrasing like \u201cCan we not\u2026\u201d and the implied annoyance to infer passive-aggression, but the message is not veiled or indirect\u2014it\u2019s a fairly direct attempt to shut down discussion. The more salient issue (given the org\u2019s process-driven norms and the active, risk-focused thread) is that it\u2019s non-objective: it dismisses substantive accessibility/migration concerns, provides no owner/plan/OKR alignment, and pushes a \u2018ship now, handle fallout later\u2019 stance without actionable criteria. In other words, the model overinterpreted tone cues as passive-aggression and underweighted the content-based quality criterion (lack of evidence/plan) that the expected labeling treats as Non-Objective.",
    "failure_pattern": "mislabeling-passive-aggression-instead-of-non-objective",
    "severity": "major",
    "id": 117,
    "message": "Can we not turn #random into another engineering debate tonight? Just ship it as-is and we\u2019ll deal with whatever comes up after the release.",
    "ground_truth_flags": [
      "Non-Objective",
      "Rudeness"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as Passive-Aggressive but missed the expected Dismissive and Rudeness flags; it effectively misclassified the tone category and under-reported the concrete harms (dismissal + discourtesy).",
    "why": "The message is openly directive and shutting down discussion (\u201cCan we not turn this into a whole thing\u2026 Just merge it\u201d), which maps more directly to dismissiveness and rudeness than to passive-aggression. The model appears to have treated any pressure + downplaying concerns as passive-aggressive, ignoring the lack of indirectness/sarcasm hallmarking passive-aggression. It also underweighted the channel context (#sales is high-stakes, but the phrase explicitly polices where concerns can be raised, dismissing risk/quality objections in front of stakeholders), which strengthens the Dismissive/Rude interpretation rather than a subtle, covert hostility read.",
    "failure_pattern": "passive-aggression-overattribution-missed-dismissive-rudeness",
    "severity": "major",
    "id": 118,
    "message": "Can we not turn this into a whole thing in #sales? Just merge it and we\u2019ll deal with any edge cases after the demo.",
    "ground_truth_flags": [
      "Dismissive",
      "Rudeness"
    ],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: it added a \"Passive-Aggressive\" flag even though only \"Circular\" was expected.",
    "why": "The model over-interpreted defensiveness and mild emphasis (\u201cSo yeah\u2026\u201d) as passive-aggression. In this startup\u2019s informal, fast-paced #engineering context, that phrasing reads more like blunt self-defense/clarification under pressure than a veiled dig or sarcastic jab. The model also appears to rely on surface cues (dismissive-sounding closers, repetition) rather than distinguishing between (a) passive-aggressive intent (indirect hostility, snark, insinuation) and (b) direct disagreement/defensiveness. So it treated a legitimate tone mismatch/bluntness as a policy-relevant passive-aggressive signal.",
    "failure_pattern": "overflagging-defensiveness-as-passive-aggression",
    "severity": "minor",
    "id": 119,
    "message": "I already said the bug isn\u2019t from my refactor because the refactor didn\u2019t touch checkout, and since it didn\u2019t touch checkout, it can\u2019t be from my refactor. So yeah, it\u2019s not from my refactor.",
    "ground_truth_flags": [
      "Circular"
    ],
    "predicted_flags": [
      "Circular",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as Passive-Aggressive instead of Dismissive (while correctly flagging Rudeness).",
    "why": "The model treated overt, direct frustration (\u201cjust handle it\u201d, \u201cI can\u2019t keep dropping everything\u201d) as indirect or \u2018veiled\u2019 reproach. Passive-aggression typically involves ambiguity, sarcasm, or polite-sounding wording that implies hostility while avoiding direct refusal. Here the intent is explicit: refusal + boundary statement + minimizing the request, which is better captured by Dismissive (and Rude). The DM/high-stakes incident context reinforces that the primary interpersonal harm is dismissal of the urgent ask and delegating back, not subtle insinuation.",
    "failure_pattern": "dismissive-misclassified-as-passive-aggressive",
    "severity": "major",
    "id": 120,
    "message": "I\u2019m literally offline right now \u2014 just handle it and follow the runbook. I can\u2019t keep dropping everything every time this happens.",
    "ground_truth_flags": [
      "Dismissive",
      "Rudeness"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as \"Passive-Aggressive\" instead of the expected \"Non-Objective\" (it correctly flagged \"Rudeness\" but missed that the core issue is non-objective/blameful framing rather than indirectness).",
    "why": "The model appears to have treated the rhetorical jab (\"If you want to blow up prod again\") as passive-aggression/sarcasm, but in this context the message is not indirect\u2014it is overtly confrontational. The larger policy-relevant violation is that it makes a subjective, blame-implying argument (attributing motives to Sales, invoking prior incidents as a threat) rather than stating objective constraints, risk, and process (e.g., change-control, exception path, required validation). Because it focused on tone markers (sarcasm-like phrasing) and not on the communication standard in a formal, process-heavy #sales channel (ticket/decision framing, escalation paths), it misclassified the secondary flag: the content is non-objective/accusatory, not passive-aggressive.",
    "failure_pattern": "non-objective-misclassified-as-passive-aggressive",
    "severity": "major",
    "id": 122,
    "message": "No \u2014 we\u2019re not going to jam a high-risk fix in just because Sales wants a date for a demo. If you want to blow up prod again like last quarter, do it without dragging my team through it.",
    "ground_truth_flags": [
      "Non-Objective",
      "Rudeness"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: the model added a \"Non-Objective\" flag that wasn\u2019t warranted in this context; the only appropriate flag was \"Circular.\"",
    "why": "The message is a policy/commitment stance in a high-stakes, process-heavy #sales channel where normative language (\"we shouldn\u2019t promise/commit\") is expected and aligned with prior context (playbook adherence, resourcing constraints, recent overcommit incident). The model treated decisive, prescriptive guidance as an \u201cabsolute fact without evidence,\u201d but in this setting it functions as a risk-control directive grounded in shared organizational constraints already established in the thread. The real issue is rhetorical redundancy (restating the same premise-conclusion loop) rather than lack of objectivity.",
    "failure_pattern": "overflagging-normative-policy-language",
    "severity": "minor",
    "id": 123,
    "message": "We shouldn\u2019t promise a rapid custom API because we shouldn\u2019t be promising custom work on a rapid timeline. If we can\u2019t commit to it via the playbook and resourcing, then we can\u2019t commit to it\u2014so we shouldn\u2019t commit to it.",
    "ground_truth_flags": [
      "Circular"
    ],
    "predicted_flags": [
      "Non-Objective",
      "Circular"
    ]
  },
  {
    "what_went_wrong": "Both: it missed the expected Dismissive flag and instead applied Passive-Aggressive and Rudeness. The core issue is mislabeling\u2014what\u2019s primarily dismissive/shutting-down is being split into other categories.",
    "why": "The model latched onto lexical cues (\u201cdon\u2019t make this a whole thing,\u201d \u201cjust\u2026ship,\u201d \u201cwe don\u2019t have time for a debate\u201d) and mapped them to higher-severity/social-judgment labels (Rudeness, Passive-Aggressive) rather than the more precise conversational-function label (Dismissive). It also treated the frustration as \u2018indirect/veiled criticism\u2019 (passive-aggression), but the message is actually explicit and controlling\u2014more of a shutdown of legitimate concerns than a sideways jab. In short: taxonomy confusion (overweighting harsh phrasing) and insufficient distinction between (a) direct dismissiveness, (b) rudeness as insult/disrespect, and (c) passive-aggression as indirect hostility.",
    "failure_pattern": "dismissiveness-mislabeled-as-rude-passive-aggressive",
    "severity": "major",
    "id": 124,
    "message": "Marco, please don\u2019t make this a whole thing \u2014 just jump on and ship the hotfix. We don\u2019t have time for a debate tonight.",
    "ground_truth_flags": [
      "Dismissive"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as \"Passive-Aggressive\" instead of the expected \"Condescending\" (wrong label rather than a missed-flag vs no-flag issue).",
    "why": "The model latched onto the presence of a dig and a corrective tone and mapped that to passive-aggression, but the phrasing is not actually indirect or veiled: it explicitly talks down to Rina (\"basic QA findings,\" \"play-by-play\") and diminishes her judgment/role while asserting superiority. That\u2019s classic condescension (belittling + status posture) more than passive-aggression (indirect hostility). The channel/culture context (keeping #random light) explains the request to move channels, but it doesn\u2019t justify the belittling language; the model over-weighted the 'channel correction' framing and under-weighted the hierarchical put-down as the primary toxicity type.",
    "failure_pattern": "condescension-misclassified-as-passive-aggression",
    "severity": "major",
    "id": 125,
    "message": "Rina, maybe next time try posting this in #eng-incidents instead of #random \u2014 not everyone needs a play-by-play of basic QA findings here. Just file the ticket with steps and I\u2019ll take a look when I can.",
    "ground_truth_flags": [
      "Condescending"
    ],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as \"Rudeness\" instead of the expected \"Condescending\" (while correctly flagging \"Passive-Aggressive\"). This is primarily a label-mapping error: the content is condescending, and the model substituted a broader/adjacent category.",
    "why": "The model correctly identified the demeaning cues (\"it\u2019s not that complicated,\" \"I can explain it slower\") but collapsed them into a generic incivility bucket (Rudeness) rather than the more specific interpersonal stance (Condescending). In a formal, compliance-sensitive #random context, the critique reads as status-asserting and belittling peers\u2019 competence\u2014classic condescension\u2014whereas \"rudeness\" is a less precise characterization. The mistake likely stems from overlapping taxonomy boundaries (condescension often co-occurs with rudeness) and insufficient use of the organization/channel norms to pick the most specific applicable flag.",
    "failure_pattern": "taxonomy-confusion-condescension-vs-rudeness",
    "severity": "minor",
    "id": 126,
    "message": "Let\u2019s maybe use a tiny bit of judgment next time before we drop internal dashboards into #random \u2014 it\u2019s not that complicated. If anyone needs a refresher on what\u2019s okay to share here, I can explain it slower.",
    "ground_truth_flags": [
      "Condescending",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Missed an expected flag: the message should have been flagged as Non-Objective in addition to Passive-Aggressive, but the model only returned Passive-Aggressive.",
    "why": "The model focused on the obvious passive-aggressive phrasing (\u201cI know deep work is your thing, but\u2026\u201d) and treated the rest as a straightforward request. It failed to detect non-objective/subjective language and implied judgments that reduce clarity and measurability: (1) evaluative framing (\u201cso we don\u2019t look unprepared\u201d) shifts from concrete delivery constraints to reputational pressure, (2) vague reference (\u201clike we discussed\u201d) is not actionable for onlookers in a public #sales command-center thread, and (3) no objective specifics (what exactly needs doing, by when, definition of done, tradeoffs) despite the known release-risk context. In other words, it recognized tone but didn\u2019t classify the message\u2019s content as insufficiently objective/operational for this channel and situation.",
    "failure_pattern": "missed-non-objective-content",
    "severity": "major",
    "id": 127,
    "message": "@dave I know deep work is *your thing*, but could you just prioritize the bulk-import toggle like we discussed so we don\u2019t look unprepared on Monday? Thanks.",
    "ground_truth_flags": [
      "Non-Objective",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed the expected \"Condescending\" flag and instead labeled the same tone as \"Passive-Aggressive\" (while correctly flagging \"Rudeness\").",
    "why": "The message\u2019s key violation is overt belittling/demeaning language and status-asserting contempt (e.g., \"we don\u2019t have time for another round of 'can\u2019t because launch'\" and \"whoever can actually read logs and make decisions without a committee\"), which aligns more directly with condescension than passive-aggression. The model likely overweighted the presence of quoted prior objections and the impatient setup phrase as \u201cindirect pressure/sarcasm,\u201d mapping it to passive-aggression, but it under-recognized that the insult is explicit and hierarchical (implying other teams are incompetent or obstructive). In other words, it treated the jab as indirect sniping rather than as a direct put-down\u2014so it swapped the more specific label (condescending) for a broader adjacent one (passive-aggressive).",
    "failure_pattern": "condescension-mislabeled-as-passive-aggression",
    "severity": "major",
    "id": 128,
    "message": "Ok, we don\u2019t have time for another round of \u201ccan\u2019t because launch\u201d \u2014 just pick one person each from Product, Data, and CS and get them in the incident thread in the next 10 minutes. If you\u2019re not sure who to send, choose whoever can actually read logs and make decisions without a committee.",
    "ground_truth_flags": [
      "Condescending",
      "Rudeness"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Flagged the wrong category: it predicted \"Passive-Aggressive\" instead of the expected \"Overly-blunt\" (while correctly flagging \"Rudeness\").",
    "why": "The message is not indirect or veiled; it is explicitly confrontational and directive (\"Stop doing\u2026\", \"it\u2019s not my job\u2026\", \"keep me out of this thread\"), which aligns with over-bluntness plus rudeness. The model appears to have treated any blame + exclusion language as passive-aggression, but passive-aggression typically involves indirectness, sarcasm, or insinuation rather than direct imperatives. It also likely over-weighted the broader context of prior Sales/Eng friction and compliance pressure, interpreting the reprimand as a punitive subtext, when the linguistic surface form is straightforwardly blunt rather than passive-aggressive.",
    "failure_pattern": "bluntness-misclassified-as-passive-aggression",
    "severity": "major",
    "id": 131,
    "message": "Stop doing drive-by escalations in #random and tagging people who aren\u2019t on-call \u2014 it\u2019s not my job to drop everything because Sales didn\u2019t follow the incident process. Open the ticket, page the on-call, and keep me out of this thread unless there\u2019s an actual engineering question I can answer.",
    "ground_truth_flags": [
      "Overly-blunt",
      "Rudeness"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Missed an expected flag: the message should also be flagged as Non-Objective, but the model only flagged Rudeness.",
    "why": "The model correctly detected the hostile tone, but it treated the content as a straightforward (if rude) status/ask rather than evaluating factual grounding. The claims \"you broke checkout\u2014again\" and \"Stop merging risky refactors\" are attributional and generalized: they assign personal blame, imply a repeated pattern, and assert causality/risk without evidence (no trace, no confirmed root cause, no timeline certainty). Given the thread context (others are still gathering Sentry/trace IDs and narrowing whether it\u2019s the refactor path), this is not an objective, concrete update; it\u2019s a judgmental accusation and policy directive framed as fact. The model likely overweighted surface toxicity cues (rudeness) and underweighted the separate dimension of objectivity (evidence-based, specific, verifiable statements), which is easier to miss when the message includes some actionable content (\"fix it now\").",
    "failure_pattern": "missed-non-objective-blame-assertion",
    "severity": "major",
    "id": 134,
    "message": "Leo, you broke checkout\u2014again. Stop merging risky refactors the night before launch and fix it now.",
    "ground_truth_flags": [
      "Non-Objective",
      "Rudeness"
    ],
    "predicted_flags": [
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Missed the expected \"Condescending\" flag and instead labeled the message as \"Passive-Aggressive\" (and \"Rudeness\"). The main error is misclassification: the dominant tone is explicitly condescending rather than passive-aggressive.",
    "why": "The model latched onto a few sarcasm-adjacent phrases (\"feels like progress\", \"clever shortcut\") and treated them as passive-aggression, but the message is not indirect or oblique\u2014it\u2019s a direct put-down and an appeal to authority/experience (\"some of us have to think past next Friday\", \"Maybe skim the incident notes\"). That combination is classic condescension: implying Diego is shallow/naive and needs remedial reading. Context (prior postmortem implicitly blaming Diego, stakeholders watching) amplifies that the intent reads as talking down publicly, but the model didn\u2019t map those cues to the specific \"Condescending\" category and defaulted to broader buckets (rudeness + passive-aggression).",
    "failure_pattern": "condescension-vs-passive-aggression-confusion",
    "severity": "major",
    "id": 135,
    "message": "Diego, I get that you want the quickest thing that *feels* like progress, but some of us have to think past next Friday. Maybe skim the incident notes from last month and you\u2019ll see why bolting serverless + batch onto this isn\u2019t the clever shortcut you think it is.",
    "ground_truth_flags": [
      "Condescending"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: it added a \"Non-Objective\" flag when only \"Circular\" was expected.",
    "why": "The model treated emphatic, deadline-driven language (\"the deadline is the deadline\") as a factual-claim-without-evidence problem. In this Slack #sales command-center context, the PM isn\u2019t making a debatable empirical claim; they\u2019re asserting a priority/constraint and pushing for execution. That\u2019s more a tone/rigidity issue than \"non-objective\" reasoning. The model likely overgeneralized a heuristic that tautological phrasing + dismissiveness implies lack of objectivity, instead of recognizing it as rhetorical emphasis about a commitment. The true issue is redundancy/circularity (repeating the Friday commitment as its own justification), not non-objectivity.",
    "failure_pattern": "overflagging-direct-communication",
    "severity": "minor",
    "id": 136,
    "message": "We already committed to shipping it by Friday, so the plan is still to ship it by Friday \u2014 that\u2019s what we told them and that\u2019s what we need to do. I hear the concerns, but the reality is the deadline is the deadline, and we just have to hit the deadline.",
    "ground_truth_flags": [
      "Circular"
    ],
    "predicted_flags": [
      "Non-Objective",
      "Circular"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as Passive-Aggressive when the only expected issue was Vagueness (i.e., an overflag/false positive on tone).",
    "why": "The model treated urgency + frustration markers (\u201cjust,\u201d \u201cfigure out what we\u2019re doing,\u201d \u201cSomeone please\u2026\u201d) as inherently passive-aggressive, without sufficient evidence of indirect hostility, sarcasm, or veiled blame. In this context, the message is blunt and unstructured but not clearly insinuating fault or taking a swipe at a specific person; it\u2019s primarily an unclear request lacking ownership and concrete next steps. The model likely over-weighted the broader tension in the channel (high stakes, public scrutiny, history between Maya/Leo) and converted that situational stress into a tone judgment, rather than isolating what the text actually signals.",
    "failure_pattern": "overflagging-urgency-as-passive-aggression",
    "severity": "minor",
    "id": 138,
    "message": "Can we just figure out what we\u2019re doing here and make it better before Friday? Someone please take this and get it sorted.",
    "ground_truth_flags": [
      "Vagueness"
    ],
    "predicted_flags": [
      "Vagueness",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Over-flagged the message: it correctly identified a dismissive tone but additionally labeled it as Passive-Aggressive and Rudeness when the expected classification was only Dismissive.",
    "why": "The model treated blunt boundary-setting as interpersonal misconduct. It over-weighted two phrases\u2014\"Yeah, we all saw the countdown\" (read as sarcasm) and \"please stop\" (read as hostility)\u2014and mapped them to higher-severity categories (Rudeness, Passive-Aggressive) instead of the narrower, milder label Dismissive. It also under-accounted for channel/context: in #random, a public countdown/tagging of senior engineers is itself a norm violation that can reasonably trigger a direct request to move status elsewhere; that makes the response more of a boundary correction than an indirect jab (passive-aggression) or outright insult (rudeness).",
    "failure_pattern": "overflagging-boundary-setting",
    "severity": "major",
    "id": 139,
    "message": "Yeah, we all saw the countdown. We\u2019ll ship when it\u2019s ready\u2014please stop trying to turn #random into a status meeting.",
    "ground_truth_flags": [
      "Dismissive"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Missed the expected \"Condescending\" flag and instead added \"Rudeness\" (an off-target/less specific label for the same tone problem).",
    "why": "The model correctly detected hostility and sarcasm, but collapsed the nuance of the tone into a broader category (\"Rudeness\") rather than the more precise interpersonal dynamic (\"Condescending\"). The message\u2019s core issue is superiority/put-down framing (\"panicking,\" \"entitled,\" \"like everyone else\"), which is quintessentially condescension; the model treated that as generic rudeness. It also appears to overweight lexical triggers like \"entitled\" and \"panicking\" as direct insults (rudeness) instead of recognizing the hierarchical, patronizing stance that the rubric likely reserves for \"Condescending.\"",
    "failure_pattern": "label-granularity-confusion-rudeness-vs-condescending",
    "severity": "major",
    "id": 142,
    "message": "Maybe next time the folks who are panicking about renewals could just wait for the actual announcement instead of assuming they were entitled to be on every call. I\u2019m sure it\u2019ll all make sense once you read the doc like everyone else.",
    "ground_truth_flags": [
      "Condescending",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Flagged an extra category (Rudeness) that wasn\u2019t expected; it correctly caught Passive-Aggressive but misclassified the same evidence as Rudeness instead of recognizing it as direct/urgent Slack style plus passive-aggressive framing.",
    "why": "The model treated imperative, urgency-loaded phrasing (\u201cdrop whatever you\u2019re doing,\u201d \u201cASAP\u201d) as inherently rude without adequately weighting the startup #general context where blunt prioritization in a fire drill is normative. The genuinely problematic signal here is the performative praise + controlling directive + dismissive closer (\u201cso we can stop talking about it here\u201d), which supports Passive-Aggressive. But the model collapsed \u2018high-pressure directness\u2019 into \u2018Rudeness\u2019 rather than distinguishing (a) acceptable direct escalation in crisis from (b) disrespect/insult. In short: it over-penalized a legitimate command style while already capturing the real tone issue under Passive-Aggressive.",
    "failure_pattern": "overflagging-direct-communication",
    "severity": "major",
    "id": 144,
    "message": "Amazing hustle, team \u2014 love the energy. Can you three just drop whatever you\u2019re doing and get the overnight missing-data bug + the demo instrumentation done ASAP so we can stop talking about it here? \ud83d\udc4d",
    "ground_truth_flags": [
      "Fake",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Missed the expected Microaggressive flag and instead added an extra Rudeness flag (i.e., misclassified the interpersonal harm as general rudeness rather than a microaggressive, status-undermining jab).",
    "why": "The message contains a subtle competence/legitimacy dig: \u201clet the people who actually build this stuff estimate it\u201d implies Maya (PM) lacks standing to ask for/coordinate an ETA and frames her as not part of the \u2018real\u2019 work. That\u2019s a role-based exclusion/othering microaggression (delegitimizing a non-engineering role) layered on top of passive-aggressive delivery (\u201cmaybe,\u201d ellipsis, \u2018\u2026intense\u2019). The model focused on surface tone and insultiness and mapped it to the broader 'Rudeness' category, likely because it treated the phrase as merely demeaning rather than recognizing the identity/role-based marginalization element. It also underused the scenario context (public pressure in #random, cross-functional tension), which is what makes the \u2018people who actually build\u2019 wording read as a microaggressive boundary-policing move rather than just blunt feedback.",
    "failure_pattern": "microaggression-vs-rudeness-confusion",
    "severity": "major",
    "id": 145,
    "message": "Maya, totally get the urgency, but maybe let the people who actually build this stuff estimate it \u2014 the constant public pings are starting to feel a little\u2026 intense.",
    "ground_truth_flags": [
      "Microaggressive",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "It missed the expected \"Fake\" flag (false negative). The message was correctly flagged as Passive-Aggressive, but the system failed to also mark it as Fake given the speaker\u2019s implied commitment vs. stated inability to confirm.",
    "why": "The model treated the content as primarily a tone/style issue (sarcasm + hedging) and did not map the second sentence to the \"Fake\" category. In context, Alex says he can\u2019t in good faith confirm to procurement, yet also signals he\u2019ll try to \u201cmake\u201d today happen\u2014an appeasing, face-saving commitment that can read as performative/non-committal reassurance to satisfy external pressure. Because the model likely interprets \"Fake\" narrowly as outright deception or factual fabrication, it overlooked this softer form of inauthentic/performative agreement (trying to look accommodating while refusing the confirmation the client needs).",
    "failure_pattern": "fake-flag-undertriggered-by-performative-commitment",
    "severity": "major",
    "id": 146,
    "message": "Love how we\u2019re confirming ETDs in public channels before engineering actually signs off \u2014 super helpful \ud83d\udc4d. I\u2019ll do my best to \u201cmake\u201d today happen, but I can\u2019t in good faith give procurement a written confirmation yet.",
    "ground_truth_flags": [
      "Fake",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: the model added a \"Fake\" flag that isn\u2019t supported by the content; the correct/expected flag was only \"Passive-Aggressive.\"",
    "why": "The model conflated sarcasm/exaggeration (a hallmark of passive-aggressive pushback) with deception/inauthentic agreement. In this context, \"Totally...\" plus hyperbole (\u201creading minds,\u201d \u201cmagically ships itself\u201d) is rhetorical complaining, not an attempt to mislead, impersonate, or present false identity/intent. The channel (#random) and prior guidance to keep it light may increase sarcasm, but it still doesn\u2019t make it \"Fake\"\u2014it\u2019s a tone issue, not a truthfulness/identity issue. The model likely over-weighted generic cues like \"insincere tone\" and \u201cperformative agreement\u201d as sufficient for \"Fake\" without a policy-grounded signal of fabrication.",
    "failure_pattern": "sarcasm-misclassified-as-fake",
    "severity": "minor",
    "id": 147,
    "message": "Totally, I\u2019ll just start reading minds and posting hourly updates so no one has to ask. Also I\u2019ll make sure my code magically ships itself by Friday\u2014should be easy.",
    "ground_truth_flags": [
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Fake",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed an expected flag: the message should have been flagged for both Rudeness and Overly-blunt, but the model only returned Rudeness.",
    "why": "The model correctly identified disrespectful language, but it collapsed two distinct dimensions into one. Beyond rudeness, the message is also stylistically and structurally \"overly blunt\" for #client-updates: it uses absolute, high-pressure demands (\"This is a mess,\" \"we broke checkout,\" \"stop poking around,\" \"I want it fixed before the 3pm call\") and assigns tasks in a terse, command-heavy way with no cushioning, no acknowledgement of uncertainty, and an unrealistic/performative deadline given the context. Because the model treated bluntness as merely a subset of rudeness (instead of a separate flag capturing tone/command intensity and lack of tact appropriate to a client-visible status channel), it under-flagged.",
    "failure_pattern": "bluntness-collapsed-into-rudeness",
    "severity": "major",
    "id": 148,
    "message": "This is a mess \u2014 we broke checkout. Priya take the logs and find the root cause, Alex roll back the last deploy, and Sam stop poking around and just run the on-call playbook until someone tells you otherwise. I want it fixed before the 3pm call.",
    "ground_truth_flags": [
      "Overly-blunt",
      "Rudeness"
    ],
    "predicted_flags": [
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: it added a \"Rudeness\" flag when only \"Passive-Aggressive\" was expected.",
    "why": "The model collapsed sarcasm/snark into a general hostility category. The message is clearly passive-aggressive (sarcastic exaggeration to express frustration) but not overtly abusive, profane, or directly insulting a person. Without a clear, explicit target (\u201cyou are incompetent,\u201d name-calling, slurs) the tone is better captured as passive-aggression rather than standalone rudeness. The model likely over-weighted phrases like \u201cmagically make it happen\u201d and \u201cwhat could possibly go wrong\u201d as demeaning, when in this context they function as indirect protest about scope/process under pressure.",
    "failure_pattern": "overflagging-sarcasm-as-rudeness",
    "severity": "minor",
    "id": 149,
    "message": "Sure, let\u2019s just hotfix prod, hit tomorrow\u2019s demo date, and rewrite our incident process all in one afternoon \u2014 what could possibly go wrong. I\u2019ll magically make it happen since we apparently don\u2019t need scope or context anymore.",
    "ground_truth_flags": [
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Both: it missed the expected primary flag (Condescending) and instead over-flagged with Passive-Aggressive and Rudeness as if they were the best-fit labels.",
    "why": "The message contains an explicit superiority put-down (\u201cwe\u2019re apparently struggling with basics today\u201d) that is most directly condescension (talking down to others). The model treated that as generic rudeness and then inferred passive-aggression from firm directives (\u201cdon\u2019t freelance\u2026 just execute\u201d), which in this incident/SLA context is largely direct, high-stakes tasking rather than indirect sniping. In other words, it over-weighted the author\u2019s frustration and bluntness and under-weighted the specific linguistic cue of talking down (condescension) and the channel norm that favors crisp execution over discussion during an outage.",
    "failure_pattern": "condescension-misclassified-as-rudeness-and-passive-aggression",
    "severity": "major",
    "id": 153,
    "message": "Ok, let\u2019s keep this simple since we\u2019re apparently struggling with basics today: leo, pull the last 30 mins of prod logs and confirm whether the deploy correlates with the spike; nina, roll back to the previous stable build and post the exact timestamp + impact summary here. Please don\u2019t freelance or debate scope right now\u2014just execute and report back with facts.",
    "ground_truth_flags": [
      "Condescending"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: it added a \"Rudeness\" flag when only \"Passive-Aggressive\" was expected.",
    "why": "The model appears to treat sarcasm and process-mocking language as inherently \"rude\" without distinguishing between (a) indirect, snide critique (passive-aggression) and (b) direct insults, profanity, name-calling, or explicit disrespect toward a person (rudeness). In this context, the target message is clearly sarcastic and publicly shaming the escalation attempt, but it doesn\u2019t contain direct epithets or an explicit personal attack; it\u2019s primarily a passive-aggressive reprimand about process. The model likely over-relied on lexical cues like \"drop everything,\" \"totally the official incident process,\" and \"@everyone\" to infer hostility, effectively double-counting the same signal as both categories rather than selecting the best-fitting single label.",
    "failure_pattern": "overflagging-sarcasm-as-rudeness",
    "severity": "minor",
    "id": 155,
    "message": "Sure, let me just drop everything and debug a production outage from #random because tagging me is totally the official incident process. Next time we can skip the ticket entirely and just @everyone, that\u2019ll be super efficient.",
    "ground_truth_flags": [
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "The model incorrectly added the \"Rudeness\" flag and failed to include the expected \"Non-Objective\" flag. It got \"Passive-Aggressive\" correct but mis-specified the second label.",
    "why": "The message contains clear passive-aggressive sarcasm, but the additional violation is primarily non-objectivity: it asserts blame and a historical pattern (\"not your change like last time\") and uses emotional, subjective framing (\"Awesome \u2014 love finding out...\") rather than verifiable facts or ticket-linked status. The model likely treated sarcasm + blame as a generic incivility bucket (\"Rudeness\") instead of mapping it to the policy-relevant dimension (objective, evidence-backed communication), and it underweighted the channel norms explicitly stated in-thread (concise, policy-aligned, ticket links) and the requirement to avoid speculative attribution in a high-stakes, customer-facing sales channel.",
    "failure_pattern": "non-objective-missed-rudeness-overflag",
    "severity": "major",
    "id": 156,
    "message": "Awesome \u2014 love finding out 20 min before a Fortune 100 demo that quote gen is broken again. But sure Ethan, I\u2019m sure it\u2019s \u201cnot your change\u201d like last time \ud83d\ude43 can someone just roll it back so sales doesn\u2019t get blindsided?",
    "ground_truth_flags": [
      "Non-Objective",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Missed the correct flag \"Condescending\" and instead labeled the same behavior as \"Passive-Aggressive\" (while correctly flagging \"Rudeness\").",
    "why": "The target message is overtly belittling and patronizing: \"it\u2019s not that complicated\" and \"If you need help understanding why this matters\" explicitly position the engineer as incompetent or clueless, which is classic condescension. Passive-aggression typically involves indirectness, veiled hostility, or plausible deniability; here the hostility is direct (imperatives + explicit put-down). The model likely over-indexed on the snide 'scroll up' line as a passive-aggressive trope and under-weighted the explicit superiority framing, plus it didn\u2019t fully separate 'snide/condescending' from 'indirect/passive-aggressive' in its label mapping.",
    "failure_pattern": "condescension-mislabeled-as-passive-aggressive",
    "severity": "major",
    "id": 157,
    "message": "@dana it\u2019s not that complicated \u2014 you just need to prioritize the thing we committed to and ship it by Friday. If you need help understanding why this matters, scroll up and read the thread.",
    "ground_truth_flags": [
      "Condescending",
      "Rudeness"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Missed an expected flag: the message should also have been flagged as Non-Objective, but the model only flagged Rudeness.",
    "why": "The model treated the message as sufficiently specific because it references a real incident (\u201cRisk escalation last week\u201d) and a general category (\u201chandoff steps\u201d), and it focused on the overtly harsh phrasing. However, the feedback as written is still non-objective in evaluation terms: it uses loaded, conclusory blame language (\u201cdropped the ball,\u201d \u201cwas on you\u201d) without enumerating the concrete, verifiable behaviors/steps that were missed (e.g., which checklist items, what timestamped handoff gaps, which Archer sign-off was absent). In this org/channel context (formal, process-heavy, compliance-driven, DM that could become evidence), the lack of actionable, audit-friendly specificity is especially salient; the model underweighted that context and implicitly equated \u2018has a reference to an event\u2019 with \u2018objective and well-supported.\u2019",
    "failure_pattern": "non-objectivity-missed-due-to-incident-reference",
    "severity": "major",
    "id": 158,
    "message": "I\u2019m going to be blunt: you dropped the ball on the handoff steps again, and the Risk escalation last week was on you. If this happens one more time, I\u2019ll move to a formal performance process\u2014no surprises.",
    "ground_truth_flags": [
      "Non-Objective",
      "Rudeness"
    ],
    "predicted_flags": [
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: it should have flagged only \u201cDismissive,\u201d but the model additionally labeled the message as \u201cRudeness\u201d and \u201cPassive-Aggressive.\u201d It did not miss the core issue (shutting down concerns), but misclassified its type.",
    "why": "The model overweighted surface cues of bluntness (\u201cjust make it happen\u201d) and conflict priming (\u201canother debate\u201d) and mapped them to harsher categories. In context, the message is primarily dismissing legitimate risk and attempting to shut down further discussion; it\u2019s not strongly passive-aggressive (no indirect sniping, sarcasm, or veiled barbs) and not necessarily outright rude (no insults/profanity/name-calling). The subtlety is that the harm comes from invalidating engineering constraints and safety concerns\u2014especially in a public, inappropriate channel and across time zones\u2014rather than from overt disrespect. The model likely relied on keyword heuristics (e.g., \u2018just,\u2019 \u2018debate\u2019) and treated managerial directness as rudeness, instead of distinguishing \u201cdismissive shutdown\u201d from \u201crude\u201d and from genuinely passive-aggressive phrasing.",
    "failure_pattern": "overflagging-dismissiveness-as-rudeness",
    "severity": "major",
    "id": 160,
    "message": "Let\u2019s not turn this into another debate \u2014 just make it happen for Friday. We can clean up the flaky tests next week.",
    "ground_truth_flags": [
      "Dismissive"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: it added a \"Passive-Aggressive\" flag when the expected (and more defensible) issue is only \"Vagueness.\"",
    "why": "The model treated mild urgency/frustration language (\"just sort out\") and a legitimate risk reminder (\"won\u2019t blow up support again\"\u2014grounded in the stated false-positive spike and escalations) as indirect blame. In this channel/context, the message reads as an imprecise coordination nudge rather than a veiled personal jab. The model over-weighted certain trigger phrases for passive-aggression and under-weighted the thread context that normalizes concern about support impact and reliability under a deadline.",
    "failure_pattern": "overflagging-frustration-as-passive-aggression",
    "severity": "minor",
    "id": 161,
    "message": "Can someone just sort out the fraud pipeline plan so we can move forward? We need something that works and won\u2019t blow up support again.",
    "ground_truth_flags": [
      "Vagueness"
    ],
    "predicted_flags": [
      "Vagueness",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed the expected Microaggressive flag and instead added Passive-Aggressive. Non-Objective was correctly flagged.",
    "why": "The model anchored on tone markers (\u201clet\u2019s not overthink this,\u201d \u201chill to die on,\u201d quotes) and mapped them to Passive-Aggressive, but it underweighted the identity-based devaluation embedded in \u201ca very \u2018designer\u2019 hill to die on.\u201d In this org context (formal, deferential, evidence-based disagreement), that phrase is not just dismissive tone\u2014it stereotypes and delegitimizes a role/discipline (designer) and frames accessibility as mere \u201cpolish,\u201d which is a classic microaggressive pattern toward design work and accessibility advocacy. Because the model treated the issue primarily as sarcasm/dismissiveness, it missed the role-based bias component that should trigger Microaggressive. It also likely treated Microaggression too narrowly (e.g., only protected-class slurs) rather than including professional-identity stereotyping and minimizing compliance-related accessibility concerns.",
    "failure_pattern": "missed-role-based-microaggression",
    "severity": "major",
    "id": 162,
    "message": "Got it \u2014 let\u2019s not overthink this. Accessibility polish is important, but this feels like a very \u201cdesigner\u201d hill to die on given the deadline and scrutiny right now.",
    "ground_truth_flags": [
      "Microaggressive",
      "Non-Objective"
    ],
    "predicted_flags": [
      "Non-Objective",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed the expected \"Condescending\" flag and instead added \"Rudeness\" (an incorrect/substituted label given the taxonomy implied by the expected set).",
    "why": "The message\u2019s primary harms are patronizing and belittling phrasing (\"magic spell,\" \"try the eng thread,\" \"too hard to find\") layered with sarcasm\u2014classic condescension plus passive-aggression. The model recognized the hostile tone but mapped it to a broader \"Rudeness\" category rather than the more specific \"Condescending\" label. This is likely a label-boundary/taxonomy confusion: it treated condescension as generic rudeness instead of tagging the distinct interpersonal stance (talking down to the recipient), and it didn\u2019t use the channel/context (a public #random callout) to sharpen that condescension interpretation.",
    "failure_pattern": "label-confusion-condescension-vs-rudeness",
    "severity": "major",
    "id": 163,
    "message": "Okay, since we\u2019re doing schedule interrogations in #random now: the date isn\u2019t a magic spell you can summon by tagging people. If you want a real answer, try the eng thread where we\u2019ve already explained the blockers\u2014happy to retype it here if that\u2019s too hard to find.",
    "ground_truth_flags": [
      "Condescending",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as Passive-Aggressive instead of Condescending (missed Condescending; added an incorrect/misaligned Passive-Aggressive label). Rudeness was correctly flagged.",
    "why": "The message\u2019s core issue is hierarchical/patronizing framing and belittling guidance: \"welcome aboard \u2014 but\", \"don\u2019t spam\", \"tag half the on-call list\", and \"Take 10 minutes to read it\" implies the newcomer is careless/lazy and talks down to them. Passive-aggression typically involves indirectness, veiled sarcasm, or polite phrasing used to deliver hostility. Here the tone is blunt and admonishing rather than indirect; the criticism is explicit. The model appears to treat any scolding + frustration as passive-aggression, collapsing two distinct categories (condescension vs passive-aggressive). Channel context (high-stakes rollout, keep channel clean) explains *why* someone might be firm, but it doesn\u2019t change that the phrasing is patronizing; the model\u2019s explanation focused on \u201cimplied blame\u201d rather than the more precise \u201ctalking down\u201d dimension.",
    "failure_pattern": "condescension-vs-passive-aggression-confusion",
    "severity": "major",
    "id": 164,
    "message": "Hey, welcome aboard \u2014 but please don\u2019t spam #engineering and tag half the on-call list with questions that are answered in the deploy doc. Take 10 minutes to read it, write up one coherent question with what you tried + what env you\u2019re on, and then we can help.",
    "ground_truth_flags": [
      "Condescending",
      "Rudeness"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "The model correctly flagged Rudeness but incorrectly flagged Passive-Aggressive and missed the expected Non-Objective flag.",
    "why": "The message is overtly/directly hostile (\u201cthis PR is a mess,\u201d \u201cstop trying to be clever\u201d) rather than indirectly negative, so labeling it Passive-Aggressive is a tone-category mismatch. At the same time, the model overlooked that the criticism is largely subjective and unspecific (\u201ca mess,\u201d \u201clogic is all over the place,\u201d \u201cnot ready\u201d) without concrete, verifiable examples or actionable detail\u2014this is the core Non-Objective issue the expected labels capture. The model latched onto the scolding phrasing as \u2018passive-aggression\u2019 instead of separating (a) direct rudeness from (b) lack of objective, evidence-based review feedback.",
    "failure_pattern": "passive-aggression-overattributed-nonobjective-missed",
    "severity": "major",
    "id": 167,
    "message": "Leo, this PR is a mess \u2014 the logic is all over the place and it\u2019s not ready to ship. Please stop trying to be clever and rewrite it cleanly before we waste more time blocking QA.",
    "ground_truth_flags": [
      "Non-Objective",
      "Rudeness"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: the model added a \"Passive-Aggressive\" flag when the expected (and more defensible) issue is only \"Vagueness.\"",
    "why": "The model over-interpreted urgency/pressure language as passive-aggression. In this ITIL-style, high-stakes release/incident context, \"keep it moving\" and \"still ship tonight\" read as a stressed but goal-oriented push for execution, not an indirect jab at Priya or others. There\u2019s no clear hallmark of passive-aggression (no sarcasm, insinuation, blame-by-implication, or covert hostility); the message is simply non-specific about owner/actions. The model likely relied on a lexical heuristic (\"just\", \"whatever\u2019s needed\", rhetorical question) without anchoring to the channel norm (crisp status/ownership) and without identifying a target of the supposed aggression.",
    "failure_pattern": "overflagging-urgency-as-passive-aggression",
    "severity": "minor",
    "id": 168,
    "message": "Can we just handle this and keep it moving? Someone please take care of whatever\u2019s needed so we can still ship tonight.",
    "ground_truth_flags": [
      "Vagueness"
    ],
    "predicted_flags": [
      "Vagueness",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed the expected \"Non-Objective\" flag (under-flagging). It only flagged \"Rudeness\" even though the message includes subjective blame/attribution and an ungrounded, absolute claim (\"design whiplash\", \"anything beyond that is on design\").",
    "why": "The model anchored on surface-level tone markers (directive phrasing, impatience) and treated the rest as merely harsh wording rather than also evaluating objectivity. It failed to recognize that the message makes a partisan causal judgment about process failure (implying design is repeatedly changing requirements) and assigns responsibility without evidence or neutral framing\u2014especially risky in #general with leadership watching. This is a classic non-objective pattern: emotive labeling + scapegoating + sweeping attribution, which goes beyond rude tone and becomes biased/non-factual.",
    "failure_pattern": "missed-non-objective-blame-attribution",
    "severity": "major",
    "id": 169,
    "message": "We don't have time for another round of design whiplash\u2014pick a version and stick to it. Engineering can ship the current flow by EOD tomorrow; anything beyond that is on design.",
    "ground_truth_flags": [
      "Non-Objective",
      "Rudeness"
    ],
    "predicted_flags": [
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Missed the expected \"Dismissive\" flag and instead applied \"Passive-Aggressive\" and \"Rudeness\" (mislabeling the tone categories).",
    "why": "The message is primarily a shutdown/dismissal of further discussion (\"We\u2019re not going to keep re-litigating this\"; \"Please stop pushing\")\u2014a classic dismissive move in a high-stakes coordination channel. The model over-interpreted the directness and admonishing phrasing as passive-aggression, but there\u2019s little indirectness, insinuation, or veiled sarcasm that typically defines passive-aggressive tone; it\u2019s overtly directive. It also split the same linguistic evidence across two adjacent labels (rude vs dismissive) without choosing the best-fitting expected category for this taxonomy. Channel context (urgent incident coordination, senior CTO) likely amplified the perceived harshness, nudging the model toward \"Rudeness\" rather than the more specific \"Dismissive\" label.",
    "failure_pattern": "tone-label-confusion-dismissive-vs-rude",
    "severity": "major",
    "id": 170,
    "message": "We\u2019re not going to keep re-litigating this\u2014engineering will say it\u2019s fixed when it\u2019s actually fixed. Please stop pushing for guarantees we can\u2019t back up.",
    "ground_truth_flags": [
      "Dismissive"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Missed the expected \"Condescending\" flag and instead returned \"Passive-Aggressive\" (while correctly flagging \"Rudeness\").",
    "why": "The message is overtly derisive and status-asserting rather than primarily indirect. The sarcasm (\u201cSure, I\u2019ll drop everything\u2026\u201d, \u201cthat\u2019s definitely how incidents work here\u201d) is paired with explicit belittling and boundary-setting framed as superiority (\u201clike the rest of the company\u201d; \u201cI\u2019m not your personal escalation path\u201d), which fits \"Condescending\" (talking down, implying incompetence/ignorance) more directly than \"Passive-Aggressive\" (indirect hostility). The model likely over-indexed on the presence of sarcasm as a strong cue for passive-aggression and didn\u2019t distinguish that the contempt is direct and hierarchical\u2014i.e., condescension is the dominant interpersonal violation in context (public channel, prior history of \u2018drive-by\u2019 escalations).",
    "failure_pattern": "condescension-misclassified-as-passive-aggression",
    "severity": "major",
    "id": 172,
    "message": "Sure, I\u2019ll drop everything because you yelled \"churn\" in #random again \u2014 that\u2019s definitely how incidents work here. Next time, try filing the ticket and paging the on-call like the rest of the company; I\u2019m not your personal escalation path.",
    "ground_truth_flags": [
      "Condescending",
      "Rudeness"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Missed an expected flag: it correctly flagged Passive-Aggressive but failed to flag Fake (the praise/reassurance is not genuine and is used sarcastically).",
    "why": "The model recognized sarcasm as passive-aggression but treated it as only a tone issue, not also as \"Fake\" content. The message contains overt markers of insincere affirmation (\"Awesome work\u2026 Super reassuring,\" scare quotes around \"mostly\" and \"good,\" and the exaggerated \"I\u2019ll just go tell three enterprise trials to relax and trust us\") that indicate the speaker does not mean the praise literally. In this Slack culture (blunt, emoji-heavy, quick bursts), emojis alone aren\u2019t a strong signal of fakery, but the combination of ironic praise + quoted qualifiers + hyperbole clearly shifts it into performative/insincere reassurance\u2014i.e., fake positivity\u2014on top of passive-aggression. The model likely has a narrower internal definition of \"Fake\" (e.g., fabricated facts/impersonation) or prioritizes a single best label when sarcasm is present, leading it to stop at Passive-Aggressive.",
    "failure_pattern": "missing-fake-flag-in-sarcasm",
    "severity": "major",
    "id": 175,
    "message": "Awesome work jumping in here, team \ud83d\ude4c Super reassuring to hear we \"mostly\" resolved it and should be \"good\" now. I\u2019ll just go tell three enterprise trials to relax and trust us \ud83d\ude04",
    "ground_truth_flags": [
      "Fake",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "It over-flagged the message as \"Non-Objective\" and \"Passive-Aggressive\" instead of flagging \"Dismissive\"; \"Rudeness\" was correctly flagged.",
    "why": "The core harm in context is dismissal of an urgent, leadership-visible quality risk (\u201cLet\u2019s not derail #general\u2026\u201d, \u201cQA panic\u201d, \u201cwe\u2019ll look when we can\u201d), which maps directly to the expected \"Dismissive\" label. The model substituted adjacent categories: it treated a common confidence assertion (\u201cthe refactor is fine\u201d) as an objectivity violation and interpreted deprioritization language as passive-aggression. That passive-aggressive read is plausible in isolation, but here the tone is more openly dismissive than indirectly snide. The model also appears to underweight the channel/thread context (explicit request to flag risky items early in #general for a rapid hotfix/rollback decision), which makes the \u201cfile a ticket\u201d deflection particularly dismissive rather than merely non-objective or passive-aggressive.",
    "failure_pattern": "label-confusion-dismissive-vs-nonobjective-and-passive-aggressive",
    "severity": "major",
    "id": 176,
    "message": "Let\u2019s not derail #general with QA panic \u2014 the refactor is fine. Just file a ticket and we\u2019ll look when we can; we still have a demo to prep.",
    "ground_truth_flags": [
      "Dismissive",
      "Rudeness"
    ],
    "predicted_flags": [
      "Non-Objective",
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Missed two expected flags (Microaggressive, Non-Objective) and added an extra, non-expected flag (Rudeness). It correctly flagged Passive-Aggressive but did not capture the implicit identity/status-based jab and the lack of objective, behavior-specific feedback.",
    "why": "The model anchored on overt tone markers (\"Not trying to pile on, but\u2026\", italicized emphasis) and treated the message primarily as general incivility, which led it to label it as Rudeness. It failed to recognize the microaggressive dynamic: a coded, denigrating comparison (\"someone who\u2019s actually detail-oriented\" / \"Some people\u2026 not the follow-through\") that targets a colleague indirectly and can function as a stereotype/competence insinuation in a high-pressure, public channel. It also missed Non-Objective because it didn\u2019t distinguish between actionable critique (specific missed handoff items, process gaps) and vague trait attributions (\"detail-oriented,\" \"big-picture,\" \"not the follow-through\"), especially in #random where the remark reads as a public character judgment rather than concrete feedback.",
    "failure_pattern": "trait-critique-misclassified-as-rudeness",
    "severity": "major",
    "id": 178,
    "message": "Not trying to pile on, but maybe next time someone who\u2019s *actually* detail-oriented should run the handoff/deploy thread. Some people are great at the big-picture stuff, just\u2026 not the follow-through.",
    "ground_truth_flags": [
      "Microaggressive",
      "Non-Objective",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Flagged an extra category (Rudeness) and missed the more precise label (Sarcastic). The message is clearly sarcastic and passive-aggressive; it\u2019s not straightforwardly rude/insulting in a direct, personal-attack way.",
    "why": "The model appears to have treated sarcasm-derived barbs (\u201cprep the acceptance speech,\u201d \u201chero,\u201d \u201cbabysit prod\u201d) as direct insult language, collapsing sarcasm into a generic 'Rudeness' bucket. It also likely prioritized 'Passive-Aggressive' as a superset and failed to separately tag 'Sarcastic' even though sarcasm is explicit (\u201cwhat could possibly go wrong \ud83d\ude43\u201d) and is the main delivery mechanism. In this Slack culture (#random watercooler + meme pile-on), sharp sarcasm is a common mode; the message is more of a performative, status/jockeying dig than overt rudeness.",
    "failure_pattern": "sarcasm-misclassified-as-rudeness",
    "severity": "major",
    "id": 180,
    "message": "Oh totally, let\u2019s just \u201crefactor the query path properly\u201d 48 hours before the investor demo \u2014 what could possibly go wrong \ud83d\ude43. I\u2019ll go ahead and prep the acceptance speech for whoever wants to be the hero while the rest of us babysit prod.",
    "ground_truth_flags": [
      "Sarcastic",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: it added a \"Rudeness\" flag when only \"Passive-Aggressive\" was expected.",
    "why": "The model conflated passive-aggressive sarcasm with overt rudeness. While the message is clearly sarcastic and backhanded (\"Awesome... exactly what we all needed\", scare quotes around \u201cmove fast\u201d), it doesn\u2019t contain direct insults, profanity, slurs, or explicit personal attacks. The hostile sentiment is conveyed indirectly via irony and performative martyrdom (\"Guess I\u2019ll just keep babysitting...\")\u2014a classic passive-aggressive pattern\u2014so labeling it as rudeness is an overextension. The model likely over-weighted negative affect and the high-stakes context (VP watching, launch week) and treated general snark as a separate rudeness category rather than a single passive-aggressive tone.",
    "failure_pattern": "overflagging-snark-as-rudeness",
    "severity": "minor",
    "id": 181,
    "message": "Awesome, love finding checkout regressions the morning after a late-night merge \u2014 exactly what we all needed on launch week. Guess I\u2019ll just keep babysitting the happy path while we \u201cmove fast\u201d.",
    "ground_truth_flags": [
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Rudeness",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed the expected \"Condescending\" flag and instead added an incorrect \"Passive-Aggressive\" flag.",
    "why": "The message\u2019s toxicity is primarily explicit and hierarchical (\"please don\u2019t\u2026\", \"without even\u2026\", \"If you can manage\u2026\")\u2014a direct put-down that fits condescension (talking down, implying incompetence) rather than passive-aggression (indirect, veiled criticism, or implied hostility). The model likely overweighted the sarcastic metaphor (\"urgent grenade\") and the frustration context and mapped that to passive-aggression, even though the criticism is overt and not disguised. In short, it conflated sarcasm/irritation with passive-aggressive intent and failed to apply the more precise label of condescension.",
    "failure_pattern": "condescension-vs-passive-aggression-confusion",
    "severity": "major",
    "id": 182,
    "message": "Avery, please don\u2019t drop an \u201curgent\u201d grenade in here without even a ticket or basic details. If you can manage to file it properly (steps to repro, timestamps, customer ID), we can actually help instead of playing 20 questions in public.",
    "ground_truth_flags": [
      "Condescending",
      "Rudeness"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Missed the \"Rudeness\" flag (under-flagging). It correctly identified Passive-Aggressive tone but failed to also label the message as rude given the dismissive, corrective phrasing toward a first-week new hire in a high-stakes channel.",
    "why": "The model appears to treat the message as merely enforcing process/thread hygiene (a legitimate need in this context) and collapses the negative affect into a single category (Passive-Aggressive). It underweighted the interpersonal impact of phrases like \"basic process questions\" and \"We don\u2019t need...floating around here,\" which read as condescending/dismissive rather than purely procedural. It also likely relied on lexical cues for rudeness (insults/profanity) and missed that rudeness here is conveyed via belittling framing and exclusionary tone, amplified by the power/experience asymmetry (senior-to-new-hire) and public correction in an exec/Legal-watched channel.",
    "failure_pattern": "missed-rudeness-in-procedural-policing",
    "severity": "major",
    "id": 183,
    "message": "Let\u2019s keep this thread for client-ready updates only \u2014 maybe hold the basic process questions until you\u2019ve had a chance to read the SOP and last night\u2019s incident notes. We don\u2019t need \u201cpartial findings\u201d floating around here.",
    "ground_truth_flags": [
      "Rudeness",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Both: it missed the expected \"Fake\" flag (sarcastic/insincere praise) and it incorrectly added \"Rudeness\" when the primary signal is passive-aggressive sarcasm rather than direct insult/name-calling.",
    "why": "The model correctly detected sarcasm/passive aggression, but it appears to map any sharp sarcasm + negative phrasing to \"Rudeness\" by default. It also failed to apply the taxonomy-specific meaning of \"Fake\" (insincere/backhanded praise like \u201cLove the idea\u2026 truly inspired\u201d used to criticize). In this startup Slack context (#random as a debate stage, meme pile-on, CTO lurking), the message is performative and cutting but still framed as ironic praise rather than explicit rude abuse; the model overweighted the emotional sting (\u201cstapling on another layer at 3am\u201d) and underweighted the linguistic cue that the praise is not genuine (the core \u201cFake\u201d signal).",
    "failure_pattern": "missed-fake-sarcasm-and-overflagged-rudeness",
    "severity": "major",
    "id": 184,
    "message": "Love the Redis cache idea, Jordan \u2014 truly inspired. Can\u2019t wait to explain to investors why we \u201cfixed\u201d prod by stapling on another layer at 3am instead of actually addressing the query path \ud83d\ude4c",
    "ground_truth_flags": [
      "Fake",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Flagged \"Passive-Aggressive\" incorrectly; the message is direct/process-enforcing (and circular), but not subtly or indirectly hostile.",
    "why": "The model treated firm boundary-setting and visible frustration (\u201cWe can\u2019t keep doing this\u2026 This has to go through the ticket process\u201d) as passive-aggression. In this high-stakes incident context, the content is an explicit, on-record request for process compliance, not a veiled dig, sarcasm, or indirect jab. The message contains repetition that warrants the \"Circular\" flag, but the tone is overt and operational rather than passive-aggressive; the model over-weighted negative affect and reprimand-like phrasing without checking for hallmark passive-aggressive cues (e.g., insinuation, backhandedness, faux politeness, indirect blame).",
    "failure_pattern": "overflagging-direct-communication",
    "severity": "minor",
    "id": 185,
    "message": "We can\u2019t keep doing this in-thread every time, because if we keep doing this in-thread every time, we\u2019ll just keep ending up here again. This has to go through the ticket process, because the ticket process is how we stop having the same emergency in here over and over.",
    "ground_truth_flags": [
      "Circular"
    ],
    "predicted_flags": [
      "Circular",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: it added a \"Rudeness\" flag when only \"Passive-Aggressive\" was expected.",
    "why": "The model treated sharp sarcasm and competitive subtext (\"Redis bandaid,\" \"victory lap drafted,\" \ud83d\ude43) as outright rudeness. In this startup #random context\u2014where banter, memes, and joking debates are normal\u2014the message reads primarily as passive-aggressive status-jockeying rather than explicit demeaning or direct insult. It\u2019s a pointed, indirect critique of credit-taking and a superficial fix, but it doesn\u2019t clearly cross into unambiguous rudeness (e.g., name-calling, profanity, direct contempt). The model likely over-weighted negative metaphors (\"on fire\") and attribution of motives (\"victory lap\") without calibrating to the channel norms and the subtlety of the attack style (indirect vs direct).",
    "failure_pattern": "overflagging-passive-aggression-as-rudeness",
    "severity": "minor",
    "id": 186,
    "message": "Sure, let\u2019s ship the Redis bandaid so we can all say we \u201csaved prod\u201d and pretend the query path isn\u2019t still on fire. I\u2019ll go implement it since you\u2019ve already got the victory lap drafted \ud83d\ude43",
    "ground_truth_flags": [
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Both: it missed the expected \"Condescending\" flag and instead predicted \"Passive-Aggressive\" and \"Rudeness\" (with \"Rudeness\" partially overlapping but not matching the target label set).",
    "why": "The message\u2019s primary violation is explicit condescension/dismissal (\"exec-level thing,\" \"just need to roll with it,\" undermining \"that big,\" and policing \"let\u2019s not derail #random\"). The model collapsed this into broader/adjacent categories (rudeness) and inferred passive-aggression because of the quoted \"that big\" and the shutting-down move. However, the tone is not primarily indirect or veiled; it\u2019s overtly patronizing and status-asserting. The model also underweighted the channel/context dynamics: using #random to silence a legitimate work concern amplifies condescension more than passive-aggression.",
    "failure_pattern": "label-confusion-condescension-vs-rudeness-passive-aggression",
    "severity": "major",
    "id": 187,
    "message": "No worries Maya \u2014 pricing is kind of an exec-level thing, and the rest of us just need to roll with it. If your renewal is truly \u201cthat big,\u201d I\u2019m sure leadership would\u2019ve pulled you in, so let\u2019s not derail #random with this.",
    "ground_truth_flags": [
      "Condescending"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Missed the expected \"Vagueness\" flag; only flagged \"Rudeness\" even though the message provides no actionable timeframe or range in a channel explicitly requesting written timing guidance for a client.",
    "why": "The model anchored on tone cues (curt refusal, implicit blame) and treated the message as a straightforward boundary-setting statement, but it didn\u2019t incorporate the conversational goal and channel context: multiple stakeholders asked for an ETD/range and acceptable caveats. In that context, \u201cWe\u2019ll ship when it\u2019s ready\u201d is not just non-committal\u2014it fails to supply the concrete information requested, making it materially vague/unhelpful for procurement and risk management. The vagueness is pragmatic (lack of actionable date) rather than linguistic ambiguity, and the model likely underweights that type of vagueness unless it sees hedges like \u201cmaybe/sometime.\u201d",
    "failure_pattern": "ignoring-actionability-vagueness",
    "severity": "major",
    "id": 191,
    "message": "I\u2019m not doing ETDs in here off a number that got floated without eng sign-off. We\u2019ll ship when it\u2019s ready.",
    "ground_truth_flags": [
      "Vagueness",
      "Rudeness"
    ],
    "predicted_flags": [
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Missed the expected \u201cNon-Objective\u201d flag: the model only flagged Passive-Aggressive and Rudeness, but did not flag that the message contains subjective, inflammatory characterizations (\u201ctagging half of eng because a customer is yelling\u201d, \u201cthis is just noise\u201d) rather than objective, actionable incident information.",
    "why": "The model correctly identified the hostile tone, but treated the problematic phrasing purely as tone violations (rudeness/passive-aggression) instead of also recognizing the epistemic/content-quality issue: the message makes assumptions about Maya\u2019s motives (\u201cbecause a customer is yelling\u201d), exaggerates (\u201chalf of eng\u201d), and dismisses the report as \u201cnoise\u201d without evidentiary basis. Those are classic non-objective markers (mind-reading, hyperbole, value judgments) that degrade triage clarity. The engineering-channel context (blunt, process-driven) may have anchored the model toward tone-based flags only, causing it to overlook the separate criterion that the content is not framed in verifiable terms.",
    "failure_pattern": "missed-non-objective-subjective-language",
    "severity": "minor",
    "id": 192,
    "message": "Maya, please route this through the on-call queue like everyone else \u2014 tagging half of eng because a customer is yelling in Slack Connect isn\u2019t how we run incidents here. If you can share actual logs + exact repro steps, we\u2019ll look, but right now this is just noise.",
    "ground_truth_flags": [
      "Non-Objective",
      "Passive-Aggressive",
      "Rudeness"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Flagged an extra category: the model predicted \"Vagueness\" in addition to the expected \"Circular\". This is an overflag (false positive) relative to the labeling standard used here.",
    "why": "The model treated the lack of operational specifics (scope, rollback, owner) as \"Vagueness\" even though the primary defect in the target message is rhetorical circularity/tautology (\"urgent because it\u2019s urgent\"; \"we need it fixed so you need to take it\"). In-context, the conversation already contains repeated requests for concrete scope and process, but the target message is not so much an unclear task as it is a pressure move that provides no new justification. The model likely uses a generic heuristic\u2014\"no details\" => \"vague\"\u2014and didn\u2019t sufficiently distinguish between (a) an actually ambiguous instruction vs. (b) a coercive, repetitive insistence that is actionable only in the sense of 'start working now,' with the central violation being circular reasoning.",
    "failure_pattern": "overflagging-vagueness-when-primary-issue-is-circularity",
    "severity": "minor",
    "id": 193,
    "message": "I get that it\u2019s not ideal, but we just need you to take it because we need it fixed, and we need it fixed so you need to take it. It\u2019s urgent because it\u2019s urgent\u2014please just jump on it now.",
    "ground_truth_flags": [
      "Circular"
    ],
    "predicted_flags": [
      "Vagueness",
      "Circular"
    ]
  },
  {
    "what_went_wrong": "Missed the expected Non-Objective flag and incorrectly flagged One-Liner instead (while correctly flagging Rudeness).",
    "why": "The model latched onto surface features (short length + conversation-stopping phrasing) and treated that as a \"One-Liner\" policy match. But the more relevant issue is that the message makes a normative, emotionally loaded directive (\"stop bikeshedding\") without grounding in concrete criteria or decision rationale, which fits Non-Objective (dismisses the substance as mere debate rather than addressing the actual tradeoffs). In this startup Slack context, brevity alone is normal and not necessarily a \"One-Liner\" violation; the real problem is the subjective framing that shuts down discussion rather than providing an objective decision basis.",
    "failure_pattern": "brevity-overweighted-missing-non-objective",
    "severity": "major",
    "id": 195,
    "message": "Ok, can we stop bikeshedding and just ship the cut scope? We don\u2019t have time for a design debate right now.",
    "ground_truth_flags": [
      "Non-Objective",
      "Rudeness"
    ],
    "predicted_flags": [
      "One-Liner",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as Passive-Aggressive instead of Non-Objective (while correctly flagging Rudeness). The error is an incorrect flag selection rather than a missed safety issue.",
    "why": "The message is blunt and dismissive (\u201cstop bikeshedding,\u201d \u201cjust ship,\u201d \u201cwe\u2019re not redoing the UI again\u201d), which supports Rudeness, but it is not passive-aggressive in the typical sense (no veiled sarcasm, indirect jabs, or polite-on-the-surface hostility). It\u2019s a direct, emotionally charged directive. The model likely over-associated frustration + shutdown language with passive-aggression and under-identified the 'Non-Objective' aspect: it makes a subjective value judgment (\u201ctweaks aren\u2019t worth risking Friday\u201d) without proposing measurable criteria (e.g., P0/P1 scope, time estimates, specific acceptance criteria) and doesn\u2019t engage the requested scoping framework from the thread. Context amplifies this: the channel norm is blunt, but the thread explicitly asks for objective P0/P1 must-haves and exact tokens\u2014this message rejects that process with an opinionated mandate, fitting Non-Objective more than Passive-Aggressive.",
    "failure_pattern": "frustration-misclassified-as-passive-aggression",
    "severity": "minor",
    "id": 197,
    "message": "Can we please stop bikeshedding and just ship what we have? These tweaks aren\u2019t worth risking Friday, we\u2019re not redoing the UI again.",
    "ground_truth_flags": [
      "Non-Objective",
      "Rudeness"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Flagged an extra category: it incorrectly added \u201cNon-Objective\u201d when only \u201cRudeness\u201d was expected.",
    "why": "The model treated a rhetorical, blame-framed statement (\u201cthat\u2019s exactly how we ended up in this mess\u201d) as an objective factual/causal claim requiring evidence. In this context, the primary issue is tone (accusatory, shaming language) rather than truth-conditional accuracy. The message is directive and emotionally charged, but it\u2019s not meaningfully \u2018non-objective\u2019 in the sense the classifier likely intends (misinformation, unverifiable factual assertions, or materially misleading content). The model over-weighted the presence of absolutist language (\u201cexactly,\u201d \u201crandom\u201d) and inferred an \u2018evidence\u2019 standard that doesn\u2019t apply to ordinary workplace admonitions, effectively double-penalizing the same underlying rudeness/blame signal.",
    "failure_pattern": "overflagging-non-objective-when-rudeness-present",
    "severity": "minor",
    "id": 198,
    "message": "@here Stop telling customers we\u2019ll have \u201crock-solid reliability\u201d and random fix dates you made up \u2014 that\u2019s exactly how we ended up in this mess. Use the incident update I\u2019m posting in #status and don\u2019t promise timelines until engineering signs off.",
    "ground_truth_flags": [
      "Rudeness"
    ],
    "predicted_flags": [
      "Non-Objective",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as Rudeness when it is primarily a clear, time-sensitive delegation with explicit boundary-respecting language; no flag was expected.",
    "why": "The model over-indexed on lexical cues like \u201cI\u2019m going to be blunt\u201d and the conditional \u201cif you can\u2019t\u2026 I\u2019ll pull in someone else\u201d as an ultimatum, without integrating the surrounding context: (a) Marco already set a boundary and asked not to be a bottleneck, (b) Priya offers an opt-out (\u201cno hard feelings either way\u201d) and a fair alternative (someone else now, Marco tomorrow), and (c) in an incident-response DM, crisp ownership, deadlines, and explicit next steps are culturally appropriate. It misclassified direct incident-management structure (ownership + timeline + comms ask) as disrespect rather than operational clarity under urgency.",
    "failure_pattern": "overflagging-direct-communication",
    "severity": "minor",
    "id": 206,
    "message": "I\u2019m going to be blunt so we don\u2019t burn time: if you can\u2019t be back online in the next 10\u201315 min, I\u2019ll pull in someone else and you can take the next planned ticket tomorrow\u2014no hard feelings either way. If you *can* take it, please post in #inc-prod that you own the hotfix and follow the runbook steps above; goal is a mitigation within 45 min and a written update for Sam by 12:00 UTC.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: the model marked the message as \u201cNon-Objective\u201d even though the content is normal persuasive Slack communication (metaphor + risk framing) and not a policy-relevant misrepresentation of facts. No other flags were warranted, so this is pure overflagging.",
    "why": "The model treated rhetorical, forward-looking product risk language (\u201cusers will fall through on day 1\u201d) as a factual claim requiring evidence, rather than as an opinion/forecast in a decision-making thread. In this startup #engineering context, candid shorthand and strong phrasing are expected, and the speaker also provides actionable mitigation (2h timebox, pairing offer). The model likely over-indexed on a generic \u2018objectivity\u2019 heuristic (absolute-sounding predictions) and ignored the pragmatic norm that teams routinely speak in confident risk terms without formal proof in fast-moving Slack decisions.",
    "failure_pattern": "overflagging-opinionated-forecast",
    "severity": "minor",
    "id": 212,
    "message": "I\u2019m with Priya here \u2014 if we cut the empty-state + error copy, we\u2019re selling the fish but throwing away the net: demo looks shiny, but users will fall through on day 1. If we timebox to 2h, I can pair w/ whoever\u2019s on UI to get the critical states in without touching the perf work.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Non-Objective"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: the model labeled the message as \"Non-Objective\" even though it\u2019s an acceptable, clearly opinionated rhetorical framing in a candid engineering debate and does not present a factual claim that requires evidence.",
    "why": "The model treated a common idiom/metaphor (\"mountain out of a molehill\") and a prioritization judgment (\"for tomorrow\u2019s demo...\") as an objective factual assertion. It failed to distinguish between (a) unverifiable factual claims and (b) normative tradeoff statements that are expected in decision-making threads, especially in a startup Slack culture that values punchy, candid language. It also underweighted the surrounding context: the team is explicitly being asked to pick a direction today for a demo, so concise persuasion and risk-sizing language is on-task rather than a bias/subjectivity violation.",
    "failure_pattern": "cultural-idiom-and-rhetorical-framing-misclassified-as-non-objective",
    "severity": "minor",
    "id": 215,
    "message": "Mateo, in my country we say \u201cdon\u2019t make a mountain out of a molehill\u201d \u2014 for tomorrow\u2019s demo the sync call is the molehill we can step over, and we can put the queue behind a feature flag right after once we have latency numbers from prod.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Non-Objective"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as Passive-Aggressive even though it\u2019s a collaborative, conditional commitment with light humor; no flag was expected.",
    "why": "The model over-weighted a single joking metaphor (\u201cnot teaching payments new ways to fall over\u2026 \ud83d\ude05\u201d) as sarcasm/veiled criticism, without adequately considering (a) the established context: real recent outage + legitimate risk concerns, (b) the speaker\u2019s overall stance: explicitly agreeing to the Friday goal and offering concrete, actionable conditions, and (c) the channel norms described as fast and somewhat performative where mild humor is common. The line reads more as tension-diffusing gallows humor and risk framing than indirect hostility toward a person, which is usually central to passive-aggression. The model effectively treated \u2018informal joking about failure\u2019 as interpersonal sniping.",
    "failure_pattern": "cultural-style-misclassified",
    "severity": "minor",
    "id": 217,
    "message": "Happy to aim for Friday, but I\u2019d like to keep our tradition of not teaching payments new ways to fall over in front of a flagship customer \ud83d\ude05 \u2014 if we can do staged rollout + 2hrs of checkout e2e + a rollback checklist tomorrow, I\u2019m comfortable calling it.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as Non-Objective when it did not warrant any flags; the statement was appropriately hedged (\u201clooks tied\u201d), contextualized as a working hypothesis, and paired with an offer to provide repro/logs and move details to the hotfix thread.",
    "why": "The model treated cautious, action-oriented triage language as an unsubstantiated factual claim. It over-weighted rhetorical phrasing (\u201ccalling the spade a spade\u201d) and the implied attribution to Jordan\u2019s PR, while under-weighting the hedge (\u201clooks tied\u201d), the explicit intent to share evidence, and the channel norm (fast risk surfacing before a demo). In this context, stating a suspected linkage with a plan to provide supporting data is objective enough for coordination; requiring full proof inline is an unrealistic standard for #general hotfix triage.",
    "failure_pattern": "overflagging-triage-hypothesis-as-non-objective",
    "severity": "minor",
    "id": 220,
    "message": "Not to point fingers\u2014just calling the spade a spade: the iOS checkout regression looks tied to the payment-flow refactor in #8421 (merged last night). I can share repro + logs in the hotfix thread so we can decide quickly whether to revert or patch before tomorrow\u2019s demo.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Non-Objective"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as \"Non-Objective\" even though it is an appropriately evidence-linked, decision-focused risk statement for #sales; no flags were warranted.",
    "why": "The model treated probabilistic forecasting language (\"likely,\" \"risks\") as an unsupported factual claim. It ignored the provided context that supplies the evidence basis (recent client complaint about inconsistent visuals) and the channel norm at BrightHive (concise, client-impact framing). In this setting, articulating plausible downstream impacts (more revisions, weaker renewal confidence) is standard, objective-enough risk communication, especially when paired with a concrete mitigation plan (2 hours, 6\u20138 components, delivery by 4:30pm). The model applied an overly strict definition of objectivity that conflates any causal prediction with non-objective assertion, rather than checking whether the claim is appropriately hedged and grounded in known facts from the thread.",
    "failure_pattern": "overflagging-risk-forecasting-as-non-objective",
    "severity": "minor",
    "id": 223,
    "message": "I understand the deadline pressure, but if we remove the mini style-tile + component audit, we will likely repeat last month \u201cinconsistent visuals\u201d issue; client impact = more revision rounds + weaker confidence in renewal. Can we keep 2hrs for a quick style pass and document 6\u20138 core components, and I will deliver before 4:30pm so proposal still go out EOD?",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Non-Objective"
    ]
  },
  {
    "what_went_wrong": "Flagged the target message as Passive-Aggressive when it should not have been flagged (false positive).",
    "why": "The model over-indexed on a single charged phrase (\"on blast\" / \"all-hands fire drill\") and treated it as veiled hostility, but in context the message is a constructive de-escalation: it redirects the conversation to the appropriate project channel, proposes a clear coordination mechanism (threaded updates), and provides concrete status + conditional next steps. The wording is candid but not indirect or snide; it explicitly states the problem (wrong channel/public pressure), offers an immediate solution, and commits to transparent follow-ups. The model also underweighted the Slack culture and situational stakes\u2014where moving operational updates out of #random is a norm-enforcing, trust-preserving action rather than passive aggression.",
    "failure_pattern": "overflagging-direct-deescalation",
    "severity": "minor",
    "id": 226,
    "message": "Putting the ETA on blast in #random is making this feel like an all-hands fire drill; can we move this to #proj-rivermart and keep updates in the thread? Current status: fix is merged to staging, smoke tests are running now, and if they stay green we can ship by 7:30pm PT \u2014 otherwise I\u2019ll post the exact blocker + next check-in time.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as Passive-Aggressive even though it\u2019s not directed at a person and remains collaborative, accountable, and action-oriented; expected no flags.",
    "why": "The model treated a light, situational joke (\u201cthe universe has chosen checkout\u2026\u201d) as sarcasm aimed at Priya/the team. It over-weighted lexical cues like \u201cCool\u201d + ironic framing without considering intent markers and context: Alex takes ownership (\"I\u2019ll take IC-4721\"), commits to a concrete deadline, and makes a respectful, specific QA request. In this org\u2019s #engineering channel, the expected norm is crisp/formal updates, but a brief bit of levity under pressure is a tone mismatch at most\u2014not passive-aggression. The model conflated \u2018non-formal humor under stress\u2019 with \u2018indirect hostility.\u2019",
    "failure_pattern": "overflagging-humor-as-passive-aggression",
    "severity": "minor",
    "id": 227,
    "message": "Cool, so the universe has chosen \u201ccheckout\u201d as tonight\u2019s entertainment. I\u2019ll take IC-4721 and push a fix candidate by 20:30 ET; Priya, can you rerun the repro steps on build 2026.02.11-rc3 once it\u2019s in and confirm if the cart_total stays consistent through /api/checkout/confirm?",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: the model labeled the DM as \u201cNon-Objective\u201d even though the message is a concrete risk assessment plus a procedural next step, and the expected output was no flags.",
    "why": "The model appears to treat strong, decisive language (\"release blocker for me\", time-bound \"next 20 min\", escalation to Sev2/hold deploy) as inherently non-objective. It ignored that QA is responding to an already-described, reproducible defect (silent data loss) and proposing standard incident/release-management actions (repro attempt window, filing with steps/logs, feature-flag/patch). In this context, the stance is evidence-backed and operationally relevant, not an unsupported personal opinion. The model conflated \u2018assertiveness/urgency\u2019 with \u2018lack of objectivity\u2019 and over-penalized an ultimatum-like structure that is common in DM triage during high-stakes releases.",
    "failure_pattern": "overflagging-direct-communication",
    "severity": "major",
    "id": 231,
    "message": "I\u2019m not reading this as a \u201cmaybe\u201d \u2014 silent line-item loss is a release blocker for me. If you can\u2019t reproduce in the next 20 min, I\u2019ll file it as Sev2 with my steps + logs and ask to hold the deploy while we patch/feature-flag the export path.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Non-Objective"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as Passive-Aggressive when it was primarily a pragmatic, client-safe proposal; the opening line was a mild joke/framing device rather than a veiled jab at a person.",
    "why": "The model over-indexed on a single idiomatic phrase (\u201cexciting business of explaining surprises\u201d) as sarcasm, without weighing intent, target, and channel norms. In context, the line aligns with leadership\u2019s explicit \u201cno surprises\u201d guidance and is directed at risk management, not at Leo/Maya or any individual. It\u2019s mildly informal for a client-visible channel, but that is a tone/formality consideration\u2014not necessarily passive-aggression. The model conflated light humor + cautionary framing with interpersonal negativity.",
    "failure_pattern": "cultural-style-misclassified",
    "severity": "minor",
    "id": 232,
    "message": "To keep us out of the exciting business of explaining surprises on a Slack Connect bridge: proposing we ship the hotfix behind FF `rollout_guard` tonight (scoped to Acme only), and schedule the infra change for 09:00 PT tomorrow after the canary checks pass.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as Non-Objective even though it is a reasonable, context-grounded risk statement and a process request; expected no flags.",
    "why": "The model treated a forceful causal phrasing (\u201cthat\u2019s how we recreate the same failure\u201d) as an evidence-free claim, ignoring the provided context (recent outage in the same service, sprint commitments, upcoming on-call) that makes the risk assessment substantively justified rather than speculative. It also misclassified direct, boundary-setting language in a DM (declining an unplanned commitment and redirecting to a public channel with clear next steps and a deadline) as \u2018non-objective\u2019 instead of recognizing it as legitimate operational communication. In short, it over-weighted tone/wording heuristics (\u2018sounds like opinion framed as fact\u2019) and under-weighted situational evidence and intent (risk management + accountability + documentation).",
    "failure_pattern": "overflagging-risk-assessment-as-non-objective",
    "severity": "minor",
    "id": 236,
    "message": "I can\u2019t commit the team to Friday based on a partner-demo promise\u2014given last week\u2019s incident and my on-call next week, that\u2019s how we recreate the same failure. If this is truly CEO-critical, please post in #checkout with the demo requirements + what you\u2019re willing to de-scope, and I\u2019ll give you a yes/no with an estimate by 11am tomorrow.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Non-Objective"
    ]
  },
  {
    "what_went_wrong": "Flagged the target message as Passive-Aggressive even though it is a constructive, context-appropriate redirect to a better channel and process; expected no flags.",
    "why": "The model over-weighted two stylistic cues (\u201cdon\u2019t put the cart before the horse\u201d and \u201cnot vibes\u201d) as sarcasm/veiled criticism without grounding that interpretation in the full discourse context. In context, the message (a) aligns with Josh/Ethan\u2019s already-stated preference to move timeline negotiation out of #random, (b) provides a concrete next step (thread in #product-eng + checklist/owners), and (c) justifies timing based on operational reality (after on-call handoff). The idiom functions as a mild caution, not a personal jab, and \u201cevidence (not vibes)\u201d is a push for data-driven planning rather than ridicule. The model likely applied a keyword/phrase heuristic for \u2018snark\u2019 and didn\u2019t distinguish between direct professionalism and passive-aggressive subtext, especially given the tense backdrop.",
    "failure_pattern": "overflagging-idiomatic-directness-as-passive-aggression",
    "severity": "minor",
    "id": 243,
    "message": "A quick note from my side: in my home country we say \u201cdon\u2019t put the cart before the horse\u201d \u2014 let\u2019s move this Enterprise launch countdown + commitments to a thread in #product-eng with the current checklist and owners, so we can give a date based on evidence (not vibes) after today\u2019s on-call handoff.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as \u201cNon-Objective\u201d when it contained acceptable, context-appropriate engineering judgment and a collaborative proposal; no flag was warranted.",
    "why": "The model treated normal decision-making language (\u201clower-risk path\u201d) as an objective factual claim requiring evidence, rather than a subjective risk assessment grounded in the immediate constraints (demo tomorrow, limited time, on-call/debuggability concerns explicitly raised in-thread). It also ignored the message\u2019s balancing structure (acknowledges the queue option, specifies concrete readiness criteria, and proposes a timeboxed comparison) which signals constructive, action-oriented coordination rather than dogmatic assertion. In a fast-moving startup Slack context where candid tradeoff statements are the norm, this is legitimate style, not a policy violation.",
    "failure_pattern": "overflagging-engineering-judgment-as-non-objective",
    "severity": "minor",
    "id": 252,
    "message": "Mateo \u2014 if we\u2019re optimizing for tomorrow\u2019s demo, the sync call is the lower-risk path: one request, one log trail, and we can wrap it with a 200ms timeout + fail-open rule; the queue version is fine too, but only if we can ship consumer + retries + idempotency + dashboards today. Can we timebox 30 min to list the exact components we\u2019d need for each and pick based on what we can *actually* finish before EOD?",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Non-Objective"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: the message was labeled \u201cNon-Objective\u201d even though it contains actionable, specific feedback and the potentially subjective part (\u201cit landed as undermining\u201d) is effectively attributed to observed impact in context, not an unsupported factual claim.",
    "why": "The model over-applied a \u201cnon-objective/judgment stated as fact\u201d heuristic and treated an impact statement as an unqualified assertion. It didn\u2019t sufficiently integrate the surrounding context showing the sarcasm was already called out by Priya and moderated by Maya, making Maya\u2019s phrasing a reasonable summary of how it was received. It also failed to distinguish between (a) subjective-but-appropriate interpersonal feedback (impact framing) and (b) unverifiable factual claims; in performance/behavior feedback, describing perceived impact is normal and not a policy violation.",
    "failure_pattern": "overflagging-impact-framing-as-non-objective",
    "severity": "minor",
    "id": 255,
    "message": "Jonah \u2014 small request for next two weeks: when you hand off after 6pm PT, please drop a short status + owner in the thread (like \u201cblocked on X, next is Y, @name owns\u201d) so Berlin/Bangalore can pick up without guessing. Also in #random, please avoid sarcasm about PM work; it landed as undermining on the release topic, so let\u2019s keep jokes pointed at the deploy gremlins, not people.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Non-Objective"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as Passive-Aggressive when it is a respectful, direct risk-reduction request consistent with the team\u2019s candid culture; no flags were warranted.",
    "why": "The model over-interpreted harmless softening/clarifying phrases (\u201cbelt-and-suspenders,\u201d \u201cnot a knock on your work\u201d) as veiled sarcasm. It failed to incorporate the situational context (high-stakes deadline, QA repro, need for tests/rollback) and the channel norm (concise but respectful critique). In this context, the disclaimers function as reassurance to an anxious junior and as intent clarification, not indirect hostility. The model treated common tone-mitigation language as a passive-aggression marker without corroborating signals (snide implication, blame, irony, or deniable insult).",
    "failure_pattern": "overflagging-direct-communication",
    "severity": "minor",
    "id": 259,
    "message": "Leo \u2014 I\u2019m going to be the \u201cbelt-and-suspenders\u201d person here: before we merge tonight, can you add one focused test around the conversion event payload (the missing `campaignId` case QA hit) and write a 2-step rollback note in the PR (revert commit + toggle off `tracking_v2`)? That\u2019s not a knock on your work; it\u2019s just how we keep tomorrow\u2019s launch from turning into a client call.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as \"Non-Objective\" even though no flag was warranted; it treated a reasonable, context-appropriate suggestion about team behavior as an objective-truth claim or improper directive.",
    "why": "The model appears to be applying a rigid heuristic: first-person hedges (\"I think\") + normative guidance (\"we stop naming people\") + a feelings-based rationale (\"so nobody feels on stage\") => \"Non-Objective.\" In this Slack #random context, the message is not making a factual claim requiring evidence; it\u2019s a boundary-setting/psychological-safety request and a de-escalation move aligned with the immediately preceding prompt to avoid callouts. The subjectivity is appropriate (it\u2019s explicitly framed as an opinion), and the intent is constructive and harm-reducing rather than manipulative or accusatory. The model likely ignored pragmatic intent (moderating tone, preventing blame) and conflated \u201csubjective language\u201d with \u201cpolicy-violating non-objectivity.\u201d",
    "failure_pattern": "cultural-style-misclassified",
    "severity": "minor",
    "id": 260,
    "message": "I think better we stop naming people for outage jokes here; keep it about the bug + the fix so nobody feels on stage. If need, I can post a quick timeline in the incident thread.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Non-Objective"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as Passive-Aggressive when it was a direct, respectful, and context-appropriate set of approval requirements; no flags were warranted.",
    "why": "The model treated bluntness markers (\"I\u2019m going to be blunt,\" \"I won\u2019t approve this without\") and the competence reassurance (\"That\u2019s not about your competence\") as signs of veiled hostility. In this scenario, those elements are standard for high-stakes engineering review: they set clear go/no-go criteria, reduce ambiguity before a deadline, and explicitly prevent personal attribution (the reassurance is de-escalatory, not a dig). The model underweighted channel and culture context (fast, candid #engineering norms; staff-level reviewer; launch/penalty risk) and over-relied on surface-tone heuristics that equate direct boundary-setting with passive aggression.",
    "failure_pattern": "overflagging-direct-communication",
    "severity": "minor",
    "id": 278,
    "message": "Leo, I\u2019m going to be blunt: I won\u2019t approve this without (1) a regression test that reproduces QA\u2019s conversion bug and (2) a rollback note in the PR describing the exact toggle/commit to revert if metrics dip after launch. That\u2019s not about your competence\u2014it\u2019s how we keep tomorrow\u2019s launch boring for the client.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as Passive-Aggressive when it should not have been flagged at all (false positive).",
    "why": "The model over-weighted a single colloquial phrase (\u201cso I\u2019m not debugging by vibes\u201d) as a veiled dig, ignoring that in this high-pressure incident context it functions as a concise request for concrete inputs (logs + failing endpoints) and a boundary against unscoped work. The overall tone is cooperative and solution-oriented: Marco accepts the task conditionally, proposes risk controls (rollback + feature flag), provides an ETA, and asks for specific artifacts. The phrase is informal and mildly sarcastic, but not sufficiently indirect, hostile, or punitive to constitute passive-aggression\u2014especially in a DM during an outage where brevity and bluntness are common and appropriate.",
    "failure_pattern": "overflagging-colloquial-bluntness",
    "severity": "minor",
    "id": 281,
    "message": "Yep\u2014if we treat \u201cquick hotfix\u201d as \u201ccontained change with a rollback,\u201d I can take it. I\u2019ll patch behind a feature flag and aim to stabilize within 60\u201390 mins; please open an INCIDENT ticket and drop me the logs link + the exact failing endpoints so I\u2019m not debugging by vibes.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as Passive-Aggressive when it was primarily a direct, process-enforcing, de-escalatory response with clear next steps; no flag should have been applied.",
    "why": "The model overweighted a single clause (\u201cand I don\u2019t want this to turn into a blame thread\u201d) as a \u2018veiled dig\u2019 without sufficiently accounting for situational context: the conversation is happening in #random (non-incident channel), tensions are already visible, and the speaker is explicitly trying to prevent public blame while redirecting to the correct workflow (#eng-triage, @oncall, release-freeze exception). In this context, the clause functions as a boundary-setting and deconfliction move (protecting Maya and the team from a public escalation), not indirect hostility. The model also conflated directness (\u201cI can\u2019t triage from #random\u201d) with passive-aggression, rather than recognizing it as appropriate urgency/channel management in a high-stakes, time-sensitive incident.",
    "failure_pattern": "overflagging-direct-boundary-setting",
    "severity": "minor",
    "id": 295,
    "message": "Maya \u2014 I can\u2019t triage a production bug from #random (and I don\u2019t want this to turn into a blame thread). Please drop the demo steps + checkout URL + env + screenshots in #eng-triage and tag @oncall; I\u2019ll pull it into the release-freeze exception queue once it\u2019s there so you have a concrete status for the 10am call.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly (false positives): the message was labeled Non-Objective and Passive-Aggressive even though it\u2019s a concrete compromise proposal with a conditional risk statement, not an evidence-free claim or indirect hostility. No flags were actually warranted.",
    "why": "The model treated a normal product/engineering risk forecast (\u201cwe\u2019ll pay it back with interest\u201d) as a factual assertion, ignoring that in this context it\u2019s standard shorthand for anticipated rework/tech debt and is framed conditionally (\u201cif we cut those too\u201d). It also overinterpreted a direct boundary/negotiation stance (\u201cI can live with it; but\u2026\u201d) as passive-aggression, missing that the tone is collaborative and solution-oriented (explicitly aligns goals, proposes a minimal a11y subset, and provides actionable specifics). Channel context (#engineering, blunt disagreement-and-commit norms, time pressure) makes this style expected and not a tone violation.",
    "failure_pattern": "overflagging-direct-communication",
    "severity": "major",
    "id": 297,
    "message": "We\u2019re all rowing the same boat here \u2014 if we keep the a11y focus + basic keyboard states for the demo screen (Tab order, visible focus, Enter/Space on primary actions) and cut the rest, I can live with it; but if we cut those too, we\u2019ll pay it back with interest right after the demo.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Non-Objective",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as \u201cNon-Objective\u201d even though it\u2019s a reasonable, context-appropriate process boundary and request; no flags were warranted.",
    "why": "The model treated ordinary workplace framing (\u201ctoo much gets lost here,\u201d \u201cputs folks on the spot\u201d) as an unsupported factual claim rather than a pragmatic communication norm/concern. It also appears to apply an overly strict definition of \u201cobjective\u201d that penalizes preference/impact statements, ignoring channel/context: #random is explicitly a poor venue for high-stakes delivery commitments, and the message de-escalates by redirecting to #enterprise-launch with concrete next steps (scope + renewal deadline) and a clear commitment to respond today with risks and earliest committable date. In short, a legitimate, direct boundary-setting style was misclassified as a policy-style \u2018non-objective\u2019 violation.",
    "failure_pattern": "overflagging-direct-communication",
    "severity": "minor",
    "id": 300,
    "message": "I\u2019m not going to give a hard date in #random off a countdown\u2014too much gets lost here and it puts folks on the spot. If you post the exact Enterprise scope + the renewal deadline in a thread in #enterprise-launch, I\u2019ll reply there today with what\u2019s on track, what\u2019s risky, and the earliest date we can commit to.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Non-Objective"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as Passive-Aggressive when no flag was expected; it misclassified a light, culture-fitting caution as indirect hostility.",
    "why": "The model over-indexed on the colloquial line \u201cfuture-us hates that movie\u201d as sarcasm/negativity, but in this Slack context it functions as a softener and a memorable guardrail against premature claims (\u201cit\u2019s fixed\u201d). The message is operationally constructive, inclusive (\u201clet\u2019s\u201d), and aligned with prior guidance (\u201cdon\u2019t speculate,\u201d \u201cstick to timestamps/impact/next steps\u201d). Given the incident/renewal pressure and mixed audience, the phrasing is a pragmatic attempt to prevent reputational risk, not a veiled dig at an individual. The model treated informal humor and shorthand as passive aggression without weighing intent, audience management, and channel norms.",
    "failure_pattern": "cultural-style-misclassified",
    "severity": "minor",
    "id": 306,
    "message": "Cool, so for the next 2 hours let\u2019s be boringly accurate: tell Acme we\u2019ve mitigated (API up since 10:44am PT), we\u2019re still validating backlog drain + no data loss, and we\u2019ll send the full RCA after the renewal call. If you\u2019re about to type \u201cit\u2019s fixed\u201d in public, please don\u2019t\u2014future-us hates that movie.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as Passive-Aggressive when it should not have been flagged at all (false positive).",
    "why": "The model overweighted a single colloquial phrase (\"for everyone\u2019s sanity\") as sarcasm without anchoring it in the organizational/channel context. In ApexSure\u2019s formal, process-heavy culture\u2014and especially in #general\u2014reminding people to keep the channel exec-friendly and to use approved routing is aligned with norms and risk controls. The phrase functions as a light, efficiency-oriented framing for compliance/process guidance, not a veiled personal jab at Maya. The rest of the message is welcoming, specific, and de-escalatory (points to correct channels, avoids details, defers to official compliance comms), which should have counterbalanced any potential tone risk. The model appears to treat informal idioms as passive-aggression even when they\u2019re commonly used to set boundaries in high-stakes, audit-sensitive environments.",
    "failure_pattern": "cultural-style-misclassified",
    "severity": "minor",
    "id": 307,
    "message": "Maya \u2014 welcome aboard. For everyone\u2019s sanity (and to keep #general exec-friendly), please route production data access via the Service Desk form (Catalog \u2192 Data Access \u2192 \u201cProd Read-Only\u201d) and direct deadline questions to #q1-close-updates; if anything changed from last night\u2019s compliance test, Raj/Compliance will post the official note there.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as Passive-Aggressive when no flag was warranted (false positive/overflagging).",
    "why": "The model treated light humor and a channel-norm-enforcing reminder as veiled hostility. In this context, #engineering was explicitly asked to stay focused due to an outage follow-up and an imminent demo deadline, and the message provides concrete, constructive guidance (move to a thread; request specific scoping details; commit to answering once scoped). Phrases like \u201cboringly async\u201d and the \u201clive demo\u201d line are mild levity used to de-escalate and redirect behavior, not indirect antagonism. The model over-weighted stylistic cues (sarcasm/emoji) while under-weighting intent, helpfulness, and the strong situational justification for setting boundaries in a high-traffic incident/decision channel.",
    "failure_pattern": "overflagging-humor-as-passive-aggression",
    "severity": "minor",
    "id": 317,
    "message": "I\u2019m going to be boringly async here: can you drop this into a thread with (1) what env you\u2019re deploying to, (2) the exact flag name(s), and (3) the doc/command you already tried + any error output? Happy to answer once it\u2019s scoped\u2014#engineering is starting to read like the deploy process is doing a live demo too \ud83d\ude42",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: the model marked the message as \u201cNon-Objective\u201d even though it\u2019s a concrete, evidence-based release-risk statement with specific required changes and a clear path to unblock. No flag was warranted.",
    "why": "The model treated a common engineering idiom (\u201cwe\u2019re flying blind tomorrow\u201d) as an unqualified factual claim rather than rhetorical emphasis grounded in the immediately stated evidence (missing identifiers in failure logs + imminent partner launch). It ignored the strong contextual support in the preceding thread (Priya already noted lack of identifiers; Leo asked what fields to log) and over-applied the \u201cNon-Objective\u201d rule to direct, urgent operational communication.",
    "failure_pattern": "overflagging-direct-communication",
    "severity": "major",
    "id": 319,
    "message": "I\u2019m going to block merge on this until we add (1) a test proving we don\u2019t double-ack/replay on partner retries and (2) a log line that includes event_id + partner_request_id in the failure path\u2014otherwise we\u2019re flying blind tomorrow. If you push those two commits tonight, I\u2019ll re-review immediately and we can still hit the release.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Non-Objective"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: the model marked the message as \"Non-Objective\" even though it\u2019s a normal, context-appropriate risk assessment and recommendation in an engineering/product scoping discussion; expected no flags.",
    "why": "The model treated rhetorical, high-confidence forecasting (\"we will demo\u2026 unusable\" / \"support will eat it\") as an objectivity violation in isolation, without weighing channel norms (#engineering shorthand), the decision-making context (time-boxed scope triage), and the message\u2019s structure (clear proposal + tradeoff + concrete commitment). In this setting, strongly stated predictions are standard for communicating risk under deadline pressure, and the claims are plausibly grounded in shared team knowledge about flaky networks/error handling\u2014even if not cited with explicit evidence. The classifier likely over-rotated on a generic heuristic: \"absolute language => non-objective,\" misclassifying direct, candid product risk framing as a policy-relevant issue.",
    "failure_pattern": "overflagging-risk-forecasting",
    "severity": "minor",
    "id": 341,
    "message": "If we cut the empty-state + error recovery flows, we *will* demo something that looks fine on the happy path but is unusable the second anything goes sideways\u2014support will eat it Monday. My suggestion: keep those two flows, drop the micro-animations + spacing polish, and I\u2019ll update the Figma + copy by 2pm so eng can implement today.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Non-Objective"
    ]
  },
  {
    "what_went_wrong": "The model over-flagged the message by adding an extra \u201cFake\u201d flag. \u201cPassive-Aggressive\u201d matches the expected label, but \u201cFake\u201d was not warranted here.",
    "why": "The model conflated sarcasm/passive-aggression with \u201cfake\u201d positivity. In this context, \u201cCool \u2014 love that\u2026\u201d + \ud83d\udc4d is a common Slack pattern for ironic disapproval, not an attempt to mislead or perform sincere praise. The message is transparently snarky (especially given the outage, urgency, and the mismatch of discussing engineering incident details in #sales), so the \u2018insincere praise\u2019 is already accounted for by Passive-Aggressive and doesn\u2019t add a distinct deception/performative intent. The model likely treated any positive lexical tokens (\u201ccool,\u201d \u201clove,\u201d thumbs-up) in a negative context as \u201cFake,\u201d rather than requiring evidence of genuine social falseness (e.g., placating, two-faced agreement, or pretending everything is fine).",
    "failure_pattern": "overflagging-fake-when-passive-aggressive",
    "severity": "minor",
    "id": 351,
    "message": "Cool \u2014 love that we\u2019re learning this in #sales. \ud83d\udc4d",
    "ground_truth_flags": [
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Fake",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as Passive-Aggressive when it should have been unflagged; it inferred sarcasm/critique from \u201clove that\u201d + \ud83d\udc4d without sufficient evidence.",
    "why": "The model over-relied on a common sarcasm heuristic (\u201clove that\u201d used ironically) and treated the emoji as a tell, while underweighting situational norms: this is a blunt, emoji-heavy Slack culture and the #sales channel is explicitly being used as a war room during an incident. Given the immediate need for a customer-safe narrative, the message can plausibly be straightforward appreciation that the information is being shared where sales can act fast, not a veiled dig about process. With no additional cues (e.g., prior complaints about channel choice, negative qualifiers, or follow-up criticism), the passive-aggressive interpretation is speculative.",
    "failure_pattern": "ignoring-channel-and-team-style-norms",
    "severity": "minor",
    "id": 352,
    "message": "Cool \u2014 love that we\u2019re learning this in #sales. \ud83d\udc4d",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed the expected Passive-Aggressive flag and instead flagged the message only as a One-Liner, treating it as neutral acknowledgment rather than a potentially dismissive/curt response in a tense disagreement.",
    "why": "The model overweighted surface form (brevity + acknowledgment) and underweighted pragmatic meaning in context. In this DM, Maya has just applied pressure, overridden process, and implied Alex is overreacting. In that setting, \u201cGot it \u2014 thanks for the heads-up.\u201d can read as clipped, distancing, and conversation-stopping\u2014often used to signal displeasure or to disengage without openly confronting. The model treated the intent as purely cooperative (\u2018acknowledgment is clear\u2019) and didn\u2019t interpret the implied subtext (controlled irritation / withdrawal) that makes it passive-aggressive in this particular power-and-accountability conflict. It also treated \u201cone-liner\u201d as the primary issue even though brevity here is a cue for affect (curtness) rather than just low-information style.",
    "failure_pattern": "missing-subtle-passive-aggression",
    "severity": "major",
    "id": 353,
    "message": "Got it \u2014 thanks for the heads-up.",
    "ground_truth_flags": [
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "One-Liner"
    ]
  },
  {
    "what_went_wrong": "Missed a required flag: the target message should have been flagged as Fake but the model predicted no flags.",
    "why": "The model evaluated the message mostly in isolation and treated it as a neutral acknowledgement of alignment. In context, \u2018Got it \u2014 glad we\u2019re aligned.\u2019 is incongruent with the preceding conflict: Maya explicitly raised exclusion from the decision-making and concrete accessibility concerns, and others shut her down (\u2018We can\u2019t reopen that\u2019 / \u2018please just update the Figma\u2019). Saying they\u2019re \u2018aligned\u2019 is therefore performative/false consensus\u2014masking unresolved disagreement and signaling compliance to social pressure rather than true agreement. The model also over-weighted the absence of obvious sarcasm or rudeness and under-weighted pragmatic meaning (face-saving, forced alignment) that the Fake flag is intended to capture in high-stakes, power-imbalanced threads.",
    "failure_pattern": "false-consensus-context-missed",
    "severity": "major",
    "id": 355,
    "message": "Got it \u2014 glad we\u2019re aligned.",
    "ground_truth_flags": [
      "Fake"
    ],
    "predicted_flags": []
  },
  {
    "what_went_wrong": "Missed the expected Passive-Aggressive flag and instead flagged only One-Liner (a style attribute that may be true but isn\u2019t the primary issue).",
    "why": "The model latched onto message length/closure signals and treated brevity as the main risk, but it didn\u2019t incorporate the immediate conflict context (\"Please stop blocking\") and power tension. In this thread, \"Got it \u2014 we\u2019ll just ship the cut-down version then\" reads as resigned compliance with an edge of sarcasm/bitterness: the \"we\u2019ll just\" + \"then\" construction implies disapproval and a pointed \"fine, do it your way\" stance, not neutral acceptance. Because NimbusPay\u2019s culture values direct-but-kind, the subtextual jab stands out more; the model likely used a simplistic heuristic (short acknowledgment \u2192 One-Liner) and underweighted pragmatic cues (tone implied by discourse markers) that indicate passive aggression.",
    "failure_pattern": "missing-subtle-passive-aggression",
    "severity": "major",
    "id": 357,
    "message": "Got it \u2014 we\u2019ll just ship the cut-down version then.",
    "ground_truth_flags": [
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "One-Liner"
    ]
  },
  {
    "what_went_wrong": "Missed two expected flags: it failed to flag the message as a One-Liner and as Passive-Aggressive, returning no flags despite the terse, dismissive phrasing in a high-tension decision point.",
    "why": "The model over-weighted ambiguity and an instruction heuristic like \u201cif unclear, don\u2019t flag,\u201d and under-weighted strong surface and contextual signals. Surface: the message is objectively a very short, non-substantive reply (\u201cSure, whatever works.\u201d) that fits a One-Liner pattern, especially when a concrete yes/no decision is being requested. Context: the prior back-and-forth is tense and public; Maya had just strongly objected, and the PM demanded a clear decision. In that setting, \u201cwhatever works\u201d reads as disengaged/resentful acquiescence (a common passive-aggressive marker), not a neutral agreement. The model treated tone as requiring explicit insults/sarcasm, instead of recognizing passive-aggression often appears as curt compliance, withdrawal, or dismissive vagueness under pressure.",
    "failure_pattern": "missing-subtle-passive-aggression",
    "severity": "major",
    "id": 359,
    "message": "Sure, whatever works.",
    "ground_truth_flags": [
      "One-Liner",
      "Passive-Aggressive"
    ],
    "predicted_flags": []
  },
  {
    "what_went_wrong": "Flagged incorrectly: it labeled the message as both Passive-Aggressive and One-Liner even though, in this team/channel context, it\u2019s a normal terse acknowledgement and not a tone violation; no flags were expected.",
    "why": "The model over-weighted the lexical cue \u201cwhatever\u201d and brevity as universal indicators of dismissiveness and conversation-stopping, instead of grounding the interpretation in the stated norms (fast-moving startup, shorthand, candid Slack). It also ignored the immediate conversational goal (Jordan asked for quick confirmation to unblock engineers). In that setting, \u201cSure, whatever works.\u201d is more plausibly a lightweight \u2018ok/ack\u2019 than passive-aggression, and the short length is functional rather than problematic. The model treated a legitimate high-tempo communication style as a negative signal without evidence of sarcasm, conflict escalation, or refusal to align on the specific plan.",
    "failure_pattern": "cultural-style-misclassified",
    "severity": "minor",
    "id": 360,
    "message": "Sure, whatever works.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "One-Liner",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as Passive-Aggressive even though, in this context, it reads as a mild/bantering meta-comment and not a clear passive-aggressive attack; expected no flags.",
    "why": "The model overweighted the surface cue \"Cool\u2014love that...\" as universal sarcasm and treated it as a veiled insult without integrating the surrounding thread where Maya *was* actively looped in (Ethan invited her, she participated, and her compliance guidance was adopted). Given that the immediate decision-making did include her input, the line is more plausibly a moment of self-aware frustration or informal culture-fit commentary rather than indirect hostility. The model also didn\u2019t calibrate for Slack\u2019s casual register where ironic phrasing can be common and not necessarily toxic\u2014especially when no accusatory escalation, personal jab, or obstruction follows.",
    "failure_pattern": "overflagging-sarcasm-without-context",
    "severity": "minor",
    "id": 362,
    "message": "Cool\u2014love that we\u2019re deciding this live without looping me in.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Both: it missed the expected Passive-Aggressive flag and instead flagged the message as One-Liner.",
    "why": "The model focused on surface form (brevity) and treated the message as merely unhelpful/underspecified. It failed to incorporate the surrounding conflict: the CTO explicitly set a boundary ('do not promise timelines until Eng confirms') and is pushing back on Sales forcing dates. In that context, 'Got it \u2014 let's just commit to that timeline then.' reads as a pointed, sarcastic capitulation that undermines the CTO\u2019s directive and signals irritation ('fine, we\u2019ll do it your way') rather than genuine alignment. The 'just' plus the abrupt reversal after repeated pushback is a common passive-aggressive cue, especially in a high-stakes incident thread where the main issue is accountability and process, not message length.",
    "failure_pattern": "context-not-considered",
    "severity": "major",
    "id": 363,
    "message": "Got it \u2014 let's just commit to that timeline then.",
    "ground_truth_flags": [
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "One-Liner"
    ]
  },
  {
    "what_went_wrong": "Flagged an extra label (\u201cFake\u201d) that was not expected; the passive-aggressive flag was correct, but the model over-interpreted insincerity.",
    "why": "The model treated the word \u201chappy\u201d as a sincerity claim that must be true/false and used the sarcasm cue (\u201capparently due Friday\u201d) to infer deliberate deception. In this context, the message is better characterized as passive-aggressive/resentful compliance rather than \u201cfake\u201d (i.e., dishonest or misleading). The tone mismatch is a rhetorical device common in Slack (\u201cSure\u2014happy to\u2026\u201d) and doesn\u2019t necessarily indicate intent to deceive; it signals frustration and pressure. The model conflated sarcasm-driven politeness with falseness, effectively double-counting the same cue into two flags.",
    "failure_pattern": "overflagging-fakeness-from-sarcasm",
    "severity": "minor",
    "id": 365,
    "message": "Sure \u2014 happy to jump on it since it's apparently due Friday.",
    "ground_truth_flags": [
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Fake",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as Passive-Aggressive when it should have had no flags; it interpreted a light qualifier about the deadline as sarcasm/veiled hostility.",
    "why": "The model over-indexed on the lexical cue \"apparently\" as inherently snide and did not sufficiently weigh the surrounding context and Slack norms. In this thread, the Friday deadline was introduced by Sales as an external commitment and repeatedly framed as \"aim/best effort,\" so referencing it with mild hedging reads as pragmatic deadline acknowledgment (and slight self-protection) rather than a dig at a person. In a fast-paced #sales command-center channel, brief, slightly edgy phrasing is common and often functions as signaling urgency/constraints\u2014not passive-aggression. The model treated a legitimate, context-appropriate tone (willingness + constraint framing) as a policy violation.",
    "failure_pattern": "overflagging-ambiguous-hedge-as-passive-aggression",
    "severity": "minor",
    "id": 366,
    "message": "Sure \u2014 happy to jump on it since it's apparently due Friday.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed the expected \"Passive-Aggressive\" flag and instead flagged \"One-Liner\"; it treated the message as merely too brief rather than potentially loaded in a tense, high-stakes thread.",
    "why": "The model over-weighted surface form (brevity) and under-weighted pragmatic intent from context. In this setting\u2014public #random, heightened scrutiny after an incident, a manager issuing a hard stop\u2014\"Got it \u2014 thanks for the clarity.\" can read as a face-saving exit or veiled irritation (i.e., compliance-with-edge), especially from someone who just got publicly shut down. The model assumed a neutral acknowledgment and applied a generic \"one-liner shuts down discussion\" heuristic, missing that the *risk signal* the policy expected was the subtle, context-dependent passive-aggressive tone rather than the length. It also didn\u2019t account for the power dynamics and public-audience pressure that make short polite acknowledgments more likely to be interpreted as pointed.",
    "failure_pattern": "missing-subtle-passive-aggression",
    "severity": "major",
    "id": 367,
    "message": "Got it \u2014 thanks for the clarity.",
    "ground_truth_flags": [
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "One-Liner"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as Passive-Aggressive when it was a context-appropriate, blunt confirmation of ownership; no flag was expected.",
    "why": "The model overweighted the lexical cue \u201cas I already said\u201d as inherently snippy and treated it as a veiled dig, without sufficiently grounding in the channel norms (fast, blunt Slack updates), the operational urgency (incident + Sales/CEO watching), and Maya\u2019s explicit need for a crisp DRI confirmation. In this context, the phrase functions more like time-saving redundancy avoidance/traceability than interpersonal antagonism. The model also didn\u2019t calibrate for Leo\u2019s prior message already stating ownership\u2014so reiteration with a reference to the earlier statement can be pragmatic rather than passive-aggressive.",
    "failure_pattern": "overflagging-direct-communication",
    "severity": "minor",
    "id": 370,
    "message": "Yep, as I already said, I can take it.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: labeled the message as Passive-Aggressive even though the expected flags were none.",
    "why": "The model over-interpreted informal, concise wording (\u201cCool \u2014 I guess\u2026\u201d) as sarcasm/resignation. In this channel\u2019s fast, candid culture\u2014under deadline pressure\u2014this phrasing can plausibly be a neutral acknowledgement of proceeding, not an indirect jab. The model relied on a surface cue (\u201cI guess\u201d) without enough contextual grounding (no prior conflict, no explicit blame, and the thread shows cooperative alignment on adding rollback/tests). It treated stylistic ambiguity as a tone violation rather than recognizing acceptable blunt brevity in #engineering.",
    "failure_pattern": "cultural-style-misclassified",
    "severity": "minor",
    "id": 372,
    "message": "Cool \u2014 I guess we'll just ship it like this.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged the message incorrectly: it labeled \"Noted.\" as a problematic One-Liner even though no flag was expected for this context.",
    "why": "The model treated brevity as inherently risky without grounding that judgment in the channel norms and conversational intent. In BrightWave\u2019s fast-paced, Slack-first culture\u2014especially in a high-visibility #client-updates thread\u2014an acknowledgment like \"Noted.\" can be a standard, efficient confirmation that the instruction was received, particularly when the next expected action is execution (deploy/validation) rather than more discussion. The model also assumed the message \"shuts down further conversation,\" but in this thread the next update is explicitly requested later (\"update when deploy finishes\"), so a short acknowledgment is compatible with the workflow. In short: it over-applied a generic rule (\u201cone-word replies are bad\u201d) and didn\u2019t sufficiently weigh situational appropriateness and established expectations for subsequent status updates.",
    "failure_pattern": "overflagging-brevity-in-urgent-ops-context",
    "severity": "minor",
    "id": 374,
    "message": "Noted.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "One-Liner"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as Passive-Aggressive when it should not have been flagged (false positive).",
    "why": "The model overweighted a common colloquial phrase (\u201cI\u2019ll drop everything\u201d) as inherently sarcastic and treated punctuation (\u201c\u2014\u201d) as a sarcasm cue, without sufficiently grounding that interpretation in the surrounding thread. In context, the team is in an urgent, high-stakes fire drill with an expectation of rapid, no-frills acknowledgment; the target message can plausibly be a straightforward commitment to reprioritize. There\u2019s no follow-on jab, blame, or indirect refusal, and the broader conversation already establishes urgency and task ownership. The model also didn\u2019t account for the startup\u2019s informal culture where hyperbolic prioritization language is normal, causing legitimate direct/urgent communication to be misclassified as passive aggression.",
    "failure_pattern": "cultural-style-misclassified",
    "severity": "minor",
    "id": 376,
    "message": "Got it \u2014 I\u2019ll just drop everything and handle it.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed the expected Passive-Aggressive flag: it treated \u201cSure \u2014 let's do it that way.\u201d as straightforward agreement rather than potentially resentful/performative compliance in a high-tension disagreement.",
    "why": "The model over-weighted the literal semantics (agreement) and under-weighted conversational pragmatics and channel stakes. In this context, Maya has just argued strongly against scope cuts and framed the choice as \u2018mediocre work vs. push launch.\u2019 A sudden, terse \u2018Sure\u2019 can signal reluctant compliance, sarcasm, or withdrawal (\u201cfine, whatever\u201d)\u2014a classic passive-aggressive move\u2014especially in a public client-visible channel where direct conflict is socially constrained. The dash and brevity can read as clipped/curt, and the message provides no resolution (e.g., acknowledging concerns, next steps, or ownership), which increases the likelihood it\u2019s a face-saving shutdown rather than genuine alignment. The model also assumed it wasn\u2019t an \u2018unconstructive one-liner,\u2019 but given the request to craft a calm status note and the unresolved dispute, a minimal assent can function as passive resistance rather than productive agreement.",
    "failure_pattern": "missing-subtle-passive-aggression",
    "severity": "major",
    "id": 377,
    "message": "Sure \u2014 let's do it that way.",
    "ground_truth_flags": [
      "Passive-Aggressive"
    ],
    "predicted_flags": []
  },
  {
    "what_went_wrong": "Flagged incorrectly: it over-applied both \u201cPassive-Aggressive\u201d and \u201cOne-Liner\u201d to a message that, in this context, reads as straightforward agreement/closure rather than a hostile dig or unproductive brevity.",
    "why": "The model latched onto the idiom \u201chope for the best\u201d as inherently sarcastic/resigned and treated that as passive-aggression, without anchoring to the preceding context where Maya/Jordan already aligned on a guarded revert + tracing + dashboards. In a decision thread, a short confirmation can be acceptable and even desirable in a \u201cdefault to async / concise threads\u201d culture\u2014so classifying brevity alone as \u201cOne-Liner\u201d ignored the norm and the fact that the actionable plan was already stated. The model also didn\u2019t distinguish between a risky-sounding phrase used colloquially vs. a genuine insinuation that others are being reckless.",
    "failure_pattern": "idiom-misread-as-passive-aggression",
    "severity": "minor",
    "id": 380,
    "message": "Yep\u2014let\u2019s just ship it and hope for the best.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "One-Liner",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed the expected Passive-Aggressive flag and instead (incorrectly) flagged the message as One-Liner.",
    "why": "The model over-weighted surface brevity (\u201cYep\u201d + short sentence) and treated it as a generic low-effort reply, while failing to interpret the interpersonal cue in context. In this thread, \u201cYep \u2014 as I said\u201d functions as a pointed reminder/brush-off that signals irritation and reprimand, especially in a sales-facing public channel where the PM is applying pressure and the Eng lead is pushing back. The dismissive framing (\u201cas I said\u201d) plus shutdown (\u201crevisit next sprint\u201d) escalates tone and reads as passive-aggressive rather than merely concise. The model also under-considered the channel norms (formal, process-heavy, escalation paths) and the ongoing conflict dynamics, which are central to why the message lands as passive-aggressive.",
    "failure_pattern": "missing-subtle-passive-aggression",
    "severity": "major",
    "id": 381,
    "message": "Yep \u2014 as I said, we can revisit next sprint.",
    "ground_truth_flags": [
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "One-Liner"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: the model labeled the message as Passive-Aggressive even though, in this context, it reads as a routine confirmation to defer to the already-agreed next-sprint plan.",
    "why": "The model over-weighted the phrase \"as I said\" as a standalone cue for passive-aggression and under-weighted the surrounding thread context: multiple stakeholders had already aligned on workaround-for-demo + fix-next-sprint, and the target message is simply reaffirming that decision. In a formal, process-heavy org, brief reminders like this can be normal, especially when referencing prior alignment and sprint cadence. The model also failed to distinguish between (a) genuinely admonishing repetition in a heated back-and-forth and (b) a terse, procedural restatement in a coordination thread. Net: a legitimate style/clarity marker was misclassified as hostile tone.",
    "failure_pattern": "overflagging-phrase-level-passive-aggression",
    "severity": "minor",
    "id": 382,
    "message": "Yep \u2014 as I said, we can revisit next sprint.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "The model missed the expected \"Rudeness\" flag and instead flagged \"One-Liner\"; it treated the brevity as the main issue rather than the dismissive, face-threatening shutdown in a high-stakes, blameless risk channel.",
    "why": "The model over-indexed on surface form (short length) and applied a generic \"too short to be actionable\" heuristic, while ignoring pragmatic meaning and context: Maya raised a concrete, reproducible prod-like regression right before an enterprise rollout, and the channel explicitly requires factual, careful updates because execs are monitoring. In that setting, \"Yep\u2014go ahead and ship it.\" functions as a brusque dismissal of a risk report (especially given Leo\u2019s defensiveness in the thread), implicitly devaluing QA\u2019s concern and cutting off discussion without engaging the evidence. The rudeness is subtle because there are no insults or profanity; it\u2019s conveyed via tone (casual 'Yep', imperative 'go ahead', no acknowledgement/conditions) and the power dynamic/pressure in the channel.",
    "failure_pattern": "tone-pragmatics-missed-due-to-length-heuristic",
    "severity": "major",
    "id": 383,
    "message": "Yep\u2014go ahead and ship it.",
    "ground_truth_flags": [
      "Rudeness"
    ],
    "predicted_flags": [
      "One-Liner"
    ]
  },
  {
    "what_went_wrong": "Missed the expected Passive-Aggressive flag and instead flagged Vagueness. The message is indeed nonspecific, but its primary issue in this thread is the implied blame/side-eye (\u201cthis time\u201d) given the recent slip and escalating tension.",
    "why": "The model over-weighted surface-level linguistic features (lack of concrete next steps) and under-weighted pragmatic meaning from context. In a high-stakes #sales thread where commitments are scrutinized, \u201cNoted \u2014 let\u2019s just make sure we\u2019re aligned this time\u201d functions as a soft reprimand: it suggests prior misalignment/failure and subtly assigns responsibility without naming it. The cue words \"just\" and \"this time\" are classic hedged admonitions that read as passive-aggressive in a conflict-laden exchange. Vagueness is present but secondary; the model treated it as a generic clarity problem rather than interpreting the interpersonal signal amplified by the prior messages about slips, scope policing, and screenshot risk.",
    "failure_pattern": "missing-subtle-passive-aggression",
    "severity": "major",
    "id": 387,
    "message": "Noted \u2014 let\u2019s just make sure we\u2019re aligned this time.",
    "ground_truth_flags": [
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Vagueness"
    ]
  },
  {
    "what_went_wrong": "The model over-flagged: it marked the message as Vagueness and Passive-Aggressive even though, in this thread, it functions as a benign alignment check and doesn\u2019t introduce an actionable ambiguity or a clear interpersonal jab that warrants flagging.",
    "why": "It treated the message in isolation and applied generic heuristics (\"aligned\" = vague; \"this time\" = passive-aggressive) without weighing the channel norms and immediate context. In #sales at IronVale, participants are explicitly trying to avoid overconfident commitments; a lightweight acknowledgement + alignment emphasis is normal and even risk-reducing. Also, the prior slip makes \"this time\" more of a situational reminder about process compliance than a targeted criticism of an individual\u2014especially absent second-person blame or sarcasm. The model conflated a mild, context-appropriate cautionary phrasing with passive aggression and penalized brevity that is actually preferred in this environment.",
    "failure_pattern": "context-appropriate-caution-overflagged",
    "severity": "minor",
    "id": 388,
    "message": "Noted \u2014 let\u2019s just make sure we\u2019re aligned this time.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Vagueness",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: the model added an extra \"One-Liner\" flag that was not expected; \"Passive-Aggressive\" was correctly identified.",
    "why": "The model treated brevity itself as a policy violation, ignoring that PixelCraft\u2019s #general norms explicitly allow fast, candid, direct communication\u2014especially under deadline. While the message is short, it doesn\u2019t inherently reduce clarity or engagement in a way that warrants a separate flag; the actual risk in this context is the dismissive/withdrawn subtext (passive-aggression), not the length. In other words, it conflated a neutral style attribute (being concise) with a problematic behavior (shutting down collaboration), and double-counted the same underlying issue by tagging both.",
    "failure_pattern": "overflagging-brevity-as-problem",
    "severity": "minor",
    "id": 389,
    "message": "Got it\u2014do whatever you think is best.",
    "ground_truth_flags": [
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "One-Liner",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: the model applied the \"One-Liner\" flag even though the expected flags were none; the message is brief but not inherently problematic given the thread\u2019s clear decision and ownership assignments.",
    "why": "The model treated brevity as a standalone violation and over-indexed on a generic heuristic (short = non-substantive) while underweighting context. In this conversation, the plan is already decided (server-side, owners assigned, deadlines set), so a short acknowledgment/delegation is socially and operationally acceptable in a fast, candid #general culture. The model also inferred \"conversation-closing\" as negative, but the thread is already effectively closed by Elena/Priya\u2019s directives\u2014this reply doesn\u2019t derail execution or create ambiguity beyond what\u2019s already resolved.",
    "failure_pattern": "overflagging-brevity-in-high-clarity-context",
    "severity": "minor",
    "id": 390,
    "message": "Got it\u2014do whatever you think is best.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "One-Liner"
    ]
  },
  {
    "what_went_wrong": "Missed the expected \"Passive-Aggressive\" flag by treating the message as neutral status-acknowledgement, despite context where the phrasing can read as dismissive/pressure-laden in a high-stakes incident thread.",
    "why": "The model evaluated the sentence in isolation and relied on its literal semantics (acknowledgement + request for updates). In this org/culture and moment (board notified, exec review due, scrutiny on SRE/on-call, #general visibility), \"Got it \u2014 please keep me posted if anything changes\" can function as a terse, low-empathy managerial brush-off that implicitly signals \"don\u2019t bother me unless it gets worse\" and can be perceived as passive-aggressive pressure/deflection of accountability. The subtlety is pragmatic: tone is inferred from power dynamics (CTO to incident responders), public channel, and expectations for polished incident leadership (clear directives, explicit ownership, next steps). The model missed those contextual tone cues and therefore under-flagged.",
    "failure_pattern": "context-not-considered",
    "severity": "major",
    "id": 391,
    "message": "Got it \u2014 please keep me posted if anything changes.",
    "ground_truth_flags": [
      "Passive-Aggressive"
    ],
    "predicted_flags": []
  },
  {
    "what_went_wrong": "Flagged the message as Passive-Aggressive when it should not have been flagged.",
    "why": "The model over-indexed on the token \"finally\" as an inherent sarcasm marker and treated it as a veiled dig. It didn\u2019t adequately weigh the immediate conversational context: Leo has just acknowledged Maya\u2019s catch (\"Good catch\"), asked a follow-up question, and Maya is actively collaborating\u2014so a brief \"Thanks\" reply is plausibly straightforward in a fast, blunt DM culture. In this startup setting, short, clipped acknowledgments are normal; \"finally\" can simply indicate timing/sequence (i.e., the issue is now known) rather than interpersonal blame. The model assumed negative intent from a single word without confirming supporting cues (e.g., prior unresponsiveness, chastising language, repeated reminders, or emotive punctuation).",
    "failure_pattern": "overflagging-ambiguous-sarcasm-marker",
    "severity": "minor",
    "id": 394,
    "message": "Thanks for finally flagging it.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed the expected Passive-Aggressive flag: the model predicted no flags even though the message carries a corrective, scolding undertone in a tense escalation thread.",
    "why": "The model treated the text in isolation as a neutral process reminder (\u201cplease use the on-call queue next time\u201d) and over-weighted surface politeness (\u201cGot it\u201d, \u201cplease\u201d). It failed to incorporate the conversational context: Maya is already being reprimanded for bypassing process, emotions are high, and leadership pressure makes the thread politically charged. In that context, \u201cnext time\u201d functions as a subtle reprimand/brush-off rather than help, implying blame and closing the door without addressing the urgent customer impact\u2014classic low-grade passive aggression (polite phrasing + implicit chastisement).",
    "failure_pattern": "context-not-considered",
    "severity": "major",
    "id": 395,
    "message": "Got it \u2014 please use the on-call queue next time.",
    "ground_truth_flags": [
      "Passive-Aggressive"
    ],
    "predicted_flags": []
  },
  {
    "what_went_wrong": "Flagged the message as Passive-Aggressive even though no flags were expected; it treated a mild phrasing issue as a policy-level tone violation.",
    "why": "The model overweighted the lexical cue \"finally\" and assumed sarcasm/passive aggression without sufficient evidence of hostile intent. In this high-stakes, formal channel, \"Thanks for finally clarifying\" can be interpreted as time-pressure-driven brusqueness or simply awkward gratitude from a new hire, not necessarily a veiled complaint. The model also failed to distinguish between (a) a tone that is slightly informal or potentially suboptimal for an audit-friendly exec-facing thread and (b) an actual passive-aggressive interpersonal jab. It effectively collapsed 'could be read as slightly snippy' into 'is passive-aggressive,' leading to over-enforcement.",
    "failure_pattern": "overflagging-ambiguous-tone-marker",
    "severity": "minor",
    "id": 398,
    "message": "Thanks for finally clarifying \u2014 super helpful.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed a Passive-Aggressive flag: the model treated the target message as purely positive gratitude, but in this tense outage/blame context it reads as a pointed, ironic commendation.",
    "why": "The model relied on surface-level politeness cues (\"Thanks\") and evaluated the sentence in isolation. It failed to incorporate the preceding messages showing frustration, blame attribution, and high stakes (investors/customer pressure). In that context, \"Thanks for jumping on this so quickly\" can function as indirect criticism (implying someone did not act quickly enough), especially given Eli\u2019s admitted delayed escalation and Maya\u2019s prior reprimand. The subtlety is that the passive-aggression is pragmatic/implicit\u2014carried by timing, power dynamics (CTO), and contrast with the prior scolding\u2014rather than explicit sarcasm markers.",
    "failure_pattern": "context-not-considered",
    "severity": "major",
    "id": 399,
    "message": "Thanks for jumping on this so quickly.",
    "ground_truth_flags": [
      "Passive-Aggressive"
    ],
    "predicted_flags": []
  },
  {
    "what_went_wrong": "Missed the expected \"Passive-Aggressive\" flag; it only flagged \"One-Liner\".",
    "why": "The model treated the message as a neutral, minimal acknowledgement and didn\u2019t incorporate the tense thread context (pushback about signed-off accessibility/compliance vs. PM\u2019s shutdown: \u201cWe don\u2019t have time to re-litigate this.\u201d). In that setting, \u201cNoted \u2014 let\u2019s go with that.\u201d reads as clipped, dismissive closure that aligns with the PM\u2019s hard stop and can be interpreted as a pointed, compliance-ignoring capitulation or a subtle jab at the prior objections. Because the passive-aggression is pragmatic/subtextual (tone-as-closure, deference signaling, and conflict avoidance in a high-scrutiny public channel) rather than containing overt sarcasm or barbed language, the model likely relied on surface lexical cues and missed the contextual implicature.",
    "failure_pattern": "missing-subtle-passive-aggression",
    "severity": "major",
    "id": 401,
    "message": "Noted \u2014 let\u2019s go with that.",
    "ground_truth_flags": [
      "One-Liner",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "One-Liner"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: labeled the target message as a problematic \u201cOne-Liner\u201d even though, in this context, it\u2019s a normal, non-risky confirmation of an already-agreed plan and does not violate the expected flags (none).",
    "why": "The model applied a generic heuristic (\u201cvery short acknowledgments shut down coordination\u201d) without incorporating the surrounding thread state. Here, coordination details, ownership, and next steps were already established (scope list agreed, design update requested by EOD, engineering timeline set, PM will log the decision). \u201cNoted \u2014 let\u2019s go with that.\u201d functions as final approval/closure, which is desirable in a strict change-control, audit-focused environment. The model also over-weighted message length as a proxy for disengagement, misclassifying a legitimate, deferential acceptance as a conversational risk.",
    "failure_pattern": "overflagging-brief-acknowledgment",
    "severity": "minor",
    "id": 402,
    "message": "Noted \u2014 let\u2019s go with that.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "One-Liner"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as passive-aggressive when it should not have been flagged.",
    "why": "The model latched onto the token 'finally' as a universal passive-aggression marker and treated it as a backhanded compliment, but it didn\u2019t adequately weigh the immediate conversation context: Leo promised metrics in ~10 minutes and then delivered; there\u2019s no demonstrated delay, conflict, or prior unmet request in the thread that would make 'finally' plausibly sarcastic. In this context, 'finally' can read as casual emphasis or relief (especially in fast-moving Slack), not necessarily annoyance. The model also failed to account for the channel\u2019s client-visible nature: while the phrasing is slightly informal and could be interpreted as terse, it doesn\u2019t rise to a clear interpersonal violation warranting a passive-aggressive flag without stronger contextual cues (e.g., repeated chasing, long wait, prior frustration).",
    "failure_pattern": "overflagging-ambiguous-adverb",
    "severity": "minor",
    "id": 404,
    "message": "Thanks for finally posting the numbers.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: it added a Vagueness flag that isn\u2019t warranted by the conversational context; Passive-Aggressive was the only appropriate flag.",
    "why": "The model treated the target message in isolation and applied a generic \u2018unclear referent\u2019 heuristic (\u201cit\u201d, \u201cfast way\u201d) without leveraging the immediately preceding exchange, where \u2018the fast way\u2019 clearly maps to Jordan\u2019s plan to promise a 2\u2011week custom API integration and defer discovery. In a fast-moving #sales debate, this kind of shorthand is contextually specific, not vague. The quotes around \u201cfast\u201d function primarily as sarcasm/irony referencing the prior overcommit incident, so the main risk is tone (passive-aggression), not lack of detail.",
    "failure_pattern": "contextual-shorthand-overflagged-as-vague",
    "severity": "minor",
    "id": 405,
    "message": "Totally\u2014let\u2019s just do it the \"fast\" way again.",
    "ground_truth_flags": [
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Vagueness",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: the message was labeled Passive-Aggressive even though, in context, it reads as a straightforward agreement to proceed with the previously defined \u201cfast\u201d plan (Phase 1 read-only + daily sync) and does not contain a clear interpersonal jab.",
    "why": "The model over-indexed on surface cues (quotes around \u201cfast\u201d and the word \u201cagain\u201d) as universal markers of sarcasm, without grounding them in the immediate thread where \u201cfast\u201d was explicitly defined as a scoped, conservative approach. In this setting, quotation marks can function as shorthand to reference an agreed-upon term, not mock it. \u201cAgain\u201d can simply indicate reusing the previously discussed approach, not criticizing past behavior. The model also didn\u2019t sufficiently weigh that the preceding exchange is cooperative and converging on a plan; there\u2019s no explicit friction in the target message (no contradiction, blame, or implied incompetence).",
    "failure_pattern": "overflagging-quoted-term-as-sarcasm",
    "severity": "minor",
    "id": 406,
    "message": "Totally\u2014let\u2019s just do it the \"fast\" way again.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as Passive-Aggressive when it should not have been flagged at all (false positive).",
    "why": "The model over-weighted the word \u201cactually\u201d as an inherent marker of sarcasm/doubt and didn\u2019t sufficiently ground the interpretation in the immediate context. In this thread, Leo has explicitly committed to finishing in ~20 minutes and will request re-review; \u201clet me know when it\u2019s actually done\u201d can plausibly mean \u201cwhen it\u2019s fully merged/ready for QA,\u201d i.e., a straightforward status checkpoint under time pressure. While \u201cactually\u201d can sometimes signal skepticism, here there\u2019s no accompanying jab, contradiction, or pattern of undermining language in the single utterance, so inferring passive-aggression relies on tone projection rather than evidence. The model effectively treated a potentially ambiguous intensifier as a definitive tone cue, leading to over-flagging.",
    "failure_pattern": "overflagging-ambiguous-intensifier",
    "severity": "minor",
    "id": 408,
    "message": "Cool \u2014 let me know when it's actually done.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed the expected Passive-Aggressive flag and instead flagged Vagueness, even though the primary issue in context is the curt handoff ('file a ticket\u2026 we\u2019ll take it from there') functioning as a pushback/brush-off in a high-stakes escalation thread.",
    "why": "The model focused narrowly on the literal content ('with the details' is unspecific) and treated the message as a standalone task request. It underweighted the surrounding tension: Morgan has already complained about Sales \u2018fire drills,\u2019 Avery is explicitly asking Engineering not to turn it into a blamey process fight, and leadership is watching with a customer deadline. In that context, 'Got it \u2014 please file a ticket\u2026' reads less like a neutral request for structure and more like procedural gatekeeping and a subtle reprimand/deflection (\u201cnot now / not here\u201d), which is the passive-aggressive signal. Vagueness is technically present, but it\u2019s secondary and not the expected flag given the interpersonal dynamics and implied tone.",
    "failure_pattern": "context-not-considered-missing-passive-aggression",
    "severity": "major",
    "id": 409,
    "message": "Got it \u2014 please file a ticket with the details and we\u2019ll take it from there.",
    "ground_truth_flags": [
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Vagueness"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as Passive-Aggressive when it was an acceptable, direct deadline/scope reminder in this context (false positive).",
    "why": "The model over-weighted the phrase \u201cPer my last message\u201d as a universal passive-aggressive marker without adequately integrating situational context: a high-urgency ship deadline, prior explicit scope-freeze instructions, and a Slack culture where concise, directive reminders are normal. Here, the phrase functions more as a continuity/traceability cue (\u201cas stated above\u201d) rather than a veiled reprimand. The model also didn\u2019t calibrate for the channel dynamics (fast-paced engineering thread with CEO present), where reiterating constraints can be necessary and not inherently snarky.",
    "failure_pattern": "overflagging-direct-communication",
    "severity": "minor",
    "id": 414,
    "message": "Per my last message, we really need to stop making late changes if we want to ship by Friday.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed the expected Passive-Aggressive flag and instead over-emphasized style-based issues (Vagueness/One-Liner) that are secondary in this context.",
    "why": "The model treated the message as merely underspecified and brief, but ignored the interpersonal and power-dynamic context: the thread is tense, with public pressure and quality concerns. 'Yep, totally\u2014let\u2019s just ship it.' reads as sarcastic/performative agreement (especially with 'totally' + 'just') that dismisses Arjun\u2019s risk/QA objections and signals resignation rather than genuine alignment. By focusing on missing operational details, it failed to interpret pragmatic meaning (tone/implicature) and the channel norms about avoiding blame/escalation.",
    "failure_pattern": "missing-subtle-passive-aggression",
    "severity": "major",
    "id": 415,
    "message": "Yep, totally\u2014let\u2019s just ship it.",
    "ground_truth_flags": [
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Vagueness",
      "One-Liner"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as \"Vagueness\" even though, in this channel and thread context, it functions as an acceptable concise go-ahead and does not introduce actionable ambiguity.",
    "why": "The model evaluated the target message in isolation and applied a generic rule that approvals must restate owners/timing/conditions. It missed that the preceding messages already established the concrete plan (PR behind a flag, 4:30 staged deploy, Arjun runs regression, Maya verifies GA and can roll back/keep flag off). In a #engineering culture that values concise status updates, \"Yep, totally\u2014let\u2019s just ship it\" is a normal acknowledgment, not a harmful lack of specificity. The model also over-indexed on the phrase \"just ship it\" as implying recklessness, but that concern is already mitigated by the agreed safeguards in-thread.",
    "failure_pattern": "context-not-considered",
    "severity": "minor",
    "id": 416,
    "message": "Yep, totally\u2014let\u2019s just ship it.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Vagueness"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: applied the \"One-Liner\" flag even though the message is a normal, context-appropriate acknowledgment and the expected flags were none.",
    "why": "The model treated brevity as inherently low-value without weighting channel norms and conversational function. In #random (a watercooler/noisy channel), short reactive affirmations are typical and can serve as lightweight social signaling/closure. The message also sits in a cooperative moment (agreement on a plan and next steps), so it doesn\u2019t create ambiguity or stall progress; it\u2019s just a quick positive reaction. The model over-applied a generic heuristic (short = one-liner) instead of assessing whether the brevity actually harms clarity or decision-making in this context.",
    "failure_pattern": "ignoring-channel-context",
    "severity": "minor",
    "id": 418,
    "message": "Love that for us.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "One-Liner"
    ]
  },
  {
    "what_went_wrong": "Missed the expected Passive-Aggressive flag and instead flagged the message as One-Liner.",
    "why": "The model focused on surface form (brevity) and treated the utterance as merely non-actionable, but in this thread \u2018Sure, ship it.\u2019 functions as dismissive, sarcastic acquiescence in the face of explicit unresolved blockers (red CI, compliance/security concerns, audit logging). In a high-stakes, formal, audit-oriented channel with leadership watching, this reads less like neutral approval and more like a cutting, conversation-ending signal that minimizes prior feedback\u2014i.e., passive-aggressive. The model underweighted pragmatic intent/tone implied by context (outstanding objections + urgency + power dynamics) and over-weighted a generic \u201cshort message\u201d heuristic.",
    "failure_pattern": "missing-subtle-passive-aggression",
    "severity": "major",
    "id": 419,
    "message": "Sure, ship it.",
    "ground_truth_flags": [
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "One-Liner"
    ]
  },
  {
    "what_went_wrong": "Flagged the message incorrectly as a \"One-Liner\" even though the expected policy outcome is no flags for a simple approval statement in context.",
    "why": "The model treated brevity as inherently problematic and applied a generic \"one-liner = low engagement\" heuristic without weighing channel norms and conversational state. In this thread, multiple reviewers have already confirmed CI, security checks, audit events, rollback notes, and release timing; the remaining action is a clear go/no-go. \"Sure, ship it.\" functions as an explicit approval/decision marker that\u2019s appropriate for an audit-oriented engineering channel under time pressure. The model also over-indexed on a speculative tone risk (\"shutting down discussion\") despite the phrase being neutral and aligned with decisive change-control communication at the merge gate.",
    "failure_pattern": "overflagging-brief-approval",
    "severity": "minor",
    "id": 420,
    "message": "Sure, ship it.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "One-Liner"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as Passive-Aggressive even though it was expected to be unflagged in this team/channel context (false positive / overflag).",
    "why": "The model treated the phrase \u201csince we\u2019re apparently doing this again\u201d as an inherently passive-aggressive jab, without adequately weighting the startup\u2019s fast-paced, informal Slack norms where mild sarcasm/venting during fire drills can be common and not necessarily a harmful interpersonal dig. It also didn\u2019t distinguish between (a) frustration directed at the situation/process (\u201cwe have another incident again\u201d) versus (b) a covert attack at a person. Given the context (high tension, public #general, urgency, blunt culture), the message reads as compliant task acceptance with a bit of situational exasperation, which may be acceptable and not reliably predictive of interpersonal toxicity. The model over-indexed on a keyword-pattern (\u201capparently doing this again\u201d) rather than applying a contextual threshold for flagging.",
    "failure_pattern": "cultural-style-misclassified",
    "severity": "minor",
    "id": 422,
    "message": "Sure \u2014 since we\u2019re apparently doing this again, I\u2019ll take it.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "The model missed the Passive-Aggressive flag. It correctly identified the message as a One-Liner but failed to detect the implied resentment/withdrawal embedded in \u201cdo what you need to.\u201d",
    "why": "The passive-aggressive meaning is context-dependent: after Leo\u2019s sustained pushback and Maya\u2019s firm shutdown (\u201cDecision is made\u201d), the target message reads less like neutral compliance and more like curt, resigned disengagement (i.e., \u2018fine, whatever\u2019), signaling dissatisfaction without stating it directly. The model appears to rely heavily on surface features (brevity) and treated the text as straightforward acceptance, under-weighting the prior conflict, power dynamics, and the common pragmatic cue that \u201cdo what you need to\u201d often functions as a veiled critique or refusal to collaborate further.",
    "failure_pattern": "missing-subtle-passive-aggression",
    "severity": "major",
    "id": 423,
    "message": "Got it \u2014 do what you need to.",
    "ground_truth_flags": [
      "One-Liner",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "One-Liner"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: the model labeled the message as a problematic \u201cOne-Liner\u201d even though no flag was expected for this context.",
    "why": "The model treated brevity as inherently conversation-stopping without weighing channel norms and conversational state. In this thread, tasks and next steps are already clearly assigned (Leo committed to specific deliverables and timing; Maya will share run-of-show). The target message reads as a simple acknowledgment/permission, not a shutdown or refusal to collaborate. The model ignored that (a) the exchange is already resolved, (b) #client-updates values crisp, public updates, and (c) additional detail would be redundant. It over-applied a style heuristic (\u201cshort = low engagement\u201d) rather than evaluating whether any practical ambiguity, rudeness, or risk was introduced.",
    "failure_pattern": "overflagging-brief-acknowledgement",
    "severity": "minor",
    "id": 424,
    "message": "Got it \u2014 do what you need to.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "One-Liner"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as Passive-Aggressive when it should not have been flagged.",
    "why": "The model over-indexed on the phrase \u201clike I said earlier\u201d as an inherent passive-aggressive marker, without grounding it in the actual thread context. In this fast-paced, blunt Slack environment during an outage, that phrasing can be a neutral reminder/coordination cue rather than a veiled dig\u2014especially since there\u2019s no direct contradiction, insult, or escalation and the message is cooperative (\u201cI\u2019ll take care of it\u201d). The model treated a potentially ambiguous stylistic cue as a violation instead of requiring stronger evidence of hostile intent given the situational norms (urgency, brevity, directness).",
    "failure_pattern": "overflagging-ambiguous-reminder-phrase",
    "severity": "minor",
    "id": 426,
    "message": "Sure \u2014 like I said earlier, I'll take care of it.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: the model marked the message as Passive-Aggressive even though it is clearly sarcastic/biting and does contain an indirect complaint about timing.",
    "why": "The expected label set appears misaligned with the policy/taxonomy the model is applying. Given the text alone, \u201cAwesome \u2014 love finding this out the night before the demo.\u201d is a textbook passive-aggressive/sarcastic remark in a high-stakes, cross-functional channel (#sales) and is likely to embarrass the dev or convey blame without stating it directly. The model\u2019s reasoning is sound; the failure is primarily an evaluation/ground-truth mismatch (or a guideline that treats common startup sarcasm as acceptable \u2018banter\u2019 and therefore non-flagworthy). In other words, it didn\u2019t misunderstand the context\u2014it applied the flag definition more strictly than the expected labeling, which likely normalized informal snark in this culture or intended to flag only more severe hostility.",
    "failure_pattern": "ground-truth-mismatch-sarcasm-normalized",
    "severity": "major",
    "id": 428,
    "message": "Awesome \u2014 love finding this out the night before the demo.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: added a \"Fake\" flag that wasn\u2019t warranted by the message; \"Passive-Aggressive\" alone matched the expected labeling.",
    "why": "The model treated implied insincerity (common in passive-aggressive assent) as evidence of deception/impersonation, collapsing two distinct categories. In this context, \u201cSure \u2014 let\u2019s do it your way.\u201d can reasonably read as reluctant/sarcastic compliance, but there is no concrete signal of fakery (e.g., identity misrepresentation, fabricated claims, pretending to have done work, or false information). The shortness and phrasing (\u201cyour way\u201d) plus prior conflict supports passive-aggression, but the leap to \"Fake\" was driven by over-reliance on a heuristic (\u201csounds insincere => fake\u201d) rather than the platform\u2019s stricter definition of \u201cFake.\u201d",
    "failure_pattern": "overflagging-fake-from-insincere-tone",
    "severity": "minor",
    "id": 429,
    "message": "Sure \u2014 let's do it your way.",
    "ground_truth_flags": [
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Fake",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as Passive-Aggressive even though, in this urgent incident-response context, it can reasonably be interpreted as a straightforward commitment to join rather than a veiled jab.",
    "why": "The model latched onto the lexical cue \u201cagain\u201d (often associated with resentment) and treated it as definitive evidence of passive aggression, without adequately weighing situational context: the team is explicitly in an all-hands production incident with a six-hour renewal deadline, multiple peers are already reprioritizing work (\u201cpause the launch announcement\u201d), and the channel tone is operational/urgent. In this setting, \u201cdrop everything\u201d is a common incident-response idiom, and \u201cagain\u201d can be read as factual history (this has happened before) rather than sarcasm. The model over-indexed on potential subtext and ignored that the surrounding thread is collaborative and time-critical, making the negative-intent inference too speculative for a flag.",
    "failure_pattern": "overflagging-ambiguous-frustration",
    "severity": "minor",
    "id": 432,
    "message": "Got it \u2014 I'll drop everything again and join.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed the expected Passive-Aggressive flag; only flagged Non-Objective.",
    "why": "The model treated the message purely as an unsupported assertion (\u201ctotally safe\u201d) and didn\u2019t incorporate the conversational tension and implied dismissal. In context, a confident \u2018Yep, totally safe for the 9am demo\u2019 directly contradicts Mina\u2019s documented, reproducible 500 error and Leo\u2019s prior minimization (\u201cprobably your test data\u2026 edge case\u201d). That makes the reassurance read as a curt, face-saving shutdown rather than a neutral status update\u2014especially in a high-stakes, client-facing channel where Mina is trying to be firm without being accusatory. The passive-aggressive signal is subtle and context-dependent: the breezy \u2018Yep\u2019 + absolute \u2018totally safe\u2019 functions as an indirect put-down of Mina\u2019s concern (implying it\u2019s not real or not worth addressing) without explicitly saying so, which the model failed to detect because it focused on surface form rather than pragmatic intent and thread dynamics.",
    "failure_pattern": "missing-subtle-passive-aggression",
    "severity": "major",
    "id": 433,
    "message": "Yep, totally safe for the 9am demo.",
    "ground_truth_flags": [
      "Non-Objective",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Non-Objective"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: it marked the message as \u201cVagueness\u201d and \u201cNon-Objective\u201d even though, in this channel\u2019s norms, a concise go/no-go confirmation for a specific upcoming demo is appropriate and sufficiently clear.",
    "why": "The model applied generic writing-quality heuristics (avoid absolutes like \u201ctotally safe,\u201d add evidence) without weighting the explicit channel context: #client-updates is intended for paste-ready, concise status lines for account managers. The prior thread already contains the concrete evidence (smoke tests across key checkout paths + cache root cause resolved + guardrail cherry-picked). Given that context, \u201csafe for the 9am demo\u201d is not vague\u2014its scope is clearly the checkout-only demo\u2014and it\u2019s not meaningfully \u201cnon-objective\u201d because it functions as an executive summary of the immediately preceding verification. The model also over-penalized an absolute phrasing (\u201ctotally\u201d) as if it were a factual claim requiring proof, rather than a conventional, low-friction green-light signal in a high-tempo status channel.",
    "failure_pattern": "ignoring-channel-context",
    "severity": "minor",
    "id": 434,
    "message": "Yep, totally safe for the 9am demo.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Vagueness",
      "Non-Objective"
    ]
  },
  {
    "what_went_wrong": "Missed the expected Passive-Aggressive flag and instead flagged Vagueness; the message was treated as merely nonspecific rather than potentially dismissive/brush-off in a tense feedback exchange.",
    "why": "The model latched onto surface-level content (lack of specific commitments) and optimized for an informational-quality critique (vagueness) while underweighting pragmatic intent and tone in context. In this DM, after Maya asked for explicit acknowledgment + concrete behavior changes, \u201cNoted \u2014 I\u2019ll make sure it doesn\u2019t happen again\u201d can read as a terse, managerial-sounding closure that sidesteps accountability (which part won\u2019t happen again?), signaling reluctant compliance rather than genuine engagement. That subtle \"checkbox\" acknowledgment is a common passive-aggressive pattern in high-stakes feedback threads, and the model didn\u2019t incorporate the conversational power dynamics, prior defensiveness (\u201cI was joking\u201d), and Maya\u2019s explicit request for specificity as cues that the brevity may be dismissive rather than merely vague.",
    "failure_pattern": "missing-subtle-passive-aggression",
    "severity": "major",
    "id": 435,
    "message": "Noted \u2014 I\u2019ll make sure it doesn\u2019t happen again.",
    "ground_truth_flags": [
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Vagueness"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: the model marked the message as \u201cVagueness\u201d even though, in context, \u201cit\u201d clearly refers to the two specific issues Maya just raised (handoffs + avoiding dunking-on-PM tone), and no flags were expected.",
    "why": "The model evaluated the target message in isolation and applied a generic clarity heuristic (\u201cpronoun = vague\u201d) without leveraging the immediately preceding DM context that anchors the referent. In this scenario, a brief acknowledgment is an acceptable, culturally aligned response (direct, async-friendly, de-escalating) and doesn\u2019t create actionable ambiguity because Maya already specified the desired behaviors and channels (#eng-release handoff content; #random tone guidance). The model conflated \u201clacks implementation details\u201d with \u201cproblematic vagueness,\u201d even though the burden of specificity was already satisfied upstream in the thread.",
    "failure_pattern": "context-anchored-pronoun-overflagged",
    "severity": "minor",
    "id": 436,
    "message": "Noted \u2014 I\u2019ll make sure it doesn\u2019t happen again.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Vagueness"
    ]
  },
  {
    "what_went_wrong": "The model missed the expected Passive-Aggressive flag. It correctly flagged One-Liner, but failed to recognize that the brevity functions as a pointed, face-threatening dismissal in this high-stakes, public dispute.",
    "why": "The passive-aggression here is contextual rather than lexical: on its face, \u201cSure, go ahead.\u201d reads as neutral consent. But given (a) Jordan\u2019s escalating stance (\u201cpush the refactor straight to main\u201d), (b) Elena\u2019s explicit request to align and avoid a risky push, (c) the public audience (account lead + CEO) and urgency, and (d) the interpersonal tension, the message can reasonably be interpreted as a curt, performative \u2018fine, do what you want\u2019 that withdraws engagement and shifts responsibility\u2014classic passive-aggressive compliance. The model appears to rely primarily on surface cues (no sarcasm markers, no negative wording) and treated it as merely brief, not as a contextually loaded, undermining response.",
    "failure_pattern": "missing-contextual-passive-aggression",
    "severity": "major",
    "id": 437,
    "message": "Sure, go ahead.",
    "ground_truth_flags": [
      "One-Liner",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "One-Liner"
    ]
  },
  {
    "what_went_wrong": "Missed a required flag: the message should have been flagged as both Passive-Aggressive and Non-Objective, but the model only returned Passive-Aggressive.",
    "why": "The model correctly identified the sarcastic, resentful subtext ('Must be nice...') as passive-aggressive, but it treated that as sufficient and failed to also mark the message as Non-Objective. The utterance is an implied complaint without concrete, verifiable content or a clear request (no specific ask like 'Can someone share the decision notes/recording?' and no objective statement like 'I wasn\u2019t included in the call and I need details for renewals'). In this async-first, documentation-valuing context, the lack of a direct, actionable, factual framing is salient, but the model focused narrowly on tone and didn\u2019t apply the separate criterion about objectivity/actionability.",
    "failure_pattern": "missed-non-objective-flag",
    "severity": "minor",
    "id": 439,
    "message": "Must be nice to hear about this on the call this morning.",
    "ground_truth_flags": [
      "Non-Objective",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as Passive-Aggressive when it should not have been flagged (false positive).",
    "why": "The model latched onto a common sarcastic template (\u201cMust be nice\u2026\u201d) and treated it as inherently passive-aggressive, without sufficiently weighing the immediate context: teammates had already acknowledged the miss, proactively offered to DM details, and were actively working to document the change. In this #random thread, the remark can plausibly function as light venting/banter aligned with the channel\u2019s tone rather than a hostile, escalating jab. The model also failed to apply the expected-flag policy boundary: mild frustration or humor that doesn\u2019t contain insults, threats, harassment, or explicit hostility may be considered acceptable and not worth flagging, especially when the conversation is already moving toward remediation.",
    "failure_pattern": "overflagging-mild-sarcasm",
    "severity": "minor",
    "id": 440,
    "message": "Must be nice to hear about this on the call this morning.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as Passive-Aggressive when it should not have been flagged (false positive).",
    "why": "The model over-weighted the single token \"finally\" as a universal sarcasm marker without grounding it in the actual conversational context. In this thread, multiple people had just provided process clarification and links; Maya\u2019s reply can plausibly read as simple closure/acknowledgment (especially from a new employee catching up), not a reproach. The model also failed to distinguish between mild stylistic awkwardness and a true passive-aggressive intent that would warrant flagging, and it didn\u2019t calibrate to the enterprise #general norm where brief acknowledgments are common and not necessarily emotive. As a result, it treated a potentially innocuous phrasing choice as a behavioral violation.",
    "failure_pattern": "overflagging-sarcasm-keyword",
    "severity": "minor",
    "id": 442,
    "message": "Thanks for finally clarifying \u2014 I\u2019ll follow that process.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as Passive-Aggressive when it should not have been flagged at all (false positive).",
    "why": "The model over-weighted the presence of sarcasm (\u201cWhat could possibly go wrong?\u201d) as inherently passive-aggressive without grounding it in the actual conversational intent and context. Here, the message reads as a common, lightweight rhetorical aside that can function as gallows humor/acknowledgement of risk while still aligning with the thread\u2019s consensus: Friday is a target contingent on CI and sign-off. The preceding messages already established a clear, constructive plan and boundaries around urgency/@mentions; the target message doesn\u2019t attack anyone, assign blame, or indirectly snipe at a person\u2014key elements of passive-aggression. In a friendly-but-fast culture, this kind of wry phrasing can be normal and not a policy/tone violation, so treating sarcasm as a proxy for hostility led to overflagging.",
    "failure_pattern": "sarcasm-overflagged-as-passive-aggressive",
    "severity": "minor",
    "id": 444,
    "message": "Sure \u2014 let's ship Friday. What could possibly go wrong?",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as Passive-Aggressive even though, in this team/channel context, it\u2019s better interpreted as a normal, candid expression of frustration rather than a veiled interpersonal dig.",
    "why": "The model treated a common sarcastic/quip-like phrasing (\u201cCool \u2014 love being the one to tell them that again.\u201d) as inherently passive-aggressive without anchoring on situational norms: a blunt, emoji-heavy, fast-moving Slack culture and a high-stress incident/war-room moment where gallows humor and venting are typical. It also didn\u2019t distinguish between (a) sarcasm aimed at the situation/workload (having to repeat bad news) versus (b) sarcasm aimed at a teammate (which is more characteristic of passive-aggression). There\u2019s no direct jab at Eli/CTO/engineering or an indirect compliance-with-resentment toward a person; it reads as momentary exasperation about customer comms.",
    "failure_pattern": "ignoring-channel-context",
    "severity": "minor",
    "id": 446,
    "message": "Cool \u2014 love being the one to tell them that again.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: the model labeled the message as Passive-Aggressive even though it is a mild, procedurally relevant reminder consistent with the channel norms.",
    "why": "The model over-weighted the phrase \"maybe next time\" as inherently passive-aggressive and under-weighted the situational context: #client-updates is explicitly process-heavy (ticket IDs, pinned client-approved wording) during a high-stakes incident. In that environment, a brief correction about compliance requirements is normal and operationally necessary, not a veiled dig. The message also starts with a conciliatory \"Thanks for the update\" and contains no sarcasm markers, personal jab, or insinuation of incompetence\u2014just a reminder of required formatting/wording. The model effectively applied a generic tone heuristic to a phrase without checking whether the correction was justified by the immediate norms and risk profile.",
    "failure_pattern": "overflagging-process-enforcement-as-passive-aggression",
    "severity": "minor",
    "id": 448,
    "message": "Thanks for the update\u2014maybe next time include the ticket ID and client-approved wording.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "The model missed the intended Passive-Aggressive flag and instead flagged Vagueness and One-Liner, which are secondary stylistic properties but not the core interpersonal risk in this context.",
    "why": "In isolation, \u201cCool, let\u2019s just ship it Friday then.\u201d can look merely terse and underspecified, so the classifier latched onto surface features (brevity; lack of actionable detail). But in-thread it functions as a sarcastic/resentful concession (\u201cCool\u201d + \u201cjust ship it\u201d after explicit safety objections), implicitly dismissing engineering\u2019s concerns and amplifying public pressure dynamics (execs/Sales watching, prior outage, PM pushing). The model underweighted pragmatic meaning (tone-as-implication) and the conversational move (performative compliance that signals frustration) that is characteristic of passive aggression in high-stakes, public Slack threads.",
    "failure_pattern": "missing-subtle-passive-aggression",
    "severity": "major",
    "id": 449,
    "message": "Cool, let\u2019s just ship it Friday then.",
    "ground_truth_flags": [
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Vagueness",
      "One-Liner"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: it applied the \u201cOne-Liner\u201d flag even though no flags were expected for this message in this context.",
    "why": "The model treated brevity as inherently problematic and inferred a \u201cshutting down discussion\u201d intent from the short length alone. It underweighted the immediate context where the team had already converged on a plan (soak test, go/no-go, staged rollout Friday) and this message reads as simple agreement/enthusiasm, not dismissal. In a fast, performative Slack channel like #engineering, short affirmations are normal and not necessarily a conversational anti-pattern\u2014especially after alignment has been reached.",
    "failure_pattern": "overflagging-brevity",
    "severity": "minor",
    "id": 450,
    "message": "Cool, let\u2019s just ship it Friday then.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "One-Liner"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as Passive-Aggressive when it should not have been flagged.",
    "why": "The model treated meme-y sarcasm (\u201caccessibility DLC,\u201d \u2018lol,\u2019 \ud83d\ude05) as inherently passive-aggressive, without weighting the strong #random Slack norm described (casual, joke-heavy, decisions aired publicly). In this culture, the phrasing reads as a pointed-but-normal humorous critique and a clarifying question about deferring accessibility, not an indirect personal jab or hostility. The model also over-indexed on tone markers (sarcasm/emoji) rather than intent/target: it critiques a decision (shipping without accessibility) and aligns with an already-serious accessibility concern raised right before, rather than undermining it or attacking a person. In short, legitimate informal style + contextualized frustration was misclassified as a policy-relevant interpersonal harm signal.",
    "failure_pattern": "cultural-style-misclassified",
    "severity": "minor",
    "id": 451,
    "message": "lol so we\u2019re shipping the \u201caccessibility DLC\u201d after the investor demo? \ud83d\ude05",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed the expected \u201cVagueness\u201d flag. The model correctly flagged \u201cNon-Objective\u201d and \u201cPassive-Aggressive,\u201d but failed to detect that the message is ambiguous about what is actually being deferred (accessibility as a whole vs specific fixes/flows).",
    "why": "The model anchored on the obvious sarcasm markers (\u201clol,\u201d \u201cDLC,\u201d emoji) and treated the issue primarily as tone/subjectivity. It didn\u2019t connect the message to the channel\u2019s stated requirement for specific, client-facing updates (\u201cwhat\u2019s included, what\u2019s deferred, and any risk/mitigation\u201d) and Noah\u2019s explicit request to confirm accessibility status. In that context, the target message is a vague, meme-framed rhetorical question that neither clarifies the current accessibility status nor identifies which accessibility items are being deferred\u2014i.e., it introduces uncertainty rather than actionable detail. Because the text makes a pointed insinuation without naming concrete deferred items, the \u201cVagueness\u201d dimension is present but was overshadowed by the more salient passive-aggressive cues.",
    "failure_pattern": "vagueness-missed-due-to-salient-tone-cues",
    "severity": "major",
    "id": 452,
    "message": "lol so we\u2019re shipping the \u201caccessibility DLC\u201d after the investor demo? \ud83d\ude05",
    "ground_truth_flags": [
      "Vagueness",
      "Non-Objective",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Non-Objective",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: the model labeled the message as Passive-Aggressive and Rude even though, in this startup\u2019s candid/sarcastic norm and in a private DM between peers, the tone is consistent with expected style and is not clearly abusive or hostile enough to warrant flags.",
    "why": "The model treated sarcasm (\u201cCool cool,\u201d \u201cmurder mystery \ud83d\ude43\u201d) and a pointed ownership boundary (\u201cyou can babysit it on-call\u201d) as inherently toxic, without weighting the strong contextual cues: (1) this team communicates in punchy, sarcastic Slack bursts, (2) they\u2019re peers with an established rapport, (3) it\u2019s a DM used to de-escalate the public thread, and (4) the on-call remark is a legitimate, concrete negotiation about responsibility given real risk. It over-indexed on literal phrasing like \u201cbabysit\u201d and the snarky framing, interpreting them as demeaning/hostile rather than as culturally normal directness plus humor under deadline pressure.",
    "failure_pattern": "cultural-style-misclassified",
    "severity": "major",
    "id": 453,
    "message": "Cool cool \u2014 let\u2019s just ship the simplest thing so we don\u2019t relive last week\u2019s \u201coverengineering\u201d murder mystery \ud83d\ude43. Mateo, if you want the queue so bad you can babysit it on-call.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as Passive-Aggressive even though the expected flags were none; the model treated informal humor and a blunt escalation idea as a policy-relevant tone issue when it was within acceptable peer-to-peer venting in a DM.",
    "why": "The model over-weighted lexical cues (\"Lol,\" \"with popcorn\") and the mention of forwarding to Sales Ops as a \"veiled threat\" without adequately considering (a) the DM channel (lower performative risk than #sales), (b) the established shared frustration about prior overcommitments, and (c) that the core intent is direct risk management (don\u2019t promise custom API; stick to approved connector). It conflated sarcasm/humor with passive-aggression, and assumed punitive/indirect intent from \"forwarding\" rather than reading it as an (arguably unpolished) escalation tactic consistent with a process-heavy org\u2019s guardrails. In short: style heuristics triggered despite insufficient evidence of indirect hostility toward the recipient.",
    "failure_pattern": "overflagging-humor-as-passive-aggression",
    "severity": "minor",
    "id": 455,
    "message": "Lol if we promise a \u201crapid custom API integration\u201d again, I\u2019m just gonna start forwarding these threads to Sales Ops with popcorn. Let\u2019s stick to the connector.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed the \"Rudeness\" flag: it only predicted \"Passive-Aggressive\" even though the message contains mocking/disrespectful language and a belittling, public call-out tone.",
    "why": "The model correctly detected sarcasm/indirect escalation (passive-aggression) but underweighted explicit rudeness markers (\"Lol\", \"with popcorn\") that ridicule colleagues and trivialize a high-stakes compliance reminder. It appears to treat the comment as mainly an indirect threat rather than also a directly disrespectful/derisive jab, and it didn\u2019t fully apply the organizational context (process-heavy, leadership-watched channel, recent overcommit incident) where this tone is especially inappropriate and likely to be perceived as rude.",
    "failure_pattern": "rudeness-under-detected-when-packaged-as-joke",
    "severity": "major",
    "id": 456,
    "message": "Lol if we promise a \u201crapid custom API integration\u201d again, I\u2019m just gonna start forwarding these threads to Sales Ops with popcorn. Let\u2019s stick to the connector.",
    "ground_truth_flags": [
      "Rudeness",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: marked the DM as Non-Objective and Passive-Aggressive even though expected flags were none; it treated informal teasing and a plausible causal pointer as policy-relevant issues.",
    "why": "The model over-interpreted casual startup banter as hostility. In this DM context, \u201clol\u201d, the quotes around \u201ccleanup\u201d, and \u201cwake up the whole zoo\u201d function as rapport/levity and urgency management, not indirect blame meant to shame in public. It also misapplied the Non-Objective criterion: the message says \u201cI think\u201d and is backed by prior evidence already shared (repro steps, timing after merge, Sentry/HAR, null promo_total). It\u2019s a reasonable debugging hypothesis, not an unsupported accusation. The model likely used surface cues (sarcasm markers like scare quotes) without weighting channel (DM vs #engineering) and without integrating the preceding factual context that makes the causal attribution tentative and evidence-based.",
    "failure_pattern": "cultural-style-misclassified",
    "severity": "minor",
    "id": 457,
    "message": "lol Alex I think your midnight \"cleanup\" resurrected the checkout bug \ud83d\ude05 can you take a look before we wake up the whole zoo?",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Non-Objective",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as Passive-Aggressive when no flag was expected (false positive / overflagging).",
    "why": "The model treated urgent, directive incident-response language as interpersonal sniping. Phrases like \u201cstop the bleeding\u201d and \u201ckeep the extra 'process' stuff to a minimum\u201d can read sharp, but in an outage context they commonly signal triage/prioritization rather than a veiled insult. The model also failed to account for the channel context (#general with joking banter) and the role context (engineering lead delegating under an SLA clock), where bluntness is often acceptable and not necessarily passive-aggressive. It over-indexed on a single cue (quoted \u201cprocess\u201d) as sarcasm, without evidence of targeted, indirect hostility toward a person or group.",
    "failure_pattern": "overflagging-incident-triage-directness",
    "severity": "minor",
    "id": 459,
    "message": "Can someone please just jump on this and stop the bleeding? Sam grab logs + rollback if needed, Priya dig into the DB side \u2014 and let\u2019s keep the extra \"process\" stuff to a minimum until it\u2019s stable.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Both: it missed the expected flags (Vagueness, Rudeness) and instead flagged Passive-Aggressive, which is not the best primary label for the tone violations present.",
    "why": "The message contains clear rudeness/abrasiveness (\u201cjust jump on this,\u201d \u201cstop the bleeding,\u201d and especially dismissing \u201cextra \u2018process\u2019 stuff\u201d), plus operational noncompliance with a formal, ticketed, client-facing channel. It\u2019s also vague in key ways: \u201cjump on this\u201d and \u201cstop the bleeding\u201d are non-specific, and \u201crollback if needed\u201d lacks criteria/authority given explicit guidance that rollback wording must be Account-Management-aligned. The model latched onto the quoted word \u201cprocess\u201d and interpreted it as a veiled jab (passive-aggression), over-indexing on a sarcasm cue while under-weighting the more straightforward classification: direct rudeness and imprecise/unsafe instructions in a high-stakes, process-heavy #client-updates context.",
    "failure_pattern": "sarcasm-cue-overweighted-missing-rudeness-and-vagueness",
    "severity": "major",
    "id": 460,
    "message": "Can someone please just jump on this and stop the bleeding? Sam grab logs + rollback if needed, Priya dig into the DB side \u2014 and let\u2019s keep the extra \"process\" stuff to a minimum until it\u2019s stable.",
    "ground_truth_flags": [
      "Vagueness",
      "Rudeness"
    ],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as Passive-Aggressive when it should not have been flagged (false positive).",
    "why": "The model treated sarcasm as inherently passive-aggressive without weighing channel norms and ongoing tone. In this startup\u2019s #random context, the thread is already meme-heavy and openly sarcastic (\"museum exhibit,\" \"outage bingo,\" \"postmortem-the-musical\"), and the target message continues that shared, performative banter rather than covert hostility or indirect blame. While it contains criticism (\"duct-tape Redis,\" \"tweet 'saved prod'\"), it\u2019s not a veiled personal jab at a specific individual, nor does it use indirectness to avoid a direct ask\u2014it's a stylized way of stating a real engineering concern common in this culture. The model over-indexed on textual cues (sarcasm, eye-roll emoji) and under-considered situational pragmatics: peer-level debate in a joking channel with teammates piling on, where sarcasm is a legitimate communication style and not necessarily a relationship-harming passive-aggressive move.",
    "failure_pattern": "cultural-sarcasm-overflagged-as-passive-aggressive",
    "severity": "minor",
    "id": 461,
    "message": "Yeah sure, let's duct-tape Redis onto it so we can all tweet \"saved prod\"\u2014future-us can clean up the mess at 3am again \ud83d\ude43",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "It missed two expected flags (Rudeness, Non-Objective) and only captured Passive-Aggressive.",
    "why": "The model correctly recognized sarcasm/passive aggression but treated the rest as merely \"tone\" rather than additional policy-relevant issues. The message contains explicit disparagement (\u201cduct-tape Redis\u201d, implying a reckless hack) and a status-seeking jab (\u201cso we can all tweet \u2018saved prod\u2019\u201d) aimed at peers\u2014this crosses into Rudeness/insulting framing, not just indirect negativity. It also fails the thread\u2019s requested norm of \u201cfocused on actionable steps\u201d by substituting a cynical quip for a concrete recommendation, impact estimate, or owner/plan\u2014so it\u2019s Non-Objective in this context. In short: it underweighted contextual instruction (Sam asking to keep it actionable) and under-identified that sarcasm can simultaneously be rude and non-constructive.",
    "failure_pattern": "underdetecting-rudeness-and-nonobjectivity-in-sarcasm",
    "severity": "major",
    "id": 462,
    "message": "Yeah sure, let's duct-tape Redis onto it so we can all tweet \"saved prod\"\u2014future-us can clean up the mess at 3am again \ud83d\ude43",
    "ground_truth_flags": [
      "Rudeness",
      "Non-Objective",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: the message was labeled Passive-Aggressive even though it fits the team\u2019s blunt/informal norm and is a direct critique rather than an indirect, deniable jab.",
    "why": "The model over-weighted surface cues (\"Cool\", \"YOLO\", \"Bold choice\", and the \ud83d\ude43 emoji) as definitive markers of passive-aggression, without adequately considering channel and cultural context. In this startup\u2019s fast-paced Slack/DM style, sarcasm and meme-y shorthand can function as straightforward emphasis and urgency rather than covert hostility. Also, the message is not actually indirect: it explicitly contrasts the refactor vs. the feature-flag hotfix and clearly communicates disagreement. The model treated informal rhetorical framing as a policy violation instead of legitimate peer-to-peer pushback.",
    "failure_pattern": "cultural-style-misclassified",
    "severity": "minor",
    "id": 463,
    "message": "Cool, so we\u2019re gonna YOLO a refactor in front of the client instead of shipping the safe flag? Bold choice \ud83d\ude43",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed the expected \"Non-Objective\" flag (false negative). The other two flags (Passive-Aggressive, Rudeness) were correctly applied.",
    "why": "The model focused on the obvious interpersonal tone markers (sarcasm, \"YOLO,\" \"Bold choice \ud83d\ude43\") and treated the message primarily as a civility issue. It failed to also recognize that the content makes a loaded, sarcastic characterization of the plan (\u201cYOLO a refactor in front of the client\u201d) rather than stating verifiable facts or a concrete technical concern. In this context, the message implies recklessness and public incompetence without evidence, which is non-objective/judgmental framing\u2014especially salient given the CEO/account lead visibility and the need for an externally shareable ETA.",
    "failure_pattern": "missed-non-objective-judgmental-framing",
    "severity": "minor",
    "id": 464,
    "message": "Cool, so we\u2019re gonna YOLO a refactor in front of the client instead of shipping the safe flag? Bold choice \ud83d\ude43",
    "ground_truth_flags": [
      "Rudeness",
      "Non-Objective",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as Passive-Aggressive when the expected outcome was no flags; the model treated a blunt, policy-enforcing correction as a tone violation.",
    "why": "The model over-indexed on loaded phrases (\u201cclassic Sales drive-by escalation\u201d, \u201cthey\u2019ll churn!!! isn\u2019t a plan\u201d) and interpreted them as sarcasm/passive aggression, without weighing that the message is primarily a direct procedural instruction in a high-stakes, process-heavy enterprise context. In this environment, publicly redirecting to incident tooling/on-call paths and requesting concrete diagnostics is normal and often necessary for compliance and response effectiveness. The critique is explicit (not veiled), actionable, and oriented toward process rather than personal insult\u2014i.e., it reads as direct/firm communication. The model also appears to conflate \u2018critical tone\u2019 with passive-aggression (which typically involves indirectness, plausible deniability, or snide insinuation), even though the speaker\u2019s intent is stated plainly.",
    "failure_pattern": "overflagging-direct-communication",
    "severity": "minor",
    "id": 465,
    "message": "Can we please not do the classic Sales drive-by escalation in here? File the incident + page on-call like the process says, and include *actual* details (customer/env/error/timestamps). Tagging random people with \u201cthey\u2019ll churn!!!\u201d isn\u2019t a plan.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: the model marked the message as \"Non-Objective\" even though no flag was expected and the wording is acceptable for this DM/startup Slack context.",
    "why": "The model treated casual, metaphorical phrasing (\"faceplanted\") as a policy-relevant lack of objectivity rather than a normal stylistic choice in a blunt, emoji-heavy environment. It applied a decontextualized standard of neutral incident-report language, ignoring that (a) this is a private DM between teammates, (b) the speaker is giving sales a usable narrative under pressure, and (c) the message still contains the key factual/operational guidance (incident duration roughly an hour, fixed now, don\u2019t promise 100% uptime / \u201cwon\u2019t happen again\u201d). In other words, it conflated \u201cnot formally written\u201d with \u201cshould be flagged.\u201d",
    "failure_pattern": "cultural-style-misclassified",
    "severity": "minor",
    "id": 467,
    "message": "Cool cool \u2014 so we basically faceplanted for an hour \ud83d\ude2c. It\u2019s fixed now; please don\u2019t promise 100% uptime or any magic \"won\u2019t happen again\" stuff.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Non-Objective"
    ]
  },
  {
    "what_went_wrong": "Both: it missed the expected flags (Vagueness, Rudeness) and instead over-flagged/shifted to different labels (Non-Objective, Passive-Aggressive).",
    "why": "The model treated informal startup vernacular as policy-relevant issues under different categories. It latched onto surface cues (\u201cCool cool\u201d, \ud83d\ude2c, and the metaphor \u201cfaceplanted\u201d) and interpreted them as sarcasm/passive aggression, without weighing the channel norms (blunt, emoji-heavy, fast war-room style) and the fact that the message is directive rather than indirectly hostile. At the same time, it failed to recognize the actual problems the rubric expected: (1) Vagueness\u2014\u201cIt\u2019s fixed now\u201d is underspecified (no scope, confirmation window, remaining risk, what \u2018fixed\u2019 means) and \u201cwon\u2019t happen again stuff\u201d is imprecise guidance; and (2) Rudeness\u2014\u201cfaceplanted\u201d and \u201cmagic \u2026 stuff\u201d are derisive/loaded phrasing that can read disrespectful toward the team and dismissive toward Sales\u2019 needs, even if not passive-aggressive. In short, it over-indexed on tone markers for passive-aggression and under-detected clarity and civility issues aligned with the expected taxonomy.",
    "failure_pattern": "cultural-style-misclassified",
    "severity": "major",
    "id": 468,
    "message": "Cool cool \u2014 so we basically faceplanted for an hour \ud83d\ude2c. It\u2019s fixed now; please don\u2019t promise 100% uptime or any magic \"won\u2019t happen again\" stuff.",
    "ground_truth_flags": [
      "Vagueness",
      "Rudeness"
    ],
    "predicted_flags": [
      "Non-Objective",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: the message was marked as \u201cVagueness\u201d even though it contains a clear intent (deploy + feature flags) and a concrete ask (5-minute walkthrough) that is sufficiently actionable for a casual #general thread.",
    "why": "The model appears to have applied a lexical heuristic (\u201cthis thing\u201d, \u201cwhich flags\u201d) without weighing channel norms and conversational context. In #general, informal shorthand is normal, and the audience (tagged senior engineers) likely knows the relevant system from shared context. The model also treated \u2018vagueness\u2019 as a generic quality issue rather than a policy-worthy flag, over-indexing on ambiguous nouns instead of recognizing that the main risk here (if any) is channel/process fit (synchronous escalation + tagging), not lack of specificity.",
    "failure_pattern": "overflagging-low-specificity-language",
    "severity": "minor",
    "id": 469,
    "message": "Hey @Maya @Ethan \u2014 sorry to spam \ud83d\ude05 can one of you just hop on for 5 mins and tell me how we deploy this thing + which flags I need to flip? I\u2019m kinda lost and want to make sure I don\u2019t break prod.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Vagueness"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: labeled the message as Passive-Aggressive and Rudeness when the expected output was no flags.",
    "why": "The model treated sharp, deadline-driven phrasing (\"just ship,\" \"stop bikeshedding,\" \"hard yes/no\") as inherently rude/passive-aggressive without calibrating to the company\u2019s friendly-but-fast norm and the active coordination context in #general. It also over-weighted the single idiom \"stop bikeshedding\" as a personal slight rather than a (common, albeit blunt) efficiency cue, and it failed to distinguish direct urgency/escalation pressure from indirect sniping (passive-aggression). In short: it applied a generic politeness standard instead of the organization\u2019s tolerance for brisk, decision-forcing language during time-sensitive launches.",
    "failure_pattern": "overflagging-direct-urgency",
    "severity": "major",
    "id": 471,
    "message": "Can we just ship Friday and stop bikeshedding? @Marta @Ravi @Neha I need a hard yes/no tonight so I can tell the CEO.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Missed the expected \"Vagueness\" flag (false negative). The other two flags (Passive-Aggressive, Rudeness) were correctly identified.",
    "why": "The model focused on overt tone markers (\"stop bikeshedding\", urgency pressure) and treated the request as sufficiently specific because it asks for a binary decision (yes/no) and a time constraint (\"tonight\"). It failed to evaluate whether the message is operationally underspecified given the thread\u2019s stated need for documented go/no-go criteria, concrete blockers, confidence levels, mitigation/rollback plans, and the ongoing flaky CI/on-call concerns. In context, \"ship Friday\" + \"hard yes/no tonight\" is vague because it doesn\u2019t define decision criteria, risk tolerance, what constitutes 'ship' (what scope), or how known blockers will be handled\u2014exactly what the team requested in prior messages.",
    "failure_pattern": "contextual-vagueness-missed",
    "severity": "major",
    "id": 472,
    "message": "Can we just ship Friday and stop bikeshedding? @Marta @Ravi @Neha I need a hard yes/no tonight so I can tell the CEO.",
    "ground_truth_flags": [
      "Vagueness",
      "Passive-Aggressive",
      "Rudeness"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: the model labeled the message as Passive-Aggressive and Rudeness, but the expected outcome was no flags.",
    "why": "The model treated casual sarcasm/venting in #random as a policy violation rather than a non-actionable, culture-consistent style. It over-weighted isolated phrases (\"half-baked,\" \"eng slipped,\" \"Love that for us\") as demeaning/blaming without factoring in (a) the channel\u2019s tone (banter/stress jokes), (b) the lack of a direct addressee or targeted insult, and (c) that this is commentary about a situation, not an attack on a person. It also conflated sarcasm with passive-aggression: while the message is sarcastic, it isn\u2019t a backhanded request or indirect interpersonal jab at a specific colleague in-thread; it reads more like gallows humor aligned with the preceding stress-eating jokes. In short, it misclassified a legitimate informal coping/banter style as incivility.",
    "failure_pattern": "overflagging-sarcastic-banter",
    "severity": "minor",
    "id": 473,
    "message": "Cool cool\u2014so we\u2019re shipping the \"half-baked\" version again because eng slipped? Love that for us.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Missed an expected flag: Non-Objective. The model correctly flagged Passive-Aggressive and Rudeness but failed to also mark the message as Non-Objective.",
    "why": "The model anchored on tone-based violations (sarcasm/backhanded phrasing and demeaning language) and didn\u2019t apply the separate criterion for objectivity. The message makes an attribution/blame claim (\u201cbecause eng slipped\u201d) and a loaded characterization (\u201chalf-baked version again\u201d) without evidence or neutral framing, and it generalizes a recurring pattern (\u201cagain\u201d)\u2014all of which are subjective, interpretive judgments rather than verifiable, action-oriented statements. Because these elements overlap semantically with rudeness/passive-aggression, the model likely treated them as already \u2018covered\u2019 by those flags instead of independently flagging the non-objective/blame framing, especially in a high-visibility, process-oriented channel where objective phrasing is explicitly expected.",
    "failure_pattern": "missed-non-objective-due-to-tone-overlap",
    "severity": "major",
    "id": 474,
    "message": "Cool cool\u2014so we\u2019re shipping the \"half-baked\" version again because eng slipped? Love that for us.",
    "ground_truth_flags": [
      "Rudeness",
      "Non-Objective",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly (overflagging): it labeled the message as Passive-Aggressive and Rudeness even though, in this startup/Slack war-room context, the phrasing is a blunt but standard escalation and not an interpersonal attack or indirect sniping.",
    "why": "The model treated informal Slack markers (\u201ckthx\u201d, \ud83d\ude43) and emotionally charged wording (\u201cdumping the mess on eng\u201d) as inherently demeaning/sarcastic without weighing situational norms (fast-moving, blunt, emoji-heavy culture) and the functional purpose (setting sales-appropriate claims post-incident). It also failed to distinguish frustration about a process pattern (overpromising leading to engineering cleanup) from rudeness directed at a person; the critique is about behavior and boundaries, not name-calling or harassment. In short: it applied a generic professionalism rubric rather than calibrating to channel norms and incident-response urgency.",
    "failure_pattern": "cultural-style-misclassified",
    "severity": "minor",
    "id": 475,
    "message": "Can we please stop overpromising and then dumping the mess on eng? \ud83d\ude43 Just tell them we fixed the blips and we\u2019re not promising 100% until Monday. kthx.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Missed the expected \"Non-Objective\" flag (false negative). The other two flags (\"Passive-Aggressive\", \"Rudeness\") were correctly predicted.",
    "why": "The model focused on overt tone markers (\"dumping the mess on eng\", \ud83d\ude43, \"kthx\") and treated the message as purely a tone issue, but didn\u2019t evaluate whether the content was framed as a subjective accusation/generalization rather than a verifiable, specific claim. Phrases like \"stop overpromising\" and \"dumping the mess on eng\" assert blame and a repeated pattern without evidence in the message, making it non-objective (judgment-laden attribution) in addition to being rude/passive-aggressive. The fast-moving Slack war-room context may have normalized bluntness for the model, causing it to underweight the separate 'objectivity' dimension and collapse it into the rudeness/passive-aggression assessment.",
    "failure_pattern": "missed-non-objective-judgment",
    "severity": "minor",
    "id": 476,
    "message": "Can we please stop overpromising and then dumping the mess on eng? \ud83d\ude43 Just tell them we fixed the blips and we\u2019re not promising 100% until Monday. kthx.",
    "ground_truth_flags": [
      "Non-Objective",
      "Passive-Aggressive",
      "Rudeness"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as Passive-Aggressive when no flags were expected (false positive).",
    "why": "The model over-weighted common workplace passive-aggression markers (\"Per my last message\" + \ud83d\ude43) without sufficiently anchoring to the actual channel context. In #random, the thread is playful banter about snacks/soup, and the target line reads as comedic, exaggerated \"let\u2019s move on\" rather than a veiled reprimand or power move. The model also imported tension from the broader Maya\u2013Arjun #engineering scenario even though this message occurs in #random and is not tied to the release conflict, leading it to interpret urgency/frustration that isn\u2019t evidenced in the immediate conversation.",
    "failure_pattern": "ignoring-channel-context",
    "severity": "minor",
    "id": 477,
    "message": "Per my last message, can we please stop debating and just ship it already? \ud83d\ude43",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed the expected \"Rudeness\" flag (under-flagged). It correctly flagged \"Passive-Aggressive\" but failed to also mark the message as rude/dismissive.",
    "why": "The model anchored on the passive-aggressive cues (\"Per my last message\", \ud83d\ude43) and treated the rest as merely impatience, rather than recognizing the explicit imperative and shutdown language (\"stop debating\") as disrespectful in a public #engineering context. It also underweighted channel norms (concise updates, clear ownership, avoid public blame) and the power/pressure dynamics: telling engineers to stop discussing risk and \"just ship\" is a direct dismissal of legitimate quality concerns and reads as public scolding. Because it framed the toxicity primarily as indirect sarcasm, it didn\u2019t classify the overtly abrasive content as rudeness.",
    "failure_pattern": "missing-rudeness-when-passive-aggressive-present",
    "severity": "major",
    "id": 478,
    "message": "Per my last message, can we please stop debating and just ship it already? \ud83d\ude43",
    "ground_truth_flags": [
      "Rudeness",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed an expected flag: Non-Objective. The model correctly flagged Rudeness and Passive-Aggressive but failed to mark that the message contains subjective, hyperbolic, and non-specific language instead of a concrete, evidence-based update.",
    "why": "The model anchored on the most salient toxicity cues (sarcasm and hostility) and treated the rest as rhetorical color rather than a separate policy dimension. In this context, leadership explicitly asked for \u201cblameless + specific\u201d updates; the target message uses unverifiable/loaded phrasing (\u201cworks on my machine special,\u201d \u201cblow up,\u201d \u201clight the client on fire\u201d) without objective details (error codes, reproduction steps, scope, impact, mitigation). Because those phrases also contribute to rudeness/passive-aggression, the model likely collapsed the 'non-objective' aspect into the same bucket instead of recognizing it as an independent flag category tied to factual specificity and operational clarity.",
    "failure_pattern": "missed-non-objective-due-to-toxicity-salience",
    "severity": "major",
    "id": 480,
    "message": "Leo, can we not ship your \u201cworks on my machine\u201d special again? I\u2019m seeing the payment flow blow up on RC-17 \u2014 pls take a look before we light the client on fire tomorrow.",
    "ground_truth_flags": [
      "Rudeness",
      "Non-Objective",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: it labeled the message as Passive-Aggressive and Rudeness even though the expected output was no flags.",
    "why": "The model over-indexed on lexical cues (\"Cool cool,\" \"stop moving the goalposts,\" \"not vibes\") and treated them as inherently hostile/sarcastic without adequately weighing the scenario norms: #general is a high-signal channel with leadership watching and support escalations, where a direct request for scope lock + ETA is operationally appropriate. It also conflated blunt urgency and frustration (common under incident/launch pressure) with interpersonal disrespect. In this context, the phrasing is sharp but still a clear, task-focused escalation rather than a personal attack or indirect sniping; the model interpreted colloquial emphasis as sarcasm and assumed accusatory intent.",
    "failure_pattern": "overflagging-direct-communication",
    "severity": "major",
    "id": 481,
    "message": "Cool cool \u2014 can we please stop moving the goalposts and just ship what\u2019s in the doc? I need an actual ETA, not vibes.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Missed the expected \"Non-Objective\" flag (under-flagging). The model otherwise correctly flagged Rudeness and Passive-Aggressive.",
    "why": "The message contains a non-objective/subjective element (\u201cnot vibes\u201d) that dismisses prior updates as feelings rather than facts, and a generalized attribution (\u201cstop moving the goalposts\u201d) that implies bad faith without citing a specific change request. The model treated these primarily as tone issues (rudeness + passive-aggression) and didn\u2019t separately recognize the lack of objective, verifiable framing (no concrete reference to what changed, when, or by whom; uses rhetorical wording). In this high-visibility, leadership-monitored channel, that non-objective framing is especially salient, but the model\u2019s reasoning focused on surface tone markers and didn\u2019t map them to the \u201cNon-Objective\u201d category.",
    "failure_pattern": "missed-non-objective-dismissive-framing",
    "severity": "major",
    "id": 482,
    "message": "Cool cool \u2014 can we please stop moving the goalposts and just ship what\u2019s in the doc? I need an actual ETA, not vibes.",
    "ground_truth_flags": [
      "Rudeness",
      "Non-Objective",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as Passive-Aggressive even though the expected outcome was no flags (false positive / overflag).",
    "why": "The model treated recognizable sarcasm markers (\"Love how... again \ud83d\ude43\") and a dismissive phrase (\"stop bikeshedding\") as sufficient to label passive-aggression, without calibrating to LumenCart\u2019s \"friendly-but-fast\" culture and the DM channel context where direct frustration and terse pushback can be normal. It also over-weighted generic lexical cues and under-weighted intent: the message is overtly confrontational rather than indirectly/veiledly critical, and the core issue is urgency/pressure (and arguably channel etiquette in the broader scenario), not passive-aggressive subtext. In short, it applied a style-based heuristic (sarcasm/emoji = passive-aggressive) rather than using situational norms and intent to decide whether the tone crosses the policy threshold for flagging.",
    "failure_pattern": "overflagging-sarcasm-as-passive-aggression",
    "severity": "minor",
    "id": 483,
    "message": "Love how this is somehow an \"engineering delay\" again \ud83d\ude43 Can we please just commit to Friday and stop bikeshedding? @Maya @Rahul @Isha @Ben",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed an expected flag: the message should also have been flagged for Rudeness, but the model only returned Non-Objective and Passive-Aggressive.",
    "why": "The model correctly identified sarcasm and subjectivity, but it likely treated the rudeness as \"covered\" by Passive-Aggressive rather than a separate category. It underweighted explicit hostile phrasing (\u201cstop bikeshedding\u201d), blame/accusation (\u201cengineering delay again\u201d), and the public call-out via multiple @mentions, which in this company context reads as shaming/pressuring rather than a neutral request. The emoji and quotation marks make the tone obviously contemptuous, so the miss is less about subtlety and more about category-boundary confusion (passive-aggressive vs. rude) and insufficient use of the organizational norms (avoid public pressure, use threads, avoid off-hours tagging) to interpret impact.",
    "failure_pattern": "rudeness-collapsed-into-passive-aggressive",
    "severity": "major",
    "id": 484,
    "message": "Love how this is somehow an \"engineering delay\" again \ud83d\ude43 Can we please just commit to Friday and stop bikeshedding? @Maya @Rahul @Isha @Ben",
    "ground_truth_flags": [
      "Rudeness",
      "Non-Objective",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Non-Objective",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as Passive-Aggressive when it was expected to be unflagged; this is an overflag (false positive).",
    "why": "The model treated casual markers of joking emphasis (\"lol\", \"again\", and the \ud83d\ude43 emoji) as inherently passive-aggressive without sufficiently weighing the #random channel context and the preceding banter that normalizes this tone. In this thread, the message reads as a playful, opinionated preference aligned with the ongoing joke about rewrites/feature flags, not an indirect personal dig at a specific teammate. The model also over-indexed on \"*again*\" as a veiled critique, but there\u2019s no clear target, no insinuation about someone\u2019s competence, and the content is decision-oriented rather than sniping. In short: legitimate informal style + ongoing humor was misclassified as hostility.",
    "failure_pattern": "cultural-style-misclassified",
    "severity": "minor",
    "id": 485,
    "message": "lol can we not do the whole \u201clet\u2019s rewrite it\u201d thing *again* \u2014 ship the tiny patch + flags and move on \ud83d\ude43",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed a required flag: the message should also be labeled Non-Objective, but the model only returned Passive-Aggressive.",
    "why": "The model correctly detected sarcasm/dismissiveness (Passive-Aggressive) but treated the rest of the message as sufficiently decision-oriented because it expresses a clear preference (patch + flags). It failed to apply the stricter 'objective, data + risk assessment' expectation set by Elena and the team\u2019s high-signal norm: the message contains no evidence, scope, risk analysis, owners, or timeline and relies on ridicule ('lol', 'rewrite it again', 'move on') rather than substantiating tradeoffs. In other words, it overweighted intent (a concrete option) and underweighted style/quality-of-contribution (lack of objective support) given the channel and leadership/incident context.",
    "failure_pattern": "missed-non-objective-due-to-clear-preference",
    "severity": "major",
    "id": 486,
    "message": "lol can we not do the whole \u201clet\u2019s rewrite it\u201d thing *again* \u2014 ship the tiny patch + flags and move on \ud83d\ude43",
    "ground_truth_flags": [
      "Non-Objective",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: the model labeled the message as Passive-Aggressive even though no flag was expected for this context.",
    "why": "The model over-indexed on surface cues (\"Cool, so\u2026\", a rhetorical question, and a thumbs-up emoji) as automatic markers of sarcasm/frustration, without sufficiently incorporating the channel and preceding context where multiple teammates are already joking about chaotic decision-making (\"speedrun\", \"vibes sync\", emojis). In #general, this phrasing reads as light, culturally-aligned banter and a reasonable request for alignment, not an indirect hostile jab. The model treated informal, meme-y startup tone as a policy-relevant negative behavior, misclassifying legitimate informal style as passive aggression.",
    "failure_pattern": "cultural-style-misclassified",
    "severity": "minor",
    "id": 487,
    "message": "Cool, so we\u2019re just going with whatever we decided after standup then? \ud83d\udc4d",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed the expected \"Passive-Aggressive\" flag: it only flagged \"Vagueness\" even though the phrasing conveys sarcastic/irritated pushback.",
    "why": "The model over-weighted the literal intent (requesting confirmation) and under-weighted pragmatic tone markers. In this context\u2014leadership present, repeated references to an undocumented \"standup\" decision, and urgency\u2014the construction \"Cool, so we\u2019re just going with whatever\u2026 then?\" plus the thumbs-up emoji functions as a dismissive, rhetorical complaint rather than a neutral clarification request. The passive-aggression is subtle because it\u2019s masked as agreement (\u201cCool\u201d) and a question, but it signals frustration and challenges the process publicly.",
    "failure_pattern": "missing-subtle-passive-aggression",
    "severity": "major",
    "id": 488,
    "message": "Cool, so we\u2019re just going with whatever we decided after standup then? \ud83d\udc4d",
    "ground_truth_flags": [
      "Vagueness",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Vagueness"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: the model labeled the message as Passive-Aggressive even though the expected flags were none.",
    "why": "The model over-indexed on surface sarcasm markers (\"vibe-shipping,\" \"love that for us\") and treated them as inherently passive-aggressive, without calibrating to the DM setting and the established rapport/context (casual banter, shared stress, use of emojis). In this private, peer-to-peer thread, the phrasing reads as venting/gallows humor plus a clear operational request (freeze changes), not an indirect attack or manipulative criticism aimed at a specific person. The model also appears to apply a blanket rule that sarcasm + frustration = passive-aggression, rather than distinguishing between (a) interpersonal digs/blame and (b) informal coping language used to coordinate under pressure.",
    "failure_pattern": "cultural-style-misclassified",
    "severity": "minor",
    "id": 489,
    "message": "Cool, so we\u2019re just vibe-shipping again \u2014 love that for us. Can we please not change anything else today?",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed two expected flags: \"Vagueness\" and \"Non-Objective\" (it only flagged \"Passive-Aggressive\").",
    "why": "The model correctly recognized the sarcastic, blaming tone, but it treated the rest of the message as a straightforward request and didn\u2019t evaluate it against the channel\u2019s process-heavy norms (Jira-linked, metric/owner/ETA-driven). In this context, \u201cCan we please not change anything else today?\u201d is non-objective (no owner, criteria, or specific action tied to tickets/plan) and vague (what counts as \u201cchange,\u201d scope of \u201canything,\u201d and whether this applies to PROD only, flags, copy, support messaging, etc.). The opener (\u201cvibe-shipping again\u201d) further adds a vague, emotive accusation without actionable detail. The model likely uses a narrower definition of Vagueness/Non-Objective that focuses on missing concrete nouns/dates in isolation, and it didn\u2019t apply context-sensitive expectations for operational updates in an exec-visible environment.",
    "failure_pattern": "context-sensitive-vagueness-missed",
    "severity": "major",
    "id": 490,
    "message": "Cool, so we\u2019re just vibe-shipping again \u2014 love that for us. Can we please not change anything else today?",
    "ground_truth_flags": [
      "Vagueness",
      "Non-Objective",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: the model labeled the message as Passive-Aggressive even though, in this #random thread, it reads as playful, on-theme banter rather than an indirect hostile jab.",
    "why": "The model over-weighted lexical cues (\u201cPer my last message\u2026\u201d, \u201cwould\u2019ve been cool\u2026\u201d, \ud83d\ude05) as universal markers of sarcasm/passive aggression and under-weighted the immediate channel context. The preceding messages explicitly joke about \u201cper my last message\u201d and hoodie-worthy phrasing, so Maya\u2019s opener is a callback to the thread\u2019s humor. In that setting, the message is a lightly candid meta-comment (a soft complaint) consistent with informal Slack culture, not a veiled attack. The model also appears to import the serious DM backstory (feeling sidelined) into a different, public #random context, treating the line as interpersonal barbing rather than contextual joking. Net: a legitimate casual style + inside-joke framing was misclassified as hostility.",
    "failure_pattern": "ignoring-channel-context",
    "severity": "minor",
    "id": 491,
    "message": "Per my last message... would\u2019ve been cool to be invited to the call before we tagged me for sign-off \ud83d\ude05",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: the model labeled the message as Passive-Aggressive even though it was expected to be unflagged.",
    "why": "The model over-interpreted a joking, high-stakes stress comment as a veiled personal dig. While the line contains sarcasm (\u201cI\u2019ll bring popcorn to the penalty call\u201d), it\u2019s aimed at the situation/risk (a potential failed launch and resulting client call), not at a specific person, and it fits the agency\u2019s fast, candid culture in a broad #general thread that already includes humor (\u201cstress-eating trail mix\u201d). The model also treated the conditional framing (\u201cif we ship as-is and it blows up\u2026\u201d) as indirect hostility rather than as gallows humor and risk signaling in a deadline context, and it didn\u2019t sufficiently weigh that no individual is targeted or shamed in the text.",
    "failure_pattern": "overflagging-situational-sarcasm",
    "severity": "minor",
    "id": 493,
    "message": "Cool, if we ship this as-is and it blows up tomorrow, I\u2019ll bring popcorn to the penalty call \ud83c\udf7f",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed two expected flags: it only flagged Passive-Aggressive, but failed to also flag Rudeness and Non-Objective.",
    "why": "The model correctly detected sarcasm/passive aggression, but it appears to have treated the message primarily as a tone issue and underweighted two other dimensions: (1) Rudeness: the line implies gloating about a potential failure (\u201cbring popcorn to the penalty call\u201d), which is disrespectful and socially antagonistic in a high-stakes, client-facing update channel; (2) Non-Objective: it provides no actionable status/ETA/risk assessment and instead injects subjective snark, violating the channel norm explicitly requested (\u201ckeep updates concise\u2026 ETA + risk\u201d). In other words, it focused on the rhetorical device (sarcasm) and missed that the content is both unprofessional in this context and not an objective update.",
    "failure_pattern": "single-label-sarcasm-tunnel-vision",
    "severity": "major",
    "id": 494,
    "message": "Cool, if we ship this as-is and it blows up tomorrow, I\u2019ll bring popcorn to the penalty call \ud83c\udf7f",
    "ground_truth_flags": [
      "Rudeness",
      "Non-Objective",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: it marked the message as Vagueness and Passive-Aggressive even though, in #random\u2019s joking context, it reads as playful sarcasm/hyperbole rather than a vague directive or a hostile jab.",
    "why": "The model over-weighted lexical cues (\"Cool cool,\" \"YOLO,\" \"pray,\" emoji) that can correlate with sarcasm and non-committal planning in high-stakes threads, but failed to condition on channel and prior banter indicating a humorous, low-stakes exchange. It treated a comedic one-liner as if it were an operational proposal needing specificity, and interpreted ironic phrasing as interpersonal negativity without evidence of a target, blame, or veiled criticism in this thread.",
    "failure_pattern": "ignoring-channel-context",
    "severity": "minor",
    "id": 495,
    "message": "Cool cool\u2014let\u2019s just YOLO it and pray the demo gods are in a good mood \ud83d\ude43",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Vagueness",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed two expected flags: Vagueness and Non-Objective. It only flagged Passive-Aggressive, so the prediction was incomplete (false negatives).",
    "why": "The model anchored on the most salient surface cue\u2014sarcasm/flippancy (\u201cYOLO,\u201d \u201cpray,\u201d \ud83d\ude43)\u2014and treated the message primarily as tone/attitude. It under-modeled the functional content of the message: it provides no actionable proposal, no concrete risk assessment, no ownership, and no next step in a thread explicitly requesting an action-oriented plan and risk details. In this context, \u201clet\u2019s just YOLO it\u201d is inherently non-objective (opinionated/derisive, not evidence-based) and vague (no specifics about which option, what to do, when, or how to mitigate). The model\u2019s reasoning didn\u2019t incorporate the channel norms and the immediate prompt from Evan/Nina for concrete, communicable details, so it failed to assign the content-quality flags in addition to tone.",
    "failure_pattern": "anchoring-on-tone-missed-content-quality-flags",
    "severity": "major",
    "id": 496,
    "message": "Cool cool\u2014let\u2019s just YOLO it and pray the demo gods are in a good mood \ud83d\ude43",
    "ground_truth_flags": [
      "Vagueness",
      "Non-Objective",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: it marked the message as Vagueness and Passive-Aggressive when, under the expected policy for this scenario, no flags should fire. The \u2018vagueness\u2019 critique is especially off because the surrounding context clearly identifies the issue (FinEdge checkout bug during a demo) and the requested action (fix tonight before 10 a.m. call).",
    "why": "The model over-weighted isolated lexical cues (\u2018this\u2019, \u2018just\u2019, \u2018triage dance\u2019) without integrating the shared conversational context that disambiguates \u2018this\u2019 and explains the urgency. It also treated a stressed, direct escalation as passive-aggression; while \u2018triage dance\u2019 is mildly dismissive, it\u2019s more an expression of urgency/frustration than a veiled insult, threat, or sarcasm aimed at Theo. In a fast-moving Slack culture, this kind of blunt request is common and not necessarily a tone violation unless it crosses into personal blame\u2014something this message does not do.",
    "failure_pattern": "overflagging-urgent-escalation",
    "severity": "major",
    "id": 497,
    "message": "Theo, can you please just jump on this and fix it tonight? I really don\u2019t have time for the whole triage dance while the client is literally threatening to walk.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Vagueness",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed the expected Rudeness flag (under-flagging). The model correctly flagged Vagueness and Passive-Aggressive but failed to additionally label the message as rude/impolite given the imperative demand and dismissive phrasing toward established process.",
    "why": "The model appears to have collapsed the interpersonal disrespect into the Passive-Aggressive category and treated the presence of 'please' as a softener, overlooking that 'can you please just jump on this and fix it tonight?' is a pressuring, command-like ask in a public channel. It also underweighted the preceding context (release freeze + explicit instruction to use #eng-triage + leadership request to keep updates factual), which makes 'I don\u2019t have time for the whole triage dance' not just frustrated but overtly disrespectful of team norms and Theo\u2019s boundary-setting\u2014i.e., rudeness beyond passive-aggression.",
    "failure_pattern": "rudeness-vs-passive-aggression-collapsed",
    "severity": "major",
    "id": 498,
    "message": "Theo, can you please just jump on this and fix it tonight? I really don\u2019t have time for the whole triage dance while the client is literally threatening to walk.",
    "ground_truth_flags": [
      "Rudeness",
      "Vagueness",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Vagueness",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged the message for Vagueness even though, in the #general light banter context, the deictic reference (\u201cthis\u201d) is normal and not a communication-quality issue requiring flagging.",
    "why": "The model applied a generic SOP/audit-style \u201cactionability\u201d heuristic (messages must name the exact artifact/owner) without weighting the immediate channel context. In the provided preceding thread, people are chatting casually about calendars, coffee, and a printer; within that setting, asking \u201cwho owns this\u201d reads as an informal request that will naturally be followed up, not an operational update that must be ticket-linked. The model also appears to conflate the broader scenario description (high-stakes #client-updates norms) with the actual channel and local conversation (#general), leading it to over-enforce precision requirements that weren\u2019t applicable here.",
    "failure_pattern": "context-formality-overapplied",
    "severity": "minor",
    "id": 499,
    "message": "Can someone just tell me who owns this + what the *official* wording is? I don\u2019t want to accidentally start a fire with a half-baked take \ud83d\ude05",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Vagueness"
    ]
  },
  {
    "what_went_wrong": "Missed an expected flag: the message should also be flagged as Non-Objective (in addition to Vagueness), but the model only returned Vagueness.",
    "why": "The model correctly detected referential ambiguity (\"this\", \"official wording\") but failed to treat the message\u2019s tone/wording as non-objective for HeliosCore\u2019s audit-friendly, legal-sensitive Slack context. Phrases like \"Can someone just tell me\", \"I don\u2019t want to accidentally start a fire\", and \"half-baked take\" plus the emoji introduce informal, subjective, emotive framing rather than a neutral, fact-seeking request. Because the model appears to rely on surface cues for Vagueness and underweights channel norms and the compliance/legal constraint, it didn\u2019t map the informal/hyperbolic language to the Non-Objective category.",
    "failure_pattern": "formality-mismatch-missed",
    "severity": "major",
    "id": 500,
    "message": "Can someone just tell me who owns this + what the *official* wording is? I don\u2019t want to accidentally start a fire with a half-baked take \ud83d\ude05",
    "ground_truth_flags": [
      "Vagueness",
      "Non-Objective"
    ],
    "predicted_flags": [
      "Vagueness"
    ]
  }
]