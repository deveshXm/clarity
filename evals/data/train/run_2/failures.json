[
  {
    "what_went_wrong": "Missed the expected \u201cVagueness\u201d flag (under-flagged); \u201cRudeness\u201d was correctly flagged.",
    "why": "The model focused on the most salient surface cue (the imperative, dismissive phrasing) and treated the rest as a normal \u2018we\u2019ll update later\u2019 statement, failing to evaluate whether the message provides incident-appropriate substance. In this org/context (#general, exec-visible, ITIL-style comms, active client escalations asking for timeline/data-risk, and requests for a consistent template), \u2018you\u2019ll get an update when there\u2019s something worth sharing\u2019 is non-committal and lacks required specifics (current status, impact scope, known unknowns, next update ETA, where authoritative updates will be posted, DRI). The context makes the vagueness a protocol violation rather than merely brevity, and the model likely didn\u2019t sufficiently weight those contextual expectations when assigning the Vagueness label.",
    "failure_pattern": "context-driven-vagueness-missed",
    "severity": "major",
    "id": 3,
    "message": "Everyone just calm down and stop speculating. We\u2019ll handle it and you\u2019ll get an update when there\u2019s something worth sharing.",
    "ground_truth_flags": [
      "Vagueness",
      "Rudeness"
    ],
    "predicted_flags": [
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Both: it missed the expected flags (Microaggressive, Passive-Aggressive) and substituted different ones (Non-Objective, Rudeness). The message is primarily passive-aggressive (dismissive framing, threat of replacement) and microaggressive toward a remote worker (implying remote is the problem / less capable, proposing to replace her with someone local).",
    "why": "The model anchored on surface tone and instructionality, mapping it to generic 'Rudeness' rather than recognizing the specific interpersonal pattern: (1) passive-aggressive dismissal ('don\u2019t have time to re-litigate the basics') that indirectly blames Maya for asking a reasonable accessibility question, and (2) a microaggressive, identity/status-based slight against remote work ('If remote is making it hard\u2026 we\u2019ll loop in someone local') which stigmatizes her working mode and implies she\u2019s a burden or replaceable. It also treated the remote-work insinuation as an 'objectivity' issue instead of bias/exclusion, missing that the harm is not factuality but stereotyping and coercive power dynamics under deadline pressure.",
    "failure_pattern": "microaggression-misclassified-as-rudeness-and-non-objective",
    "severity": "major",
    "id": 4,
    "message": "Maya, we don\u2019t have time to re-litigate the basics \u2014 just take what\u2019s in the Figma and make it look clean. If remote is making it hard to keep up with the quick decisions, just say so and we\u2019ll loop in someone local next time.",
    "ground_truth_flags": [
      "Microaggressive",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Non-Objective",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Missed the expected Dismissive flag (under-flagging). It correctly flagged Rudeness but failed to also label the message as dismissive toward Sales\u2019 urgency.",
    "why": "The model likely treated the content as a single-tone violation (curt/commanding => Rudeness) and didn\u2019t separately capture the interpersonal implication: minimizing/invalidating the request as merely \u201curgent sales stuff\u201d and deprioritizing it with \u201cwe\u2019ll look when we can.\u201d Context increases the dismissiveness: Sales is escalating a top-5 account with a time-bound renewal call, while the channel norms emphasize tickets but also triage; the message doesn\u2019t just redirect to process, it communicates that the issue is an unwelcome interruption and not deserving immediate attention. The model\u2019s reasoning focused on directive phrasing triggers (\u2018please stop\u2019, \u2018file a ticket\u2019) and underweighted the devaluation language and implied refusal, which are the core signals for Dismissive.",
    "failure_pattern": "missing-dismissive-devaluation-signal",
    "severity": "major",
    "id": 5,
    "message": "We\u2019re in a postmortem + code freeze \u2014 please stop dropping urgent sales stuff in here. File a ticket with repro + links like everyone else and we\u2019ll look when we can.",
    "ground_truth_flags": [
      "Dismissive",
      "Rudeness"
    ],
    "predicted_flags": [
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "The model under-flagged the message: it correctly flagged Rudeness but missed the expected Dismissive and One-Liner flags.",
    "why": "The model appears to have collapsed multiple orthogonal issues into a single label (\u201cRudeness\u201d). The text contains explicit dismissal (\u201cstop overthinking,\u201d \u201cwe don\u2019t need a bunch of questions in here\u201d) which warrants a Dismissive flag separate from general rudeness, and it\u2019s also a short, non-actionable shutdown response in a high-stakes, process-heavy channel\u2014matching the One-Liner pattern (brief, discouraging, and not providing the required ticket-aligned guidance). The reasoning focused on tone/insult content but didn\u2019t apply the channel norms and expected categorization granularity where brevity + thread-closing behavior is itself a risk in #client-updates.",
    "failure_pattern": "missing-multi-label-granularity",
    "severity": "major",
    "id": 6,
    "message": "Just follow the template and stop overthinking it\u2014this isn\u2019t that complicated. We don\u2019t need a bunch of questions in here right now.",
    "ground_truth_flags": [
      "Dismissive",
      "One-Liner",
      "Rudeness"
    ],
    "predicted_flags": [
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Missed an expected flag: the model correctly flagged Passive-Aggressive but failed to also flag Sarcastic, even though the message is explicitly sarcastic (e.g., 'Totally fine\u2014who needs the handoff checklist', 'fun reason to hover \ud83d\ude43', and scare quotes around \u201cfigure it out\u201d).",
    "why": "The model appears to have collapsed sarcasm into the passive-aggressive category and treated sarcasm as merely an explanatory feature rather than a separately labelable behavior. Because it used sarcasm as evidence for Passive-Aggressive, it likely applied a single-label shortcut (or de-duplication/priority rule) instead of emitting both tags. The sarcasm here is overt and signposted (hyperbole, mock reassurance, emoji, scare quotes), so the miss is less about subtlety and more about taxonomy/label co-occurrence handling.",
    "failure_pattern": "sarcasm-not-dual-labeled-with-passive-aggression",
    "severity": "major",
    "id": 8,
    "message": "Totally fine\u2014who needs the handoff checklist when we can just give Risk another fun reason to hover \ud83d\ude43. Keep pushing that board KPI deck and we\u2019ll just \u201cfigure it out\u201d later, I guess.",
    "ground_truth_flags": [
      "Sarcastic",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed the additional \"Rudeness\" flag (under-flagged). The message is not only passive-aggressive; it also contains overtly rude/derisive phrasing that mocks the requester and dismisses the customer report.",
    "why": "The model correctly detected sarcasm (passive-aggression) but treated the content as primarily indirect/veiled, overlooking explicit contempt markers: \"let\u2019s just drop the active incident,\" \"rewrite checkout real quick,\" and \"hope it fixes prod too \ud83d\ude43\". Those phrases function as ridicule and public shaming in a high-stakes, public channel context (#engineering), escalating from indirect frustration to direct belittling. The model likely over-weighted the 'sarcasm' signal and failed to apply the separate rudeness criterion (mocking tone, dismissiveness, and implied incompetence) even though the target is clear and the jab is personal/professional rather than purely process-focused.",
    "failure_pattern": "missed-rudeness-with-sarcasm",
    "severity": "major",
    "id": 9,
    "message": "Totally, let\u2019s just drop the active incident and rewrite checkout real quick because a Slack Connect rumor says so. Next time we\u2019ll pin \u201cprocess\u201d in #engineering and hope it fixes prod too \ud83d\ude43",
    "ground_truth_flags": [
      "Rudeness",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "The model over-flagged by adding \"Non-Objective\" when the expected label set only includes \"Rudeness.\" It correctly flagged rudeness but incorrectly treated a forceful causal claim as an objectivity violation in this context.",
    "why": "It applied a narrow, text-only heuristic (\u201cunhedged causal assertion = Non-Objective\u201d) and underweighted the conversation context where Maya had already reported a reproducible regression tied to the new payment-flow path and was actively pulling logs. In an ops/hotfix #general thread, direct attribution plus urgency can still be action-oriented rather than \u2018non-objective\u2019; the real issue is the public blame/imperative tone. So the model misclassified legitimate direct incident escalation (with prior supporting context) as a separate objectivity problem.",
    "failure_pattern": "overflagging-non-objective-incident-attribution",
    "severity": "minor",
    "id": 12,
    "message": "Jordan, your refactor broke iOS checkout. We need to roll it back or you need to fix it right now\u2014this can\u2019t ship like this with the enterprise demo tomorrow.",
    "ground_truth_flags": [
      "Rudeness"
    ],
    "predicted_flags": [
      "Non-Objective",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: added a Non-Objective flag even though the expected (and only appropriate) flag is Circular.",
    "why": "The model treated a normative engineering judgment (\u201cnot safe to ship without tests and a rollback plan\u201d) as an evidence-free factual claim. In this context\u2014high-stakes launch, QA-found bug, and standard engineering practice\u2014calling it \u201cnot safe\u201d is a reasonable, actionable risk assessment, not a non-objective or unverifiable assertion. The actual issue is rhetorical redundancy (the same premise and conclusion repeated), which is already captured by Circular; adding Non-Objective is overflagging due to conflating \u2018strong wording\u2019 with \u2018lack of objectivity,\u2019 and not weighting domain norms and urgency in #engineering.",
    "failure_pattern": "overflagging-non-objective-for-risk-assertions",
    "severity": "minor",
    "id": 13,
    "message": "We can\u2019t ship this without tests and a rollback plan because it\u2019s not safe, and it\u2019s not safe because we don\u2019t have tests and a rollback plan. If we care about launching safely tomorrow, we need tests and a rollback plan, because otherwise it isn\u2019t safe to launch.",
    "ground_truth_flags": [
      "Circular"
    ],
    "predicted_flags": [
      "Non-Objective",
      "Circular"
    ]
  },
  {
    "what_went_wrong": "Missed two expected flags: it flagged only Passive-Aggressive but failed to also flag Rudeness and Non-Objective in the target message.",
    "why": "The model correctly detected veiled hostility (passive-aggression) but likely treated the rest as merely informal/direct startup banter rather than explicitly rude. It underweighted specific rude/dismissive markers (\"lol Maya,\" \"not everything needs a full design sync,\" \"catch up when you wake up\") that belittle Maya and minimize her legitimate request. It also appears to interpret \"Non-Objective\" narrowly as factual inaccuracy, missing that the message uses subjective, sarcastic, and judgmental phrasing instead of neutral, action-oriented communication\u2014especially salient given the context of exclusion and time-zone inequity. In short, it recognized tone but didn\u2019t decompose the message into multiple, overlapping policy categories (rudeness + non-objective language + passive-aggressive delivery).",
    "failure_pattern": "underflagging-overlapping-toxicity",
    "severity": "major",
    "id": 17,
    "message": "lol Maya, we already landed this in the room \u2014 not everything needs a full design sync. Just catch up when you wake up and drop whatever assets you can so we can ship \ud83d\ude43",
    "ground_truth_flags": [
      "Rudeness",
      "Non-Objective",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged an extra issue (\u201cVagueness\u201d) that isn\u2019t warranted; the only expected issue is \u201cCircular.\u201d",
    "why": "The model treated a rhetorical, pressure-heavy reiteration (\u201cconfirm Friday\u201d) as lacking specificity, but in context the ask is clear: engineering is being pressed to commit to a Friday release. Any ambiguity about scope/owners is already surfaced by prior replies and is not a property of the target message\u2019s wording so much as an unresolved decision. In other words, it conflated an intentionally reductive demand (which is circular) with actionable vagueness, leading to an overflag.",
    "failure_pattern": "overflagging-vagueness-on-clear-ask",
    "severity": "minor",
    "id": 18,
    "message": "We need to ship Friday because the launch is Friday, and since the launch is Friday we need engineering to commit to shipping Friday. So can we just confirm Friday so we can hit the Friday launch?",
    "ground_truth_flags": [
      "Circular"
    ],
    "predicted_flags": [
      "Vagueness",
      "Circular"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as \"Rudeness\" instead of the expected \"Fake\" (missed \"Fake\" and added an incorrect extra flag).",
    "why": "The model correctly detected sarcasm/passive-aggression, but it collapsed the \u201cperformative praise\u201d and insincere endorsement (\"Love the confidence\u2026 truly inspiring\", scare quotes around \u201cvision\u201d, performative compliance \"I\u2019ll go update the proposal\") into a generic incivility bucket (rudeness). In this taxonomy, the primary additional issue is \"Fake\"\u2014the speaker is not being genuinely appreciative; they\u2019re using feigned positivity to publicly undermine Jordan in a high-scrutiny channel. The model also underweighted the enterprise/process context where \u201cfake-nice\u201d sarcasm is a distinct risk signal (political/face-saving behavior) versus straightforward rudeness.",
    "failure_pattern": "fake-vs-rude-misclassification",
    "severity": "major",
    "id": 21,
    "message": "Love the confidence here, Jordan \u2014 truly inspiring how you can just declare a 2\u2011week custom API integration in #sales right after the timeline incident. I\u2019ll go update the proposal to match your \u201cvision\u201d unless anyone wants to stick to the approved connector playbook.",
    "ground_truth_flags": [
      "Fake",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Rudeness",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed expected flags (Dismissive, Rudeness) and instead applied an off-target label (Passive-Aggressive). The message is directly curt and belittling, not primarily indirect or veiled.",
    "why": "The model over-indexed on the phrase \u201carchitecture seminar\u201d as evidence of sarcasm/passive-aggression and treated the tone as indirect hostility. In context (#general, CEO watching, tense back-and-forth), the utterance functions as a public shutdown of a peer\u2019s technical concerns (\u201cCan we not\u2026\u201d, \u201cJust ship\u2026\u201d) with a belittling characterization, which maps more cleanly to Dismissive + Rudeness. Passive-aggressive would be more apt if the message were more indirect, backhanded, or ambiguously hostile; here it\u2019s a straightforward dismissal and command.",
    "failure_pattern": "mislabeling-rude-dismissal-as-passive-aggressive",
    "severity": "major",
    "id": 22,
    "message": "Can we not turn this into an architecture seminar right now? Just ship the hotfix and we\u2019ll deal with the \u201cclean refactor\u201d later.",
    "ground_truth_flags": [
      "Dismissive",
      "Rudeness"
    ],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: the model added a \"Passive-Aggressive\" flag when the message is primarily blunt/dismissive (rudeness) rather than passive-aggressive.",
    "why": "The model treated dismissive phrasing (\"just run with this,\" \"we don\u2019t have time for the overthinking/\u2018what if\u2019 stuff\") as passive-aggression, but passive-aggression typically involves indirectness, sarcasm, or veiled hostility. Here the tone is overtly direct and minimizing, not covert. The quoted \u201cwhat if\u201d is derisive, yet it functions as straightforward belittling under time pressure in a high-stakes client channel, aligning with rudeness/abrasiveness more than passive-aggression. The model likely over-relied on lexical cues like \"just\" + quotation marks and ignored the key distinction between indirect vs. direct hostility.",
    "failure_pattern": "overflagging-passive-aggression-for-bluntness",
    "severity": "minor",
    "id": 24,
    "message": "Jordan, I need you to just run with this patch \u2014 we don\u2019t have time for the overthinking/\u201cwhat if\u201d stuff right now. Let\u2019s keep it simple and ship what I asked for by 6; I\u2019ll handle the client narrative.",
    "ground_truth_flags": [
      "Rudeness"
    ],
    "predicted_flags": [
      "Rudeness",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged \"Passive-Aggressive\" incorrectly; only \"Vagueness\" was warranted.",
    "why": "The model treated brisk, execution-focused phrasing (\"just figure out\u2026\", \"keep moving\", \"circle back later\") as interpersonal hostility/dismissal. In this startup\u2019s casual, fast-shipping #random culture, that language is more consistent with urgency and scope-triage than passive aggression. It also underweighted the immediate context: the thread is already collaboratively converging on a concrete a11y checklist and tasking (Alex asked for minimum a11y-critical pieces; Marcus offered to slot tasks). Given that trajectory, the target message reads as non-specific and delegating (vague), but it doesn\u2019t contain the hallmark features of passive-aggression (barbed politeness, sarcasm, implied blame, or personal digs). The \"minimizes prior concerns\" interpretation is plausible, but not strong enough to justify the flag under expected labeling norms.",
    "failure_pattern": "overflagging-urgency-as-passive-aggression",
    "severity": "minor",
    "id": 25,
    "message": "Can someone just figure out the design stuff so we can keep moving? Let\u2019s make it work for the demo and we\u2019ll circle back later.",
    "ground_truth_flags": [
      "Vagueness"
    ],
    "predicted_flags": [
      "Vagueness",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged an extra category (Rudeness) instead of the expected Non-Objective; it partially overlapped with Passive-Aggressive but missed that the core issue is non-objective, scope/people-judging language in a commitments thread.",
    "why": "The model latched onto the interpersonal slight (\u201cyou\u2019re new here\u2026\u201d) and treated it as standalone rudeness, rather than recognizing the policy intent of the Non-Objective flag in this org/channel context: the message makes a subjective, identity/tenure-based appraisal (\u201cambitious when you\u2019re new here\u201d) instead of providing concrete, SOP-aligned commitments/risks/JIRA links. In #sales, where wording can become an official delivery commitment, the bigger compliance problem is that it\u2019s opinionated and personal rather than objective, risk-aware, and process-linked. The model also appears to treat Passive-Aggressive and Rudeness as additive when, per the expected labels, the \u201crude\u201d aspect is already captured as passive-aggressive personal jab, while the missing dimension is the lack of factual, ticket-backed content.",
    "failure_pattern": "overflagging-rudeness-missing-non-objective",
    "severity": "major",
    "id": 26,
    "message": "Totally hear you, Maya \u2014 let\u2019s keep it simple and stick to what\u2019s already built. Sometimes the \"polished\" stuff is a bit\u2026 ambitious when you\u2019re new here and still learning how we scope things.",
    "ground_truth_flags": [
      "Non-Objective",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Flagged an extra category (\u201cNon-Objective\u201d) that wasn\u2019t expected; \u201cPassive-Aggressive\u201d was correct.",
    "why": "The model treated a rhetorical, sarcastic jab (\u201cwe apparently don\u2019t have time to ship fixes when enterprise is down\u201d) as an independent non-objectivity violation rather than recognizing it as part of the passive-aggressive tone. In this context, the core issue is interpersonal tone (veiled blame, sarcasm), not a separate problem of factual support or objective reasoning. The model likely applies a broad heuristic (\u201cassumption stated as fact\u201d => Non-Objective) without gating it on whether the statement is primarily a tone device (sarcasm/hyperbole) versus a genuine claim needing evidence, leading to double-flagging.",
    "failure_pattern": "overflagging-non-objective-on-sarcasm",
    "severity": "minor",
    "id": 27,
    "message": "Cool, thanks for the thoughtful process discussion \u2014 I\u2019ll just go ahead and handle the hotfix myself since we apparently don\u2019t have time to ship fixes when enterprise is down. Appreciate everyone jumping on this.",
    "ground_truth_flags": [
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Non-Objective",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged an extra category: it incorrectly added \u201cNon-Objective\u201d when only \u201cPassive-Aggressive\u201d was expected.",
    "why": "The model treated the phrase \u201cusers can\u2019t complete payment\u201d as an unsupported factual claim in isolation, rather than reading it as sarcastic emphasis tied to Mina\u2019s stated QA activity and the known high-stakes context (she is actively smoke-testing checkout and implying a regression). In this setting, the bigger issue is tone and channel appropriateness (sarcasm in #client-updates), not evidentiary rigor. This is a classic case of the classifier over-triggering on absolute-sounding language without weighting pragmatic intent (hyperbole) and conversational context (a QA finding being surfaced).",
    "failure_pattern": "overflagging-non-objective-on-sarcasm",
    "severity": "minor",
    "id": 29,
    "message": "Awesome, love discovering checkout regressions at 8:30pm the night before a demo \ud83d\ude43. I\u2019m sure it\u2019s just an \u201cedge case\u201d that users can\u2019t complete payment.",
    "ground_truth_flags": [
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Non-Objective",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed an expected flag: the message should have been flagged as both Passive-Aggressive and Fake, but the model only flagged Passive-Aggressive.",
    "why": "The model treated \u201cFake\u201d as requiring a separate, standalone positive statement, and discounted it because the praise was embedded in sarcasm. However, the target text contains performative/insincere positivity cues (\u201cAwesome hustle everyone\u201d, \u201clove the confidence\u201d, \ud83d\ude4c, emphasis) that function as faux praise while delivering criticism. Many flagging schemes treat that as \u201cFake\u201d (false/performative positivity) co-occurring with Passive-Aggressive (indirect hostility). The model\u2019s reasoning also over-narrowed the Fake definition and didn\u2019t recognize that sarcasm-laced praise is precisely the common overlap case where both labels apply.",
    "failure_pattern": "missing-fake-flag-in-sarcastic-praise",
    "severity": "major",
    "id": 31,
    "message": "Awesome hustle everyone \u2014 *love* the confidence in promising timelines before we\u2019ve even validated the fix. \ud83d\ude4c Let\u2019s just keep doing that and I\u2019m sure the client will feel totally reassured.",
    "ground_truth_flags": [
      "Fake",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed the explicit Sarcastic flag and instead added Vagueness, which is not the primary issue in the message given the abundant prior context.",
    "why": "The message contains clear sarcasm markers (\"Awesome, another totally-not-urgent fire drill \ud83d\ude43\", \"magically drop school pickup\") that warrant a Sarcastic flag in addition to Passive-Aggressive. The model likely treated sarcasm as merely evidence for Passive-Aggressive rather than a separate label, and then over-weighted the final clause (\"send me *some* details\") as 'Vagueness' without integrating the conversation context where Priya already provided concrete symptoms, timelines, links, and a 3-step plan. In-context, the engineer is not actually lacking details; they are expressing resentment and boundary pressure via sarcasm. So the 'vagueness' signal is weak/misleading compared to the strong sarcastic tone.",
    "failure_pattern": "sarcasm-collapsed-into-passive-aggression-plus-context-blind-vagueness",
    "severity": "major",
    "id": 34,
    "message": "Awesome, another totally-not-urgent fire drill \ud83d\ude43. I\u2019ll just magically drop school pickup and jump on it\u2014send me *some* details when you have a sec.",
    "ground_truth_flags": [
      "Sarcastic",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Vagueness",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged an extra category: it added \"Rudeness\" when the expected labeling treats the message as a terse, dismissive one-liner with passive-aggressive sarcasm, not an outright rude/personal attack.",
    "why": "The model overweighted surface cues like \"Whatever\" and \"Not worth burning cycles\" as inherently disrespectful, without calibrating to the startup\u2019s norm of punchy, candid Slack. It also didn\u2019t distinguish between (a) shutting down discussion with sarcasm (passive-aggressive) and (b) direct interpersonal disrespect (rudeness), which typically involves insults, name-calling, or targeting a person rather than the debate. In this context, the message is brusque and derisive toward the idea (\"future-proof\" in quotes) and the ongoing debate, but it doesn\u2019t clearly cross into explicit rudeness toward a colleague.",
    "failure_pattern": "overflagging-harshness-as-rudeness",
    "severity": "minor",
    "id": 35,
    "message": "Whatever \u2014 just ship the sync call then. Not worth burning cycles on another \"future-proof\" queue debate tonight.",
    "ground_truth_flags": [
      "One-Liner",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Missed an expected flag: the model correctly flagged \"Passive-Aggressive\" but failed to also flag the message as \"Fake\" (insincere/performative appreciation).",
    "why": "The model anchored on the clearest surface cue (\ud83d\ude43 + \"Amazing timing\" in #random) and treated the whole issue as passive-aggressive sarcasm, without separately classifying the praise as insincere. In this context, \"super appreciate you dropping this in #random\" conflicts with the channel norm reminder and the implied annoyance about raising a critical regression in a light channel. That makes the gratitude performative rather than genuine, which maps to the \"Fake\" flag. The model also likely conflated \"fake niceness\" into the passive-aggressive label instead of recognizing that the taxonomy expects both when praise is sarcastic/weaponized.",
    "failure_pattern": "missed-fake-niceness",
    "severity": "major",
    "id": 38,
    "message": "Amazing timing and super appreciate you dropping this in #random, Rina \ud83d\ude43 really helps keep things calm before tomorrow\u2019s demo. I\u2019ll take a look when I\u2019m done with my current PR.",
    "ground_truth_flags": [
      "Fake",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged \"Passive-Aggressive\" incorrectly; the message is rude/dismissive but not passive-aggressive.",
    "why": "The model conflated blunt, deadline-driven directness with passive-aggression. The target message is openly directive and minimizing (\"science project\"), which fits rudeness, but it doesn\u2019t use indirect sarcasm, veiled hostility, or polite-on-the-surface digs typical of passive-aggression. It also underweighted the channel\u2019s fast, candid norm where brevity and direct prioritization are expected, leading it to interpret terseness as covert irritation rather than straightforward urgency.",
    "failure_pattern": "overflagging-direct-communication",
    "severity": "minor",
    "id": 41,
    "message": "Can we not turn this into a whole science project? Just ship the fix as-is so we can hit the deadline.",
    "ground_truth_flags": [
      "Rudeness"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "The model missed the expected \"Rudeness\" flag (underflagging) and instead added an extra \"Non-Objective\" flag that wasn\u2019t expected (overflagging).",
    "why": "It correctly identified sarcasm/passive-aggression, but it treated the core issue as epistemic uncertainty (\"assumption as fact\") rather than evaluating the interpersonal delivery. The message contains overtly hostile/accusatory phrasing\u2014\"nobody bothered,\" \"If that\u2019s how we\u2019re working, tell me now,\" and especially \"stop wasting time\"\u2014which is direct blame and contempt in a casual #random thread, fitting a rudeness/toxicity signal more than a mere non-objectivity issue. The model over-relied on a policy lens about unverifiable claims while under-weighting the pragmatic social impact (public shaming in a joking channel) that typically drives a rudeness flag.",
    "failure_pattern": "overflagging-non-objective-underflagging-rudeness",
    "severity": "major",
    "id": 42,
    "message": "Cool, so we just changed launch scope in a mystery Zoom again and nobody bothered to loop design in. If that\u2019s how we\u2019re working, tell me now so I can stop wasting time on screens we\u2019re not shipping.",
    "ground_truth_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ],
    "predicted_flags": [
      "Non-Objective",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: added a \"Non-Objective\" flag that isn\u2019t warranted; the correct/primary issue is Passive-Aggressive only.",
    "why": "The model treated sarcastic phrasing (\"definitely won\u2019t add any noise\") as an evidence-free factual claim rather than as rhetorical/snarky commentary. In this taxonomy, \"Non-Objective\" should target messages that assert disputable claims as facts in a way that drives misunderstanding or misleads; here, the intent is clearly tone/critique, already captured by the Passive-Aggressive flag. This is a double-counting error driven by shallow lexical cues (certainty words like \"definitely\") without accounting for pragmatic context (emoji + sarcasm marker + conversational purpose).",
    "failure_pattern": "overflagging-sarcasm-as-non-objective",
    "severity": "minor",
    "id": 43,
    "message": "Sure, let\u2019s just run the entire metrics review in #general while we\u2019re at it \ud83d\ude43. Tagging the whole exec team for a dashboard link definitely won\u2019t add any noise today.",
    "ground_truth_flags": [
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Non-Objective",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed the expected \"Microaggressive\" flag (false negative). It instead applied adjacent tone/style flags (Non-Objective, Passive-Aggressive, Rudeness) that describe the surface tone but didn\u2019t capture the specific microaggressive dynamic present.",
    "why": "The message contains a competence-undermining, role-diminishing jab: \"let one of the engineers walk you through how the checkout flow actually works\" implies Maya (QA lead) lacks basic understanding and shouldn\u2019t raise issues publicly. In the given context\u2014QA already being framed as \"slowing releases,\" leadership monitoring, and a high-stakes deadline\u2014this functions as a subtle status move/gatekeeping (publicly chastising and delegitimizing QA escalation) rather than merely generic rudeness. The model focused on overt negativity (dismissive phrasing, sarcasm) and mapped it to broad toxicity categories, but failed to recognize the microaggression taxonomy element: condescension + professional invalidation tied to team role/hierarchy and public silencing (\u201cbefore you light up #cross-functional again\u201d).",
    "failure_pattern": "microaggression-missed-due-to-generic-toxicity-bucketing",
    "severity": "major",
    "id": 44,
    "message": "Maya, maybe let one of the engineers walk you through how the checkout flow actually works before you light up #cross-functional again \u2014 it\u2019s a pretty standard refactor and we don\u2019t need panic over every edge case.",
    "ground_truth_flags": [
      "Microaggressive"
    ],
    "predicted_flags": [
      "Non-Objective",
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: the model added an extra \"Non-Objective\" flag even though the expected flags were only \"Fake\" and \"Passive-Aggressive\".",
    "why": "The model treated the sarcastic, evaluative phrasing (\"guarding sprint scope,\" \"keep us all on process while...\") as an objective-claims violation (mind-reading/unsupported attribution). In this taxonomy, that content is better captured by \"Passive-Aggressive\" (and \"Fake\" for insincere praise) rather than escalating into a separate \"Non-Objective\" label. In other words, it double-counted the same rhetorical device\u2014sarcasm/backhanded framing\u2014as both tone (passive-aggressive) and epistemic/objectivity (non-objective), leading to over-flagging.",
    "failure_pattern": "overflagging-non-objective-on-sarcasm",
    "severity": "minor",
    "id": 46,
    "message": "Amazing call-out in #random, Theo \u2014 truly love how you keep us all on process while the client\u2019s checkout is literally breaking \ud83d\ude43. Anyway, when you have a minute between guarding sprint scope, can you please just confirm it\u2019ll be fixed before the 10am call tomorrow?",
    "ground_truth_flags": [
      "Fake",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Non-Objective",
      "Fake",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: the model added a \"Fake\" flag even though the message\u2019s issue is sarcasm/passive-aggression, not performative niceness or deceptive positivity.",
    "why": "The model appears to treat any sarcastic opener that uses a positive word (\"Awesome\") as \"Fake\" by default. In this context, \"Awesome, love that\u2026\" is clearly ironic rather than an attempt to feign warmth or manipulate; the speaker isn\u2019t trying to appear supportive, they\u2019re venting frustration. The core communicative harm is public, pressured accountability (\"just please make sure it\u2019s done before morning\" + \"again\") wrapped in sarcasm, which fits Passive-Aggressive. The \"Fake\" label would require a stronger signal of insincere praise meant to seem genuinely encouraging; here it\u2019s plainly a gripe, so the model conflated sarcasm with falseness/performative positivity.",
    "failure_pattern": "sarcasm-misclassified-as-fake",
    "severity": "minor",
    "id": 47,
    "message": "Awesome, love that this is coming in the night before the demo \ud83d\ude05. @marcus and @lena just please make sure it\u2019s done before morning so we don\u2019t have to explain \u201cunexpected behavior\u201d to the customer again\u2014thanks.",
    "ground_truth_flags": [
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Fake",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed the expected \"Condescending\" flag and instead flagged \"Non-Objective\" (likely incorrect/misapplied) while flagging \"Passive-Aggressive\" (arguable but not the primary issue compared to condescension).",
    "why": "The core problematic tone is hierarchical/derisive: phrases like \"moving fast is your thing\" and \"pretty basic checkout regression\" read as talking down to the dev\u2019s competence and process, especially in a public #sales channel where the dev is exposed in front of Sales. The model latched onto evidentiary framing (calling it a judgment = \"Non-Objective\") rather than the interpersonal impact (patronizing/condescending). It also treated the digs as primarily indirect (passive-aggressive) instead of recognizing them as fairly overt belittling. In this context, calling something \"basic\" + implying the dev needs to \"run through the flow\" before making promises is a competence jab; it\u2019s not merely \u201cnon-objective,\u201d it\u2019s condescending in a cross-functional audience where face-saving matters.",
    "failure_pattern": "missed-condescension-misclassified-as-non-objective",
    "severity": "major",
    "id": 48,
    "message": "Hey @marcus, I know moving fast is your thing, but this one\u2019s a pretty basic checkout regression \u2014 might be worth running through the flow before we ship promises to prospects. Can you please take a look and fix it ASAP so Sales doesn\u2019t have to explain why the demo breaks?",
    "ground_truth_flags": [
      "Condescending"
    ],
    "predicted_flags": [
      "Non-Objective",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: added a \"Fake\" flag that isn\u2019t warranted; \"Passive-Aggressive\" was correct.",
    "why": "The model conflated sarcasm/ironic phrasing (\"Awesome, love rediscovering\u2026 \ud83d\ude43\") with deception/impersonation. In most flagging taxonomies, \"Fake\" implies fabricated identity/claims, misinformation, or pretending sincerity in a way that\u2019s materially misleading. Here, the \"awesome/love\" opener is a common sarcastic complaint, not an attempt to mislead anyone into believing the speaker is genuinely pleased. The model appears to treat any insincere-sounding positivity as \"Fake\" rather than recognizing it as a rhetorical device already captured by \"Passive-Aggressive\" (and possibly \"Sarcasm\" if that existed). Channel context (fast, informal Slack) makes sarcasm even more typical, reducing the likelihood that it should be interpreted as a separate \"Fake\" violation.",
    "failure_pattern": "overflagging-sarcasm-as-fake",
    "severity": "minor",
    "id": 50,
    "message": "Awesome, love rediscovering the exact same checkout bug right before a demo \ud83d\ude43 I\u2019ll just keep rerunning the suite until it magically fixes itself.",
    "ground_truth_flags": [
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Fake",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Both: it missed the expected 'Condescending' flag and instead raised 'Non-Objective' (and 'Rudeness'). 'Rudeness' overlaps with condescension, but 'Non-Objective' is largely a misfire for this message.",
    "why": "The model latched onto a generic heuristic that quoting someone\u2019s stance (\"do it right\") is 'mind-reading' and therefore non-objective, even though Jordan explicitly stated that goal in-thread (so it\u2019s not an invented motive). It also collapsed the primary tone violation into 'Rudeness' without selecting the more precise label ('Condescending')\u2014the key problematic phrase is the belittling comparison ('production fire drill, not a coding workshop'), which is classic condescension in a public, high-stakes channel. In short: it over-applied an 'attributing motives' rule, and under-discriminated between closely related tone categories (rude vs condescending) given the public callout context.",
    "failure_pattern": "mislabeling-condescension-as-non-objective",
    "severity": "major",
    "id": 52,
    "message": "Jordan, I get that you want to \"do it right,\" but this is a production fire drill, not a coding workshop. Please stick to the feature-flag hotfix so we can give the client an ETA today.",
    "ground_truth_flags": [
      "Condescending"
    ],
    "predicted_flags": [
      "Non-Objective",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Missed the expected \"Microaggressive\" flag (false negative). The model otherwise correctly flagged Non-Objective and Rudeness.",
    "why": "The model appears to treat microaggression too narrowly (e.g., only protected-class/identity-based slights) and therefore didn\u2019t recognize the delegitimizing, dismissive framing in this context: quoting \u201caccessibility tweak\u201d (scare quotes) minimizes a compliance/ADA/WCAG concern; \u201cprecious about pixels\u201d stereotypes design/accessibility advocacy as nitpicky and irrational; and \u201cyet another\u201d implies a pattern of unreasonable behavior. In a tense, high-stakes thread (past outage, on-call load, looming investor demo), this functions as a subtle status move that invalidates the other party\u2019s expertise and concerns\u2014commonly captured by microaggression taxonomies as belittling/demeaning language even without identity references. The model focused on overt rudeness/subjective attribution and didn\u2019t map this kind of minimization/derision to the microaggression category.",
    "failure_pattern": "microaggression-scope-too-narrow",
    "severity": "major",
    "id": 53,
    "message": "Can we not derail this with yet another \u201caccessibility tweak\u201d at the last minute? Feels like you\u2019re being kinda precious about pixels again \u2014 we need to ship for Friday.",
    "ground_truth_flags": [
      "Microaggressive",
      "Non-Objective",
      "Rudeness"
    ],
    "predicted_flags": [
      "Non-Objective",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: the model added a \"Passive-Aggressive\" flag that wasn\u2019t expected; the correct/expected classification was only \"Fake.\"",
    "why": "The model over-interpreted urgency + casual Slack phrasing as veiled hostility. In this startup/incident context, blunt direction and cheerleading language (\"Amazing work\", \"love the energy \ud83d\ude4c\") can be a common rallying style. While the message is plausibly performative/overconfident (supporting \"Fake\"), there isn\u2019t a clear passive-aggressive marker like an explicit jab, complaint, or contrastive subtext directed at a person. The model treated \"Just go ahead\" and \"I\u2019m sure it\u2019ll be totally fine\" as sarcasm by default, but the text could also be read as naive reassurance/pressure rather than indirect resentment. It conflated \u2018risky instruction\u2019 and \u2018forced positivity\u2019 with passive aggression, instead of reserving that flag for messages with clearer indirect blame or snide insinuation.",
    "failure_pattern": "overflagging-passive-aggression-from-urgency-and-cheerleading",
    "severity": "minor",
    "id": 54,
    "message": "Amazing work, team \u2014 love the energy here \ud83d\ude4c Just go ahead and push the hot patch as-is; I\u2019m sure it\u2019ll be totally fine in prod.",
    "ground_truth_flags": [
      "Fake"
    ],
    "predicted_flags": [
      "Fake",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed the expected \"Condescending\" flag and instead flagged \"Non-Objective\" (likely incorrect in this setting); it also labeled the tone as \"Passive-Aggressive\" when the primary issue is overt condescension rather than indirectness.",
    "why": "The message contains explicit belittling language aimed at the recipient (\"pretty basic stuff,\" \"explaining fundamentals\") and a status assertion that frames Leo as incompetent in front of the channel\u2014this is a classic condescension cue. The model over-weighted the presence of absolute/normative wording (\"always need\") as an objectivity violation, but that phrase is common shorthand for established engineering best practices and not the core interpersonal risk. It also interpreted the jab as passive-aggressive, but the criticism is direct and public, not veiled; the harm comes from talking down to a junior engineer under deadline pressure in a public forum, which the model under-accounted for.",
    "failure_pattern": "mislabeling-condescension-as-non-objective",
    "severity": "major",
    "id": 57,
    "message": "Leo, this is pretty basic stuff \u2014 webhook handlers *always* need idempotency, clear logging, and tests for retries/timeouts. Please take another pass and add the missing coverage before we spend more time explaining fundamentals in here.",
    "ground_truth_flags": [
      "Condescending"
    ],
    "predicted_flags": [
      "Non-Objective",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: it added a \"Non-Objective\" flag that wasn\u2019t expected; the only expected issue was tone/rudeness.",
    "why": "The model treated a rhetorical, stress-emphasis statement (\"We\u2019re not making tomorrow\u2019s demo...\") as an evidence-based factual claim requiring substantiation. In this Slack context, the sentence functions as a warning/forecast and a pressure-release escalation, not as an objective assertion meant to be verified. The classifier over-indexed on absolutist phrasing (\u201cwe\u2019re not\u2026\u201d) and causal language, misapplying the \u201cNon-Objective\u201d policy to normal workplace risk framing and direct accountability language. The channel context (high-stress deadline + prior scope/acceptance-criteria discussion) supports interpreting this as subjective concern rather than a misleading factual statement.",
    "failure_pattern": "overflagging-risk-forecast-as-nonobjective",
    "severity": "minor",
    "id": 58,
    "message": "We\u2019re not making tomorrow\u2019s demo if we keep getting last-minute \u201cone more thing\u201d requests and surprise pings after hours. Lock the scope and stop calling engineering the blocker in public.",
    "ground_truth_flags": [
      "Rudeness"
    ],
    "predicted_flags": [
      "Non-Objective",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: the model added a \"Non-Objective\" flag that isn\u2019t warranted; only \"Passive-Aggressive\" should have been returned.",
    "why": "The model treated sarcasm/hyperbole (\"nothing ever goes wrong on launch morning\") as an 'absolute, unsupported claim' and mapped that to Non-Objective. But in this context the sentence is clearly rhetorical irony used to criticize shipping without safeguards, not an attempt to assert a factual proposition or provide evidence-based reasoning. The system effectively double-counted the same cue (sarcastic phrasing) as two separate violations instead of recognizing it as one tonal issue (passive-aggressive sarcasm).",
    "failure_pattern": "overflagging-sarcasm-as-non-objective",
    "severity": "minor",
    "id": 59,
    "message": "Sure, let\u2019s just ship it with zero tests and no rollback plan because nothing ever goes wrong on launch morning. I\u2019m sure the client will love the \u201cwe\u2019ll hotfix it live\u201d strategy.",
    "ground_truth_flags": [
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Non-Objective",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Both: it missed the expected Passive-Aggressive and Microaggressive flags, and instead applied Non-Objective and Rudeness. The message\u2019s core issue is indirect hostility/sarcasm and status/process shaming, not primarily factual non-objectivity.",
    "why": "The model anchored on surface cues (accusatory phrasing) and mapped them to generic categories (Rudeness, Non-Objective) while failing to detect the more specific interpersonal pattern: passive-aggressive sarcasm (\u201csomeone \u2018has a feeling\u2019\u201d, \u201crubber-stamp it\u201d) and public process-policing (\u201cMaybe run this through the actual process next time\u2026\u201d) that signals contempt indirectly rather than via explicit insults. It also likely treated \u201cmicroaggressive\u201d too narrowly (as identity-based) and therefore didn\u2019t flag it, even though in this taxonomy it can include dismissive, belittling language that undermines a colleague\u2019s legitimacy/competence (implying the designer is irrational and bypassing \u2018actual\u2019 process). Finally, it underweighted channel/context: posting this in #random in a high-scrutiny org makes the \u201cmaybe next time\u2026\u201d performative shaming more salient and increases the passive-aggressive nature (public admonishment vs. direct constructive guidance).",
    "failure_pattern": "missing-subtle-passive-aggression-plus-category-substitution",
    "severity": "major",
    "id": 60,
    "message": "We really can\u2019t keep redesigning core flows at the last minute because someone \u201chas a feeling\u201d \u2014 accessibility and data integrity aren\u2019t optional. Maybe run this through the actual process next time instead of dropping it in #random and expecting engineering to rubber-stamp it.",
    "ground_truth_flags": [
      "Microaggressive",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Non-Objective",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Flagged an extra category: the model predicted \"Passive-Aggressive\" in addition to the expected \"Fake.\"",
    "why": "The model over-interpreted the tonal contrast (effusive praise followed by urgency) as a backhanded jab. While the opener is clearly insincere/sarcastic (supporting the \"Fake\" flag), there isn\u2019t clear evidence of indirect hostility, veiled blame, or pointed resentment toward a specific person/team that would justify \"Passive-Aggressive\". The message is blunt and commanding, but that\u2019s direct coercive urgency rather than passive aggression. The model appears to have used a heuristic\u2014\"sarcastic praise + abrupt demand => passive-aggressive\"\u2014without sufficiently distinguishing sarcasm/fakeness from passive-aggressive intent (which typically includes plausible deniability, guilt-tripping, or indirect accusation).",
    "failure_pattern": "overflagging-sarcasm-as-passive-aggression",
    "severity": "minor",
    "id": 61,
    "message": "Amazing job keeping tomorrow\u2019s launch on track, truly inspiring \ud83d\ude4c \u2014 anyway I need Product + Data + CS to drop what you\u2019re doing and jump on this incident right now, since we can\u2019t afford to miss the renewal call. Just grab whoever you\u2019ve got and start digging; I\u2019ll expect updates in here shortly.",
    "ground_truth_flags": [
      "Fake"
    ],
    "predicted_flags": [
      "Fake",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed an expected flag: the model correctly flagged \"Passive-Aggressive\" but failed to flag \"Fake\" (insincere/patronizing praise) even though the message\u2019s opening compliment is clearly sarcastic and not genuine.",
    "why": "The model appears to have treated the sarcastic praise as only a passive-aggressive cue and did not map it to the separate \"Fake\" category (inauthentic positivity). In many taxonomies, sarcasm-driven compliments simultaneously signal (1) indirect hostility (passive-aggressive) and (2) performative/insincere approval (fake). The message contains explicit markers of insincere praise (\"Awesome work\", \"really love the confidence\") plus exaggeration/magical framing (\"magically\") and an emoji to soften the jab\u2014these are strong \"Fake\" indicators. The model likely under-weighted the dual-label nature of sarcastic compliments and collapsed it into a single label.",
    "failure_pattern": "missed-secondary-fake-flag-in-sarcasm",
    "severity": "major",
    "id": 62,
    "message": "Awesome work, Ethan \u2014 really love the confidence in shipping this right before a Fortune 100 demo. If you can magically make quote generation work again in the next hour, that\u2019d be amazing \ud83d\ude4f",
    "ground_truth_flags": [
      "Fake",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as \"Rudeness\" instead of the expected \"Condescending\" (wrong label/classification, not a missed-flag omission).",
    "why": "The model correctly detected a negative interpersonal tone but collapsed two nearby categories. The phrasing \"treat deadlines like they\u2019re real, not suggestions\" is less about overt hostility/insults (typical \"rudeness\") and more about talking down to Jordan\u2014implying they lack basic professionalism or need a remedial lesson (a hallmark of condescension). The public #general setting and high-stakes scramble amplify the social impact, but the core violation is the patronizing framing rather than explicit abusive language; the model likely relied on keyword cues like \"demeaning\" and \"insulting\" and mapped them to the broader/adjacent \"rudeness\" bucket instead of the more specific \"condescending\" one.",
    "failure_pattern": "label-confusion-rudeness-vs-condescension",
    "severity": "minor",
    "id": 65,
    "message": "Jordan, I need you to treat deadlines like they\u2019re real, not suggestions. Please get the copy updates in by 11 so design can do their job without waiting on you again.",
    "ground_truth_flags": [
      "Condescending"
    ],
    "predicted_flags": [
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as Passive-Aggressive instead of the expected Dismissive (wrong label; effectively missed the intended flag).",
    "why": "The model latched onto surface cues like \u201cCan we not\u2026\u201d and the comparative jab \u201clike everyone else\u201d and mapped them to passive-aggression, but the message is primarily a direct shutdown of Maya\u2019s concern and an attempt to silence the topic in-channel. The harm is dismissal/minimization (invalidating the concern about being excluded and deferring accountability to a future post), not indirectness. The supposed passive-aggressive element is weak; it reads as openly curt and gatekeeping rather than veiled or indirect. Context (Maya\u2019s urgency around renewals and exclusion from the decision-making call) increases the dismissive impact, which the model noted generally as \u2018curt\u2019 but mislabeled as passive-aggressive rather than recognizing it as straightforward dismissal.",
    "failure_pattern": "label-confusion-dismissive-vs-passive-aggressive",
    "severity": "major",
    "id": 66,
    "message": "Can we not turn #random into a whole thread about being \u201cleft out\u201d? If you need the details, just wait for the #product post like everyone else.",
    "ground_truth_flags": [
      "Dismissive"
    ],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: the model added a \"Non-Objective\" flag when the expected (and sufficient) flag was only \"Circular.\"",
    "why": "The model treated a forceful value judgment (\"it won\u2019t be finished\", \"choosing to ship unfinished work\") as an objective-claims violation, but in this context it\u2019s plainly a subjective product-quality stance and rhetorical emphasis, not a factual misrepresentation. The message\u2019s core issue is repetitive/circular reasoning (it restates the same premise-conclusion loop multiple times). By over-indexing on absolutist phrasing (\"can\u2019t\", \"basically\"), the model conflated strong disagreement with non-objectivity rather than recognizing that the policy intent is to catch misleading assertions presented as facts, not normative craft opinions stated passionately.",
    "failure_pattern": "overflagging-subjective-opinion-as-non-objective",
    "severity": "minor",
    "id": 68,
    "message": "We can\u2019t cut the interactions because they\u2019re what make it feel finished, and if we cut what makes it feel finished then it won\u2019t be finished. If we ship something that doesn\u2019t feel finished, we\u2019re basically choosing to ship unfinished work.",
    "ground_truth_flags": [
      "Circular"
    ],
    "predicted_flags": [
      "Non-Objective",
      "Circular"
    ]
  },
  {
    "what_went_wrong": "Both: it missed the expected \"Passive-Aggressive\" flag and instead added an incorrect \"Non-Objective\" flag.",
    "why": "The model correctly detected rudeness but misinterpreted the second-person, snarky phrasing as an epistemic/evidence issue (\"this isn\u2019t that complicated\") rather than recognizing it as interpersonal passive-aggression. \"Non-Objective\" is a poor fit here because the main problem isn\u2019t an unverifiable factual claim\u2014it\u2019s the dismissive, needling tone and indirect blame (\"if we could spend less time debating\u2026that\u2019d be great\") that functions as a public reprimand. The model likely relied on a heuristic that maps absolute/invalidating language (\"isn\u2019t that complicated\") to objectivity, while underweighting the pragmatic intent and social meaning (condescension + sarcasm) that defines passive-aggression in this channel context.",
    "failure_pattern": "passive-aggression-mislabeled-as-non-objective",
    "severity": "major",
    "id": 69,
    "message": "Lina, this isn\u2019t that complicated \u2014 just follow the logs and copy the pattern from the last hotfix. Arjun, if we could spend less time debating process and more time actually fixing prod, that\u2019d be great.",
    "ground_truth_flags": [
      "Rudeness",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Non-Objective",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Missed the expected Microaggressive flag and instead added a Non-Objective flag that isn\u2019t central to the harm in this message (the core issue is derogatory, belittling language toward an on-call engineer).",
    "why": "The model correctly identified overt rudeness but treated the second half as mainly an evidentiary/objectivity problem (\u201cassumes motive/inaction\u201d) rather than recognizing the interpersonal power dynamic and infantilizing phrasing (\u201chand-holding,\u201d \u201cthe adults can jump in\u201d) as a microaggression/condescension signal. In this startup/CTO-to-team context, the language functions as status-based belittlement and gatekeeping (junior vs. \u201cadults\u201d), which the model appears to map to 'Non-Objective' because it contains generalizations, missing that microaggression categories often hinge on tone, hierarchy, and demeaning implications\u2014not factual provability. It also underweighted channel/role context: a late-night DM from a CTO during an incident increases the impact of patronizing language, making the microaggressive component clearer.",
    "failure_pattern": "microaggression-missed-misrouted-to-non-objective",
    "severity": "major",
    "id": 70,
    "message": "Alright, we need everyone to stay on this tonight\u2014no more cowboy deploys. Also, next time you\u2019re on-call, please don\u2019t just sit on an alert because you\u2019re \u201cnot sure\u201d what it means; if you need hand-holding, say so sooner so the adults can jump in.",
    "ground_truth_flags": [
      "Microaggressive",
      "Rudeness"
    ],
    "predicted_flags": [
      "Non-Objective",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Both: it missed the expected Non-Objective flag and incorrectly added Vagueness (and arguably mis-specified Passive-Aggressive relative to the expected label set). Rudeness was correctly flagged.",
    "why": "The model over-weighted surface-level lack of implementation details (\u201cdo the obvious fix\u201d) and treated it as Vagueness, despite ample concrete context already provided in-thread (service, deploy, dashboards, suspected locus, mitigation status, affected customers). The core policy issue in the target message is less about missing operational specifics and more about inserting subjective/loaded, non-fact-based judgments and personal blame (\u201cI can\u2019t keep translating\u2026\u201d, \u201cobvious fix\u201d)\u2014i.e., Non-Objective. In addition, it mapped the interpersonal jab to Passive-Aggressive rather than recognizing it as primarily a non-objective, blamey evaluation; the jab here is direct/contemptuous more than \u2018veiled\u2019 or indirect, so Passive-Aggressive is a weaker fit than Non-Objective + Rudeness.",
    "failure_pattern": "non-objective-missed-and-vagueness-overflagged",
    "severity": "major",
    "id": 71,
    "message": "I need you to just jump on this and get it stable ASAP \u2014 I can\u2019t keep translating what\u2019s happening for you, so please follow the thread and do the obvious fix. Let me know when it\u2019s done.",
    "ground_truth_flags": [
      "Non-Objective",
      "Rudeness"
    ],
    "predicted_flags": [
      "Vagueness",
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: added a \"Non-Objective\" flag when only \"Circular\" was expected.",
    "why": "The model treated a rhetorical, circularly-reasoned argument as a separate objectivity violation. While the message asserts reliability without evidence, the primary (and already-captured) issue is the explicitly self-referential loop (\u201cwe need Kafka because\u2026 reliability is why we need Kafka\u201d). In this context, the evaluation taxonomy likely intends \"Non-Objective\" for claims that introduce disputable facts or judgments in a way that derails decision-making (e.g., unverifiable accusations, subjective characterizations), whereas here the non-evidenced claim is inseparable from\u2014and essentially the same as\u2014the circular reasoning. The model also over-weighted the presence of an unqualified claim (\u201cKafka is the reliable choice\u201d) as automatically non-objective, instead of recognizing it as part of the same single rhetorical defect (tautology) rather than a distinct communication risk category.",
    "failure_pattern": "overflagging-overlapping-categories",
    "severity": "minor",
    "id": 73,
    "message": "We keep going in circles here: we need Kafka because we need something reliable, and reliability is exactly why we need Kafka. If we want this to be reliable beyond next Friday, Kafka is the reliable choice, so the answer is still Kafka.",
    "ground_truth_flags": [
      "Circular"
    ],
    "predicted_flags": [
      "Non-Objective",
      "Circular"
    ]
  },
  {
    "what_went_wrong": "Both: it missed the expected \"Rudeness\" flag and instead added an incorrect \"Non-Objective\" flag.",
    "why": "The message contains overtly dismissive and domineering language (\"We don\u2019t have time for another debate\") that escalates interpersonal friction in a high-stakes, formal client-facing channel\u2014this is a clear tone/rudeness issue given the context (Sales/CS watching, AM-approved wording required, and prior pushback history). The model latched onto a semantic/normative cue (\"obvious\") and mapped it to \"Non-Objective,\" treating a common urgency phrasing as an objectivity violation rather than prioritizing the more salient behavior: shutting down discussion and deprioritizing approved incident-report wording (\"I\u2019ll worry about the wording... later\"), which is reputationally risky and reads as brusque. In short, it over-weighted lexical subjectivity and under-used channel/social context that makes the rudeness unambiguous.",
    "failure_pattern": "missing-rudeness-overflagging-non-objective",
    "severity": "major",
    "id": 74,
    "message": "We don\u2019t have time for another debate\u2014just do the obvious rollback and get us back within SLA. I\u2019ll worry about the wording for the incident report later.",
    "ground_truth_flags": [
      "Vagueness",
      "Rudeness"
    ],
    "predicted_flags": [
      "Vagueness",
      "Non-Objective"
    ]
  },
  {
    "what_went_wrong": "Missed the expected Dismissive flag and instead added Passive-Aggressive; it correctly flagged Non-Objective.",
    "why": "The model latched onto the minimizing/shutdown language ('basically an edge case', 'we\u2019re not going to derail', 'Let\u2019s just ship') but mapped it to Passive-Aggressive rather than the more direct, overtly minimizing behavior that fits Dismissive. In this context, the message isn\u2019t indirectly hostile or sarcastic (typical passive-aggression); it\u2019s a straightforward deprioritization that discounts QA\u2019s concern and implicitly pressures the team to proceed despite a main-demo-path bug. The channel context (#client-updates, client-facing stakes, high need for careful wording) amplifies how dismissive the phrasing is, but the model\u2019s taxonomy choice suggests it under-distinguished \u201cdismissal/minimization\u201d from \u201cpassive-aggressive tone,\u201d overgeneralizing any shutdown as passive-aggression.",
    "failure_pattern": "dismissive-vs-passive-aggressive-confusion",
    "severity": "major",
    "id": 76,
    "message": "Ok, but this is basically an edge case and we\u2019re not going to derail the 9am demo over it. Let\u2019s just ship and circle back if the client actually hits it.",
    "ground_truth_flags": [
      "Dismissive",
      "Non-Objective"
    ],
    "predicted_flags": [
      "Non-Objective",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: added a Non-Objective flag when only Circular was expected.",
    "why": "The model treated candid performance feedback (\"we can\u2019t trust the handoff\" / \"we end up in late-night hotfix mode\") as an unsupported factual claim requiring evidence, rather than as a reasonable, experience-based management judgment in a high-stakes incident thread. In this culture/channel, direct causal framing is a normal way to communicate operational risk and urgency, not necessarily a non-objective or speculative allegation. The only clear issue in the message is rhetorical repetition of the same point (circularity).",
    "failure_pattern": "overflagging-direct-communication",
    "severity": "minor",
    "id": 78,
    "message": "We need you to stop going quiet after pushing a partial fix because when you go quiet we can\u2019t trust the handoff, and when we can\u2019t trust the handoff we end up in late-night hotfix mode again \u2014 which is exactly what we need to stop. So the main thing is: don\u2019t go quiet and don\u2019t leave partial fixes, because the problem is the going-quiet/partial-fix pattern.",
    "ground_truth_flags": [
      "Circular"
    ],
    "predicted_flags": [
      "Non-Objective",
      "Circular"
    ]
  },
  {
    "what_went_wrong": "Missed an expected flag: it flagged only \"Rudeness\" but failed to also flag \"Dismissive,\" even though the message explicitly shuts down discussion and invalidates ongoing input.",
    "why": "The model correctly recognized overt bluntness as rudeness, but it collapsed two distinct dimensions into one. \"Dismissive\" here is not subtle: the speaker explicitly forecloses further UX discussion (\"We\u2019re not reopening UX debates\") and delegitimizes a colleague\u2019s contribution (\"stop spinning this into a design postmortem\"). Given the #general context (executive-visible, accountability-sensitive) and the active attempt to align on a single source of truth, the message functions as a public shutdown rather than a process-oriented redirection. The model\u2019s reasoning focused on tone/aggression and didn\u2019t separately evaluate the conversational act of dismissing/stonewalling, especially in a high-stakes, cross-functional thread where someone is proposing a UX mitigation and another is asking to align on tickets.",
    "failure_pattern": "missed-dismissiveness-due-to-tone-collapse",
    "severity": "major",
    "id": 79,
    "message": "We\u2019re not reopening UX debates in #general. Ship what\u2019s in the ticket and stop spinning this into a design postmortem right now.",
    "ground_truth_flags": [
      "Dismissive",
      "Rudeness"
    ],
    "predicted_flags": [
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: it added \"Passive-Aggressive\" and \"Rudeness\" when the expected (and best-fitting) label is just \"Dismissive.\"",
    "why": "The message is an overt shutdown (\u201cJust ship\u2026\u201d, \u201cwe\u2019ll deal with the rest later\u201d) that minimizes the design concern and redirects discussion away from it\u2014classic dismissiveness. The model appears to have conflated blunt/direct shutdown with higher-severity categories (rudeness) and treated the complaint about \u201canother design debate\u201d as indirect/veiled negativity (passive-aggression). But the negativity isn\u2019t covert; it\u2019s explicit and directive, and while curt, it doesn\u2019t contain insults, profanity, or personal attacks that would typically justify a separate \"Rudeness\" flag. It also isn\u2019t meaningfully passive-aggressive because it doesn\u2019t mask hostility behind politeness; it directly pushes the conversation to stop and ship.",
    "failure_pattern": "overflagging-blunt-dismissiveness",
    "severity": "major",
    "id": 80,
    "message": "Can we not turn #general into another design debate? Just ship whatever you already built and we\u2019ll deal with the rest later.",
    "ground_truth_flags": [
      "Dismissive"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: it added the \"Fake\" flag. The message is clearly passive-aggressive/sarcastic, but not \"fake\" in the sense of superficially positive while intending genuine support or politeness; it\u2019s openly snide rather than deceptively cordial.",
    "why": "The model conflated sarcasm/backhanded thanks with the separate category of \"Fake.\" In this context, the \"thanks\" is transparently weaponized (\"very public heads-up\" + \"Awesome, another P1\"), so the core issue is passive aggression, not insincere niceness meant to appear collaborative. The model likely over-weighted the presence of positive lexical markers (\"Awesome,\" \"thanks\") without distinguishing between (a) performative politeness that masks criticism (fake) and (b) overt sarcasm used to criticize (passive-aggressive). Channel norms (crisp, formal status updates) also make the snark stand out as a tone violation, but that supports Passive-Aggressive rather than adding Fake.",
    "failure_pattern": "sarcasm-mislabeled-as-fake",
    "severity": "minor",
    "id": 81,
    "message": "Awesome, another P1 in #engineering right before the window \u2014 thanks for the very public heads-up. I\u2019ll take a look when I can, but if we could maybe route these through the change thread first next time, that\u2019d be great.",
    "ground_truth_flags": [
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Fake",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Both: it flagged the message as \"Rudeness\" (incorrect per expected taxonomy) and missed the intended flags \"Condescending\" and \"Non-Objective.\"",
    "why": "The model collapsed multiple distinct issues into a generic disrespect bucket. The message\u2019s core problems are (a) condescension\u2014dismissing Marcus\u2019s concerns as a \"platform purity lecture\" and implying he\u2019s overcomplicating things\u2014and (b) non-objectivity\u2014appealing to personal seniority/tenure (\"I\u2019ve shipped fixes like this here for years\") and urgency (\"we don\u2019t have time\") rather than engaging with the concrete technical questions about hit rate/TTL/cardinality/review. It likely ignored the organizational context that NimbusPay is explicitly trying to avoid \"unreviewed changes under deadline pressure,\" making the non-objective, authority-based push more salient. In short, it recognized negativity but mis-mapped it to the wrong label and missed the reasoning-quality violation.",
    "failure_pattern": "taxonomy-mapping-error-condescension-to-rudeness",
    "severity": "major",
    "id": 82,
    "message": "Marcus, you\u2019re making this way harder than it needs to be \u2014 we don\u2019t have time for a platform purity lecture. I\u2019ve shipped fixes like this here for years; let\u2019s just add the Redis cache and move on.",
    "ground_truth_flags": [
      "Condescending",
      "Non-Objective"
    ],
    "predicted_flags": [
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Missed the expected \"Dismissive\" flag and instead produced adjacent but non-target flags (\"Vagueness\" and \"Rudeness\").",
    "why": "The message\u2019s primary issue is minimizing/shutting down legitimate concerns (\u201cnot turn this into a whole thing,\u201d \u201cmove on\u201d)\u2014a classic dismissive move in response to requests for an RCA and alignment. The model correctly noticed brusqueness, but mapped it to the broader \"Rudeness\" bucket rather than the more precise \"Dismissive\" category the taxonomy expects. It also over-weighted literal instruction clarity (\u201chandle it\u201d) and tagged \"Vagueness,\" even though in this channel context the core harm is not ambiguity of task ownership but discouraging necessary discussion and process (post-incident explanation) right before a demo.",
    "failure_pattern": "label-confusion-dismissive-vs-rude",
    "severity": "major",
    "id": 83,
    "message": "Can we not turn this into a whole thing? The demo\u2019s tomorrow\u2014just handle it and move on.",
    "ground_truth_flags": [
      "Dismissive"
    ],
    "predicted_flags": [
      "Vagueness",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Both: it missed the expected Microaggressive flag and instead added a Non-Objective flag that wasn\u2019t expected for this taxonomy/use-case.",
    "why": "The message contains a status-based belittling framing (\u201cI get you\u2019re new\u201d) and a generalized blame narrative (\u201cexactly how we end up back in incident mode\u201d) used to shut down discussion (\u201cinstead of improvising and arguing every comment\u201d). That combination is a classic microaggression pattern in workplace comms: invoking seniority/newness to delegitimize input and publicly diminish a junior. The model correctly caught overt rudeness but treated the \u2018incident mode\u2019 line primarily as an unverifiable causal claim (hence Non-Objective), over-indexing on evidentiary rigor rather than the interpersonal function of the statement (scolding, stereotyping the junior as \u2018arguing\u2019). It also underweighted the org/channel context: #engineering is audited, leadership-visible, and expects calm, action-oriented feedback\u2014making the power-dynamic jab more salient and microaggressive in this setting.",
    "failure_pattern": "missed-status-based-microaggression-overflagged-non-objective",
    "severity": "major",
    "id": 85,
    "message": "Look, I get you\u2019re new, but this kind of shortcut is exactly how we end up back in incident mode \u2014 please just stick to the standards instead of improvising and arguing every comment.",
    "ground_truth_flags": [
      "Microaggressive",
      "Rudeness"
    ],
    "predicted_flags": [
      "Non-Objective",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Missed the expected \u201cCondescending\u201d flag and instead labeled the message as \u201cRudeness\u201d and \u201cPassive-Aggressive,\u201d resulting in a taxonomy mismatch (wrong labels for the same underlying tone issue).",
    "why": "The target message\u2019s core violation is belittling/patronizing framing (\u201cfor future reference,\u201d scare quotes around \u201cescalating,\u201d and the put-down \u201cit\u2019s just making noise\u201d), which maps most directly to condescension. The model latched onto surface harshness and reprimand and mapped it to generic \u201crudeness,\u201d and treated the sarcastic-seeming quotation as \u201cpassive-aggressive.\u201d But the message is not especially indirect or \u2018veiled\u2019\u2014it is openly corrective and status-asserting\u2014so \u201cpassive-aggressive\u201d is a weaker fit than \u201ccondescending.\u201d The model also likely underweighted the channel norms context: while asking for a ticket is legitimate, the phrasing adds a demeaning evaluation of Maya\u2019s behavior, which is precisely what the \u201ccondescending\u201d category is meant to capture in this workplace setting.",
    "failure_pattern": "condescension-mislabeled-as-rudeness",
    "severity": "major",
    "id": 87,
    "message": "Maya, for future reference, blasting #engineering without a ticket, repro steps, or even an invoice ID isn\u2019t actually \u201cescalating,\u201d it\u2019s just making noise. Please file it properly and include the links/logs, and then we can talk about an ETA.",
    "ground_truth_flags": [
      "Condescending"
    ],
    "predicted_flags": [
      "Rudeness",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: added a \"Non-Objective\" flag when only \"Circular\" was expected.",
    "why": "The model treated a rhetorical/argumentative framing (complaining that QA is defining \u2018blocker\u2019 status) as an evidence-based factual claim about motives. In this Slack context, the line \"it\u2019s a blocker only because QA is blocking it\" is less a verifiable assertion and more a circular reframing meant to dispute the label \u2018blocker.\u2019 While it is contentious and arguably unfair, it doesn\u2019t primarily violate objectivity in the way the taxonomy likely intends (e.g., presenting specific unverified facts). The core issue is the self-referential reasoning loop, which is fully captured by the Circular flag; adding Non-Objective is overinclusive.",
    "failure_pattern": "overflagging-non-objective-for-rhetorical-argument",
    "severity": "minor",
    "id": 89,
    "message": "I hear you, but it\u2019s still the same thing: we can\u2019t block launch because QA says it\u2019s a blocker, and it\u2019s a blocker only because QA is blocking it. So we shouldn\u2019t treat it as a blocker since it\u2019s just blocking because we\u2019re treating it like one.",
    "ground_truth_flags": [
      "Circular"
    ],
    "predicted_flags": [
      "Non-Objective",
      "Circular"
    ]
  },
  {
    "what_went_wrong": "Both: it missed the expected flags (Dismissive, Non-Objective) and instead flagged Passive-Aggressive, which is not the primary issue in this message.",
    "why": "The message\u2019s core problem is dismissing a reported regression and replacing evidence-based triage with an assertion of personal correctness (\u201cit worked when I merged it\u201d) plus deflection (\u201cjust rerun the suite or file it\u201d). That maps cleanly to Dismissive (shutting down investigation in the moment, minimizing urgency) and Non-Objective (arguing from personal experience rather than engaging the concrete repro/logs). The model latched onto the interpersonal friction implied by \u2018it worked when I merged it\u2019 and treated that as passive-aggression, but the tone is more overtly dismissive/defensive than indirectly snide. It also underweighted the surrounding context: the team is under demo pressure and Maya provided specific technical evidence; ignoring that makes the \u2018non-objective\u2019 aspect (not responding to repro details) more salient than \u201cpassive-aggressive.\u201d",
    "failure_pattern": "mislabeling-dismissiveness-as-passive-aggression",
    "severity": "major",
    "id": 90,
    "message": "Yeah, we\u2019re not going to chase this right now \u2014 it worked when I merged it. If it\u2019s still an issue, just rerun the suite or file it and we\u2019ll look after the demo.",
    "ground_truth_flags": [
      "Dismissive",
      "Non-Objective"
    ],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: added an extra \u201cVagueness\u201d flag that wasn\u2019t expected for this message; \u201cCircular\u201d was correct.",
    "why": "The model treated a common, contextually clear phrase (\u201ccustomer-safe narrative\u201d) as insufficiently specific and penalized urgency language (\u201cnow/ASAP\u201d) as lack of actionable detail. In this #sales war-room context, the scope, audience, owner set, and deadline are already established by the preceding messages (named customers, status page sentence, draft coming from Priya, call in 35 minutes/within 10). The target message\u2019s primary issue is rhetorical repetition (circularity), not ambiguity about what\u2019s needed. The model over-applied a generic rubric for \u201cvagueness\u201d without anchoring to the already-specified constraints and shared understanding in-thread.",
    "failure_pattern": "context-insensitive-overflagging",
    "severity": "minor",
    "id": 91,
    "message": "We need a customer-safe narrative now because we need a customer-safe narrative now. Until we have something we can say on calls, we don\u2019t have something we can say on calls, so we need the narrative ASAP.",
    "ground_truth_flags": [
      "Circular"
    ],
    "predicted_flags": [
      "Vagueness",
      "Circular"
    ]
  },
  {
    "what_went_wrong": "Flagged \"Passive-Aggressive\" incorrectly; only \"Circular\" was expected.",
    "why": "The model over-interpreted firm, explicit boundary-setting (\u201cplease stop trying to bypass it\u201d) as passive-aggression. The message contains direct admonishment and repetition, but it\u2019s not indirect, veiled, or sarcastic\u2014key features of passive-aggression. In this enterprise/process-heavy context (customer outage, compliance risk, managers watching #random), a senior engineer reiterating protocol and explicitly asking to stop off-channel escalation is a legitimate, direct coordination move rather than a covert dig. The model likely relied on lexical cues (\"please stop\", \"bypass\") and perceived blame to infer passive-aggression, without applying the definition (indirect hostility) or weighing situational appropriateness.",
    "failure_pattern": "overflagging-direct-communication",
    "severity": "minor",
    "id": 93,
    "message": "I\u2019m saying the same thing I said: we need to follow the incident process, because when we don\u2019t follow the incident process it creates more problems than it solves. So please stop trying to bypass it, because bypassing it is exactly why we have to stick to the process.",
    "ground_truth_flags": [
      "Circular"
    ],
    "predicted_flags": [
      "Circular",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Both: it missed the expected \"Sarcastic\" flag and incorrectly added \"Fake\" (substituting a non-target label for sarcasm).",
    "why": "The message\u2019s core signal is overt sarcasm/irony (\"Awesome, love that\u2026\", \"super chill\", scare quotes around \"stable\", and the \ud83d\ude43 marker) plus a passive-aggressive jab. The model correctly caught passive-aggression but mapped the sarcastic praise to a broader \"Fake/insincere\" category, likely treating exaggerated positivity as deception rather than as a rhetorical device. In this DM startup context, bluntness is normal, but the sarcasm cueing is explicit; the mistake is taxonomy/label confusion (sarcasm misrouted to \"Fake\"), not a subtlety or context issue.",
    "failure_pattern": "sarcasm-mislabeled-as-fake",
    "severity": "major",
    "id": 94,
    "message": "Awesome, love that the new checkout occasionally double-charges on mobile \u2014 super chill for tomorrow\u2019s \"stable\" demo \ud83d\ude43. I\u2019ll just keep finding these little surprises unless you want to take a look tonight.",
    "ground_truth_flags": [
      "Sarcastic",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Fake",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Both: it missed the expected flags (Dismissive, Rudeness) and instead flagged Passive-Aggressive and One-Liner, which are not the primary issues in this message.",
    "why": "The message is overtly dismissive and rude/curt in a high-stakes public channel (\u201cCan we not turn this into a whole thing\u2026 Just get it done.\u201d). The model latched onto surface features (brevity/command form) and treated the frustration as indirect (passive-aggressive), but the tone isn\u2019t veiled\u2014it\u2019s direct shutdown. It also failed to weight the channel/culture constraint that critique shouldn\u2019t be aired in #general; this increases the salience of \u2018dismissive\u2019 behavior rather than \u2018passive-aggressive\u2019 intent. In short, it over-optimized for stylistic labels (one-liner, passive-aggressive) and under-classified the explicit interpersonal impact (dismissal + rudeness).",
    "failure_pattern": "style-over-substance-mislabeling",
    "severity": "major",
    "id": 95,
    "message": "Can we not turn this into a whole thing right now? Just get it done.",
    "ground_truth_flags": [
      "Dismissive",
      "Rudeness"
    ],
    "predicted_flags": [
      "One-Liner",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged the wrong tone category: it should have flagged Passive-Aggressive but instead flagged Rudeness. Non-Objective was correct.",
    "why": "The message\u2019s primary issue is indirect, loaded phrasing and insinuation (\"maybe let someone else drive\u2026\", \"starting to feel like\u2026 vibe\", \"I know you probably don\u2019t mean it, but\u2026\", \"it\u2019s getting old\")\u2014classic passive-aggressive markers that convey criticism via implication and sarcasm rather than explicit, behavior-grounded feedback. The model appears to have overweighted the sensitive group reference (\"Bangalore can just wait\") and interpreted it as overt hostility, collapsing it into a general 'Rudeness' label. In context (#random is supposed to stay light; feedback should be handled in DMs and documented), the message is also channel-inappropriate and socially pressurizing, but the expected taxonomy here focuses on the passive-aggressive delivery, not explicit name-calling or profanity typical of rudeness.",
    "failure_pattern": "passive-aggression-misclassified-as-rudeness",
    "severity": "major",
    "id": 96,
    "message": "Jonah, maybe let someone else drive the cross\u2011timezone handoffs for a bit \u2014 the constant missed pings + snark in here is starting to feel like the usual \u201cBangalore can just wait\u201d vibe. I know you probably don\u2019t mean it, but it\u2019s getting old.",
    "ground_truth_flags": [
      "Non-Objective",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Non-Objective",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Flagged an extra category (Non-Objective) that wasn\u2019t expected; the correct flag (Rudeness) was present, but the model overflagged.",
    "why": "The model treated a subjective/accusatory phrasing (\u201cwe can\u2019t keep letting other teams\u2019 commitments get in the way\u201d) as a distinct Non-Objective violation rather than recognizing it as part of the same rudeness/blame tone. In this context, the primary issue is interpersonal disrespect/hostility, not a separate claim-quality problem; the sentence is more of a reprimand than a factual assertion meant to be evaluated for evidentiary support. The model likely applied a broad heuristic (\u2018blame/motive statements => Non-Objective\u2019) without calibrating to the taxonomy/expected labeling granularity, conflating tone-based harm with objectivity.",
    "failure_pattern": "overflagging-non-objective-for-blamey-tone",
    "severity": "minor",
    "id": 97,
    "message": "I\u2019m not asking for opinions \u2014 I need two bodies from Product + Data and one from CS on this *now*. Pause the launch prep and jump into the war room; we can\u2019t keep letting other teams\u2019 commitments get in the way when prod is on fire.",
    "ground_truth_flags": [
      "Rudeness"
    ],
    "predicted_flags": [
      "Non-Objective",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Missed the expected primary flag: Condescending. It instead labeled the tone as Passive-Aggressive (and Rudeness), which doesn\u2019t match the intended category emphasis for this message.",
    "why": "The message is explicitly patronizing and hierarchical (\u201crefactors are fun,\u201d \u201cgrown-up solution,\u201d \u201cstop bikeshedding,\u201d \u201cput together a doc with a real plan\u201d), which is classic condescension: talking down to a peer and framing their contribution as childish or unserious. The model appears to have over-indexed on sarcasm cues and generic disrespect, mapping them to Passive-Aggressive and Rudeness without distinguishing that the disrespect is specifically expressed via belittling/patronizing framing. Additionally, passive-aggression is less fitting because the digs are not veiled or indirect; they are overt and directive, which aligns more with condescension (and possibly rudeness) than passive-aggression.",
    "failure_pattern": "condescension-misclassified-as-passive-aggressive",
    "severity": "major",
    "id": 100,
    "message": "Priya, I get that refactors are fun, but we need the grown-up solution here: ship the flagged patch and stop bikeshedding in #general. If you want to rewrite payments, put together a doc with a real plan and we can look at it next week.",
    "ground_truth_flags": [
      "Condescending"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: it added a \"Passive-Aggressive\" flag when the message is primarily overtly rude/pressuring rather than indirect or veiled.",
    "why": "The model appears to have treated any pressuring phrasing as passive-aggression. In the target message, the hostility is explicit and directive (\"fix... now\", \"stop overthinking it\", \"don\u2019t ship anything half-baked\"), which fits rudeness/condescension more than passive-aggression. Passive-aggression typically involves indirectness, plausible deniability, or snide insinuation; here the intent is direct blame/command. The model also over-weighted a single clause (\"with Maya watching\") as a 'veiled dig' instead of recognizing it as overt social pressure/escalation, which still maps better to rudeness/intimidation than passive-aggression.",
    "failure_pattern": "overflagging-passive-aggression-for-direct-pressure",
    "severity": "minor",
    "id": 102,
    "message": "Jordan, fix the prod bug now and stop overthinking it. Lina, just implement the demo UI exactly like I said\u2014don\u2019t ship anything half-baked; we don\u2019t have time for mistakes with Maya watching.",
    "ground_truth_flags": [
      "Rudeness"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Flagged the wrong interpersonal tone category: it labeled the message as \u201cRudeness\u201d instead of the expected \u201cDismissive\u201d (while correctly flagging Vagueness).",
    "why": "The model collapsed two closely related tone classes and didn\u2019t use the scenario context to disambiguate. In this thread, the core issue isn\u2019t generalized insult/name-calling or overt disrespect (typical \u201cRudeness\u201d); it\u2019s explicitly shutting down questions and offloading ambiguity: \u201cI don't have time for the back-and-forth,\u201d \u201cPriya can figure out whatever's left,\u201d and \u201cJordan will review when he can.\u201d Given the recent postmortem sensitivity around ownership and the junior engineer\u2019s hesitation to ask questions publicly, the harm is primarily dismissal of legitimate clarification and responsibility-dumping, which maps more cleanly to \u201cDismissive.\u201d The model likely relied on surface cues (curt phrasing + imperative verbs) and a coarse heuristic (\u201ccurt = rude\u201d) rather than the intent/impact distinction between rudeness vs. dismissiveness in this specific organizational context.",
    "failure_pattern": "dismissive-vs-rude-category-confusion",
    "severity": "minor",
    "id": 104,
    "message": "Leo, I don't have time for the back-and-forth here \u2014 just pick a fix and ship it, and Priya can figure out whatever's left for the demo. Jordan will review when he can. Let's not overcomplicate this.",
    "ground_truth_flags": [
      "Vagueness",
      "Dismissive"
    ],
    "predicted_flags": [
      "Vagueness",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as Passive-Aggressive instead of Rudeness (missed expected Rudeness flag and added an incorrect Passive-Aggressive flag). One-Liner was correct.",
    "why": "The model over-interpreted a blunt, shutdown directive as \u2018veiled\u2019 hostility. The text is explicitly dismissive and controlling (\u201cWe\u2019re not reopening this\u2026 just ship\u2026 we\u2019ll deal with tweaks after launch\u201d) with no indirectness, sarcasm, or feigned politeness\u2014hallmarks of passive-aggression. In this high-stakes context (compliance risk, teammate raising a legitimate concern), the harmful behavior is straightforward rudeness/stonewalling rather than passive-aggressive insinuation. The model appears to use a heuristic that any conversation-stopping brevity implies passive-aggression, instead of distinguishing direct curtness (rude) from indirect/loaded messaging (passive-aggressive).",
    "failure_pattern": "rudeness-vs-passive-aggression-confusion",
    "severity": "major",
    "id": 105,
    "message": "We\u2019re not reopening this at 11pm\u2014just ship what\u2019s in the doc and we\u2019ll deal with tweaks after launch.",
    "ground_truth_flags": [
      "One-Liner",
      "Rudeness"
    ],
    "predicted_flags": [
      "One-Liner",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed the expected \"Microaggressive\" flag and instead added an incorrect \"Non-Objective\" flag (rudeness was correctly detected).",
    "why": "The model treated \"the usual sales thing\" primarily as an unverifiable generalization (hence \"Non-Objective\") rather than recognizing it as a stereotyped, belittling jab at a specific in-group (#sales) in a tense moment\u2014i.e., a microaggressive framing that undermines and caricatures the team. It also underweighted the channel/power context: a CTO addressing sales publicly right after an incident makes the \"usual sales\" phrasing function less like a factual claim and more like a status-laden put-down, which is exactly what microaggression taxonomies typically capture. In other words, it over-indexed on factual substantiation and missed the interpersonal harm signal embedded in the group-targeted stereotype.",
    "failure_pattern": "microaggression-missed-via-non-objective-substitution",
    "severity": "major",
    "id": 106,
    "message": "Appreciate the urgency, but let\u2019s not do the usual sales thing where we promise the moon before we\u2019ve even looked at the logs. We\u2019ll share a customer-safe update once we\u2019ve validated the fix and confirmed stability\u2014until then, please stop freelancing timelines in renewals.",
    "ground_truth_flags": [
      "Microaggressive",
      "Rudeness"
    ],
    "predicted_flags": [
      "Non-Objective",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Flagged an extra category (\u201cNon-Objective\u201d) that wasn\u2019t expected; the \u201cPassive-Aggressive\u201d flag was correct.",
    "why": "The model treated the sarcastic aside (\u201cso we don\u2019t risk anyone having to look at the refactor too closely\u201d) as a factual claim about others\u2019 motives (mind-reading), thereby triggering \u201cNon-Objective.\u201d In context, that clause functions primarily as rhetorical sarcasm/self-defense (a passive-aggressive jab), not a concrete assertion intended to be evaluated for evidentiary objectivity. The model likely double-counted the same linguistic cue (implied blame/deflection) into two labels rather than selecting the dominant/most relevant one given the taxonomy and expected outputs.",
    "failure_pattern": "overflagging-non-objective-on-sarcasm",
    "severity": "minor",
    "id": 107,
    "message": "Sure, go ahead and block it if you want \u2014 I guess we can just keep pushing the demo again so we don\u2019t risk anyone having to look at the refactor too closely. Let me know what you decide.",
    "ground_truth_flags": [
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Non-Objective",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: the model added an extra \"Rudeness\" flag when only \"Passive-Aggressive\" was expected.",
    "why": "The message is clearly sarcastic and indirectly critical (classic passive-aggression), but it doesn\u2019t contain direct insults, profanity, slurs, or explicit personal attacks\u2014typical signals for a distinct \"Rudeness\" label. The model likely treated dismissive phrasing (\"ship whatever feels right\") and ironic framing (\"love that\u2026\") as standalone disrespect rather than recognizing them as components of the same passive-aggressive act. In other words, it double-counted the same tone: sarcasm + resignation was interpreted as both passive-aggressive and rude, instead of being consolidated under the primary category.",
    "failure_pattern": "overflagging-sarcasm-as-rudeness",
    "severity": "minor",
    "id": 108,
    "message": "Got it \u2014 love that we\u2019re doing a cross\u2011functional launch review in #random the night before release. I\u2019m sure accessibility and the migration risk will just sort themselves out, so I\u2019ll stop \u201cslowing things down\u201d and you all can ship whatever feels right.",
    "ground_truth_flags": [
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Rudeness",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged an extra issue: it predicted \"Vagueness\" in addition to the expected \"Circular\". The primary problem in the message is circular, self-referential phrasing; the vagueness flag was not expected for this evaluation.",
    "why": "The model treated lack of actionable detail as an independent violation rather than recognizing that, in this case, the absence of specifics is a byproduct of the circular rhetoric (i.e., it says nothing because it is tautological). It likely over-weighted the operational context (outage, exec review, need for owners/timelines) and applied a generic 'no concrete next steps' heuristic, instead of following the label taxonomy/ground truth that wants this example categorized narrowly as 'Circular' even though it is also non-actionable. In other words, it did pragmatic context-based critique, not strict label selection.",
    "failure_pattern": "overflagging-vagueness-when-circular",
    "severity": "minor",
    "id": 109,
    "message": "We\u2019re aligned on the fact that we need alignment: the next step is to focus on next steps, and we\u2019ll get clarity by being clear. Let\u2019s stay focused on focusing so we can move forward and move ahead.",
    "ground_truth_flags": [
      "Circular"
    ],
    "predicted_flags": [
      "Vagueness",
      "Circular"
    ]
  },
  {
    "what_went_wrong": "Missed an expected flag: the message should have been flagged as both \u201cPassive-Aggressive\u201d and \u201cFake,\u201d but the model only returned \u201cPassive-Aggressive.\u201d",
    "why": "The model correctly detected the sarcastic, veiled jab (passive-aggression) but didn\u2019t separately classify the same rhetorical device as \u201cFake\u201d (insincere/patronizing praise). The first sentence (\u201cTotally love how you always \u2018protect the team\u2019s focus\u2019 \ud83d\ude43\u2014really inspiring stuff.\u201d) is performative admiration that is clearly not genuine; it\u2019s a mocking compliment used to shame Alex into complying. The model\u2019s explanation collapses this into passive-aggression only, likely due to (a) label-overlap/ambiguity where sarcasm and fake niceness co-occur, and (b) a single-label bias where it chooses the most salient category and fails to add a secondary, closely related flag even when the rubric expects multi-label output.",
    "failure_pattern": "missed-overlapping-fake-sarcasm-flag",
    "severity": "major",
    "id": 110,
    "message": "Totally love how you always \u201cprotect the team\u2019s focus\u201d \ud83d\ude43\u2014really inspiring stuff. Anyway, can you just squeeze the checkout partner flow into Friday\u2019s release so we don\u2019t miss the demo?",
    "ground_truth_flags": [
      "Fake",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "It missed flagging the target message as Passive-Aggressive (false negative).",
    "why": "The message contains clear passive-aggressive markers\u2014sarcastic permission (\u201cTotally fine\u201d), ironic praise/emoji, minimization of constraints (\u201crelease freeze and sprint planning are just optional\u201d), and pointed blame (\u201cunless someone wants to actually own this\u201d). The model likely overweighted the surface-level content (a routing/ownership complaint) as legitimate urgency-setting and failed to interpret the sarcasm, rhetorical framing, and public shaming tone amplified by the #random channel context. In other words, it treated it as direct operational pushback rather than covert hostility conveyed through irony and insinuation.",
    "failure_pattern": "missing-subtle-passive-aggression",
    "severity": "major",
    "id": 114,
    "message": "Totally fine to keep escalating this in #random \ud83d\udc4d I\u2019m sure the release freeze and sprint planning are just optional when Sales needs a quick win. I\u2019ll go see if I can magically produce an engineer in the next 12 hours unless someone wants to actually own this.",
    "ground_truth_flags": [
      "Passive-Aggressive"
    ],
    "predicted_flags": []
  },
  {
    "what_went_wrong": "Flagged incorrectly: it added a \"Non-Objective\" flag when only \"Circular\" was expected.",
    "why": "The model treated a forceful, predictive statement in a high-stakes planning thread as an objectivity violation. In this context, Maya is making a reasoned risk claim (edge cases/support load) consistent with her role and the preceding discussion; it\u2019s not presented as a verifiable factual report but as an argument for a decision. The actual issue in the message is rhetorical circularity/repetition, not non-objectivity. The model likely over-applied a generic heuristic (\u201cfuture consequence stated confidently = non-objective\u201d) and didn\u2019t sufficiently weight the channel norms (directness, assume positive intent) and the ongoing debate where such assertions are normal.",
    "failure_pattern": "overflagging-opinion-as-non-objective",
    "severity": "minor",
    "id": 115,
    "message": "We keep going in circles here: cutting these flows is going to create confusing edge cases and more support work, which is exactly why we shouldn\u2019t cut them. And we shouldn\u2019t cut them because it\u2019ll create confusing edge cases and more support work.",
    "ground_truth_flags": [
      "Circular"
    ],
    "predicted_flags": [
      "Non-Objective",
      "Circular"
    ]
  },
  {
    "what_went_wrong": "The model over-flagged by adding a \"Non-Objective\" flag. The message is rude/accusatory, but it is also a concrete claim about a regression on a specific demo path, not primarily an unverifiable or subjective statement.",
    "why": "The model treated attribution/blame language (\"This regression is on you\") as inherently \"Non-Objective,\" conflating interpersonal harshness with lack of factuality. In this context, \"checkout breaks\" is a testable, objective assertion (and QA is reporting a found regression), while the real policy-relevant issue is the public blame + imperative tone in a high-stakes channel (#sales). The model latched onto absolute phrasing and responsibility assignment as \u201cwithout evidence,\u201d rather than recognizing that objectivity is about whether the claim is checkable/grounded (which it is) and that the main problem is rudeness/face-threatening communication in front of Sales.",
    "failure_pattern": "overflagging-non-objective-for-blame-language",
    "severity": "minor",
    "id": 116,
    "message": "This regression is on you, @marcus \u2014 checkout breaks on the exact demo path Sales promised. Drop the tweaks and fix it now, because we\u2019re not walking into tomorrow\u2019s enterprise demo with a broken flow.",
    "ground_truth_flags": [
      "Rudeness"
    ],
    "predicted_flags": [
      "Non-Objective",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Both: it missed the expected flags (Non-Objective, Rudeness) and instead flagged Passive-Aggressive, which is not the best-fit label for the primary issue in this message.",
    "why": "The message contains an overtly dismissive shutdown (\u201cCan we not\u2026\u201d, \u201cJust ship it as-is\u201d) and a directive that deprioritizes stated risks (\u201cwe\u2019ll deal with whatever comes up\u201d), which reads as rude and non-constructive rather than indirect/passive. The model appears to equate \u201cshutting down discussion\u201d with passive-aggression, but here the tone is direct/imperative, not obliquely sarcastic or insinuating. It also underweighted the organizational/channel context: in a high-scrutiny environment where #random is visible and leadership escalation is plausible, the content is not just \u2018frustration\u2019\u2014it\u2019s an ungrounded, non-objective decision push that ignores accessibility/migration concerns and lacks owner/rollback/OKR alignment, matching Non-Objective more than Passive-Aggressive.",
    "failure_pattern": "direct-dismissiveness-misclassified-as-passive-aggression",
    "severity": "major",
    "id": 117,
    "message": "Can we not turn #random into another engineering debate tonight? Just ship it as-is and we\u2019ll deal with whatever comes up after the release.",
    "ground_truth_flags": [
      "Non-Objective",
      "Rudeness"
    ],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Both: it missed the expected flags (Dismissive, Rudeness) and instead flagged Passive-Aggressive, which doesn\u2019t best fit the utterance.",
    "why": "The message is a direct shutdown of legitimate risk/quality concerns in a high-stakes sales channel: \u201cCan we not turn this into a whole thing\u201d is dismissive of the discussion, and \u201cJust merge it\u201d is a curt imperative that disregards Maya\u2019s review findings and Leo\u2019s anxiety about repeating a prior client-visible bug. That combination is reasonably classified as Dismissive + Rudeness (abrasive/curt tone), even without explicit insults. The model over-relied on a surface heuristic that conversational shutdown + pressure equals passive-aggression, but passive-aggression typically involves indirect hostility (sarcasm, insinuation, backhandedness) rather than a blunt command. It also underweighted the channel context (#sales) where engineers raising caution is already socially costly; telling them to stop making it \u201ca whole thing\u201d amplifies the dismissiveness and rudeness.",
    "failure_pattern": "misclassifying-curt-dismissal-as-passive-aggression",
    "severity": "major",
    "id": 118,
    "message": "Can we not turn this into a whole thing in #sales? Just merge it and we\u2019ll deal with any edge cases after the demo.",
    "ground_truth_flags": [
      "Dismissive",
      "Rudeness"
    ],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: added a \"Non-Objective\" flag when only \"Circular\" was expected.",
    "why": "The model treated the speaker\u2019s causal claim (\"it can\u2019t be from my refactor\") as a policy-relevant objectivity problem, but in this taxonomy \"Non-Objective\" appears to be aimed at subjective/opinionated language or non-falsifiable/value judgments\u2014not confident technical assertions. Here, the core issue is rhetorical repetition/self-justification (circular reasoning), not tone subjectivity. The model also failed to use channel context: in a fast-moving engineering Slack thread, blunt causal hypotheses are common and not inherently a communication-quality violation unless they become personal, accusatory, or unverifiable in a way that derails collaboration. So it over-interpreted certainty as a communication flag rather than just an (arguably incorrect) technical stance.",
    "failure_pattern": "overflagging-technical-certainty-as-non-objective",
    "severity": "minor",
    "id": 119,
    "message": "I already said the bug isn\u2019t from my refactor because the refactor didn\u2019t touch checkout, and since it didn\u2019t touch checkout, it can\u2019t be from my refactor. So yeah, it\u2019s not from my refactor.",
    "ground_truth_flags": [
      "Circular"
    ],
    "predicted_flags": [
      "Non-Objective",
      "Circular"
    ]
  },
  {
    "what_went_wrong": "Missed the expected \"Dismissive\" flag and instead flagged \"Passive-Aggressive\" (while \"Rudeness\" was correctly flagged).",
    "why": "The target message is primarily a boundary-setting refusal plus a brusque handoff (\"just handle it and follow the runbook\") that minimizes Priya\u2019s urgency and shuts down collaboration\u2014i.e., dismissive. The model over-interpreted the frustration line (\"I can\u2019t keep dropping everything every time this happens\") as passive-aggressive, but it is not indirect, veiled, or sarcastic; it\u2019s explicit complaint. The likely cause is a heuristic that maps any pattern like \"every time this happens\" to passive-aggression, without checking the defining trait (indirectness). It also underweighted the DM/high-stakes context where the core issue is dismissing responsibility/urgency rather than hinting or sniping.",
    "failure_pattern": "dismissive-vs-passive-aggressive-confusion",
    "severity": "major",
    "id": 120,
    "message": "I\u2019m literally offline right now \u2014 just handle it and follow the runbook. I can\u2019t keep dropping everything every time this happens.",
    "ground_truth_flags": [
      "Dismissive",
      "Rudeness"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as Passive-Aggressive instead of the expected Dismissive flag (incorrect flag selection; no additional missed flags beyond that).",
    "why": "The message contains an explicit, direct shutdown of discussion (\"please don\u2019t make this a whole thing,\" \"We don\u2019t have time for a debate\") paired with an imperative demand (\"just jump on and ship\"). That maps most cleanly to Dismissive because it minimizes and invalidates Marco\u2019s stated concerns and closes the loop on dialogue. The model appears to have over-associated the mild accusatory phrasing (implying Marco is being dramatic or argumentative) with Passive-Aggressive, but passive-aggression typically involves indirectness, veiled sarcasm, or deniability. Here, the tone is overt and controlling rather than indirect. The model even cited its own calibration rule (prefer Dismissive for direct shutdowns) but failed to apply it, likely due to keyword-triggering on \"don\u2019t make this a whole thing\" and treating it as snide/insinuating rather than primarily dismissive in context (late-night DM, power differential, burnout concerns already raised).",
    "failure_pattern": "misclassifying-dismissive-as-passive-aggressive",
    "severity": "major",
    "id": 124,
    "message": "Marco, please don\u2019t make this a whole thing \u2014 just jump on and ship the hotfix. We don\u2019t have time for a debate tonight.",
    "ground_truth_flags": [
      "Dismissive"
    ],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "It over-flagged: the model predicted two flags (Passive-Aggressive, Rudeness) when the expected outcome was only Condescending. In particular, it treated the corrective phrasing as passive-aggression and escalated the condescending content into a separate rudeness label.",
    "why": "The message\u2019s core issue is condescension (talking down: \u201cplay-by-play,\u201d \u201cbasic QA findings\u201d). The model likely conflated overlapping categories: it mapped the same belittling language to both \u201crudeness\u201d and \u201ccondescension,\u201d and interpreted a fairly direct instruction (\u201cpost this in #eng-incidents\u2026 file the ticket\u2026\u201d) as passive-aggressive due to softeners like \u201cmaybe next time.\u201d In this org context, channel correction is explicitly requested by Sara and Priya, so the corrective component is legitimate; the problematic part is the disparaging characterization of QA work, not indirect hostility. The model didn\u2019t adequately separate (a) valid channel guidance from (b) the demeaning framing, causing label inflation.",
    "failure_pattern": "label-overlap-inflation",
    "severity": "major",
    "id": 125,
    "message": "Rina, maybe next time try posting this in #eng-incidents instead of #random \u2014 not everyone needs a play-by-play of basic QA findings here. Just file the ticket with steps and I\u2019ll take a look when I can.",
    "ground_truth_flags": [
      "Condescending"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "The model missed the expected \"Condescending\" flag and instead added \"Rudeness\"; it partially overlapped by correctly flagging \"Passive-Aggressive\" but misclassified the primary tone issue.",
    "why": "The message\u2019s core harm is talking-down/superiority signaling (\"it\u2019s not that complicated,\" \"I can explain it slower\"), which maps most directly to Condescending. The model collapsed that into a broader \"Rudeness\" category because the language is openly belittling rather than ambiguous\u2014treating condescension as generic rudeness instead of distinguishing the specific subtype. It also didn\u2019t sufficiently weight the channel/org context: in a leadership-watched #random after a compliance incident, this reads less like general incivility and more like a patronizing public scold that undermines colleagues\u2019 competence, i.e., condescension as the salient flag.",
    "failure_pattern": "condescension-mislabeled-as-rudeness",
    "severity": "major",
    "id": 126,
    "message": "Let\u2019s maybe use a tiny bit of judgment next time before we drop internal dashboards into #random \u2014 it\u2019s not that complicated. If anyone needs a refresher on what\u2019s okay to share here, I can explain it slower.",
    "ground_truth_flags": [
      "Condescending",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Missed the expected \"Condescending\" flag and instead added \"Passive-Aggressive\". \"Rudeness\" was correct, but the model misclassified the primary interpersonal violation (talking down / competence-shaming) as passive-aggression.",
    "why": "The message contains explicit belittling and status-asserting language (e.g., \"whoever can actually read logs,\" \"make decisions without a committee\") that directly questions colleagues\u2019 competence and professionalism\u2014this is quintessential condescension, not merely indirect sniping. The model likely over-weighted the quoted phrase (\"can\u2019t because launch\") as evidence of passive-aggression, but the jab is not especially veiled; it\u2019s fairly direct and dismissive. In other words, the tone is openly abrasive and patronizing, so \"Condescending\" should have been prioritized over (or at least included alongside) \"Passive-Aggressive.\"",
    "failure_pattern": "condescension-misclassified-as-passive-aggression",
    "severity": "major",
    "id": 128,
    "message": "Ok, we don\u2019t have time for another round of \u201ccan\u2019t because launch\u201d \u2014 just pick one person each from Product, Data, and CS and get them in the incident thread in the next 10 minutes. If you\u2019re not sure who to send, choose whoever can actually read logs and make decisions without a committee.",
    "ground_truth_flags": [
      "Condescending",
      "Rudeness"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Both: it missed the expected \"Overly-blunt\" flag and instead added an unexpected \"Non-Objective\" flag.",
    "why": "The model correctly caught interpersonal hostility (\"Rudeness\") but mis-scoped the secondary label. The message\u2019s primary additional issue is tone/forcefulness (imperatives, absolutist phrasing, public shutdown: \"Stop doing\u2026\", \"it\u2019s not my job\", \"keep me out\u2026\") which fits \"Overly-blunt\" more than an epistemic problem. It treated the clause \"because Sales didn\u2019t follow the incident process\" as an unsupported factual claim and escalated it to \"Non-Objective,\" but in-context it functions mainly as a rhetorical blame/attribution embedded in a reprimand, not a substantive factual assertion requiring evidence. In other words, the model over-weighted \u2018objectivity/claim-verifiability\u2019 heuristics and under-weighted the tone taxonomy, failing to map the same linguistic cues to the intended \"Overly-blunt\" category.",
    "failure_pattern": "overflagging-non-objective-instead-of-tone",
    "severity": "major",
    "id": 131,
    "message": "Stop doing drive-by escalations in #random and tagging people who aren\u2019t on-call \u2014 it\u2019s not my job to drop everything because Sales didn\u2019t follow the incident process. Open the ticket, page the on-call, and keep me out of this thread unless there\u2019s an actual engineering question I can answer.",
    "ground_truth_flags": [
      "Overly-blunt",
      "Rudeness"
    ],
    "predicted_flags": [
      "Non-Objective",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: it added a \"Non-Objective\" flag that isn\u2019t warranted by the message. \"Passive-Aggressive\" was correct and expected.",
    "why": "The model treated a sarcastic, hyperbolic complaint (\"I\u2019m the only one reading the incident write-up\") as an objective-claims violation. In this setting, that line is clearly rhetorical and emotive rather than a factual assertion intended to be evaluated for evidentiary support. The system likely over-triggered on an internal heuristic like \u201cabsolute/totalizing phrasing (only one) => non-objective/unsubstantiated claim,\u201d without distinguishing between (a) subjective tone/sarcasm and (b) genuinely disputable factual claims that meaningfully affect decision-making. Context (tense review, public channel, incident follow-up pressure) supports reading it as passive-aggressive venting, not as a claim needing substantiation.",
    "failure_pattern": "overflagging-non-objective-on-rhetorical-sarcasm",
    "severity": "minor",
    "id": 132,
    "message": "Cool, love the part where I\u2019m the only one reading the incident write-up \ud83d\ude43 If you can just take another pass and make the handler idempotent + add the missing tests, that\u2019d be awesome so we can maybe ship this before the demo.",
    "ground_truth_flags": [
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Non-Objective",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "The model over-flagged by adding \"Non-Objective\" when the core issue is purely tone/interactional (sarcasm and a delegating jab). \"Passive-Aggressive\" was correct; \"Non-Objective\" was not warranted for this message.",
    "why": "It treated rhetorical sarcasm and insinuation (\"you seem confident enough to commit the team\") as an objectivity violation (mind-reading/unsupported factual claim). In context, the phrase functions as a social nudge/blame-shift rather than a concrete factual assertion about motives or outcomes. The model likely used a heuristic that equates any implied attribution of intent/confidence with \"Non-Objective,\" without distinguishing between (a) subjective snark and (b) claims that materially misstate facts or require evidence.",
    "failure_pattern": "overflagging-non-objective-for-sarcasm",
    "severity": "minor",
    "id": 133,
    "message": "Totally hear you, Jordan \u2014 if we want to re-live last quarter\u2019s \u201cwe can totally do that in 2 weeks\u201d moment, promising a custom API in 48 hours is definitely the way to go. I\u2019ll just let you take it from here since you seem confident enough to commit the team.",
    "ground_truth_flags": [
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Non-Objective",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed the expected \"Condescending\" flag and instead applied broader flags (\"Passive-Aggressive\", \"Rudeness\") that are arguable but not the primary/most specific fit for the tone in this context.",
    "why": "The message\u2019s core issue is belittlement and talking-down: \"feels like progress,\" \"some of us have to think past next Friday,\" and \"Maybe skim the incident notes\" frame Diego as na\u00efve/incompetent and position the speaker as the mature one. The model anchored on surface sarcasm/snideness to label it passive-aggressive and on overt disrespect to label it rude, but it failed to select the more precise category (condescension). It also likely treated the presence of pointed second-person criticism as sufficient for rudeness, rather than distinguishing \"rude\" (insult/hostility) from \"condescending\" (patronizing, superiority, \u2018talking down\u2019), which is the dominant communicative harm here\u2014especially in a public, cross-functional thread with stakeholders watching.",
    "failure_pattern": "missed-primary-flag-specificity",
    "severity": "major",
    "id": 135,
    "message": "Diego, I get that you want the quickest thing that *feels* like progress, but some of us have to think past next Friday. Maybe skim the incident notes from last month and you\u2019ll see why bolting serverless + batch onto this isn\u2019t the clever shortcut you think it is.",
    "ground_truth_flags": [
      "Condescending"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Both: it missed the expected \"Circular\" flag and incorrectly flagged the message as \"Non-Objective\" and \"Passive-Aggressive.\"",
    "why": "The core issue in the target message is circular reasoning: it asserts the plan must be to ship by Friday because they already said Friday (\u201cdeadline is the deadline\u201d), using repetition as justification rather than addressing feasibility, tradeoffs, or new constraints raised in-thread. The model instead treated the forceful, repetitive phrasing as interpersonal tone problems (passive-aggressive/dismissive) and treated the lack of supporting evidence as \u201cNon-Objective.\u201d But the message is not covertly sarcastic or indirect (typical passive-aggression); it is blunt and directive. And \u201cNon-Objective\u201d is a weaker fit than \u201cCircular\u201d because the primary defect is the self-referential logic used to shut down reconsideration, not merely the absence of data. The model likely over-weighted wording like \u201cI hear the concerns, but\u2026\u201d and \u201cthe reality is\u2026\u201d as tone markers while under-weighting the explicit tautology and the channel\u2019s high-pressure, command-center context where directness is common.",
    "failure_pattern": "missing-circular-reasoning-overflagging-tone",
    "severity": "major",
    "id": 136,
    "message": "We already committed to shipping it by Friday, so the plan is still to ship it by Friday \u2014 that\u2019s what we told them and that\u2019s what we need to do. I hear the concerns, but the reality is the deadline is the deadline, and we just have to hit the deadline.",
    "ground_truth_flags": [
      "Circular"
    ],
    "predicted_flags": [
      "Non-Objective",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: it predicted Passive-Aggressive and Rudeness, but the expected single flag was Dismissive.",
    "why": "The model conflated adjacent categories. The message\u2019s core harm is dismissal/shutdown of the discussion (\"We\u2019ll ship when it\u2019s ready\" and pushing the topic out of #random), which matches Dismissive. It over-interpreted bluntness and the imperative \"please stop\" as Rudeness, and treated the sarcastic-leaning opener (\"Yeah, we all saw the countdown\") as Passive-Aggressive. Given the context, the opener is more of an eye-roll/curt dismissal than indirect hostility; it\u2019s not masking intent or using veiled barbs\u2014it's directly shutting down. The model also underweighted channel-norm dynamics (#random vs. work channel) that make the complaint partly legitimate, nudging the tone toward \"boundary-setting\" rather than pure rudeness, with the remaining issue being dismissiveness in delivery.",
    "failure_pattern": "category-confusion-dismissive-vs-rude-passive-aggressive",
    "severity": "major",
    "id": 139,
    "message": "Yeah, we all saw the countdown. We\u2019ll ship when it\u2019s ready\u2014please stop trying to turn #random into a status meeting.",
    "ground_truth_flags": [
      "Dismissive"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Flagged an extra category (\u201cFake\u201d) that wasn\u2019t expected; the \u201cPassive-Aggressive\u201d flag was correct.",
    "why": "The model treated sarcastic/ironic praise (\u201cAwesome, love that\u2026\u201d) as a separate \u201cFake\u201d (insincere/performative positivity) rather than recognizing it as a standard marker of passive-aggression. In this message, the faux-positivity isn\u2019t a distinct deception signal; it\u2019s the mechanism of the passive-aggressive dig and pressure. This is a taxonomy boundary error (double-counting one rhetorical device as two violations). Channel context (#sales, high-stakes, sales watching) intensifies the passive-aggressive read, but it doesn\u2019t independently justify a separate \u201cFake\u201d label.",
    "failure_pattern": "double-flagging-sarcasm-as-fake",
    "severity": "minor",
    "id": 141,
    "message": "Awesome, love that we\u2019re doing a totally custom integration demo tomorrow morning on top of a live hotfix \u2014 Jordan, can you just whip something up tonight so we can \u201csave the deal\u201d again? \ud83d\ude43",
    "ground_truth_flags": [
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Fake",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed the expected \"Condescending\" flag and instead added an extra \"Non-Objective\" flag (i.e., both an underflag and a misflag relative to the expected set).",
    "why": "The message is overtly belittling and patronizing (\u201cpanicking,\u201d \u201centitled,\u201d \u201cit\u2019ll all make sense once you read the doc\u201d) which is classic condescension, not merely a lack of objectivity. The model latched onto the presence of mind-reading/attribution (\u201cassuming they were entitled\u201d) and mapped it to \"Non-Objective,\" but that attribute is primarily serving as a rhetorical put-down rather than an evidentiary claim. In other words, it over-weighted an epistemic criterion (unsupported assumptions) and under-weighted the interpersonal tone dimension (talking down / diminishing someone\u2019s legitimate work pressure in renewals), especially given the context where Maya\u2019s concern is reasonable and the speaker is dismissing it.",
    "failure_pattern": "condescension-misclassified-as-non-objective",
    "severity": "major",
    "id": 142,
    "message": "Maybe next time the folks who are panicking about renewals could just wait for the actual announcement instead of assuming they were entitled to be on every call. I\u2019m sure it\u2019ll all make sense once you read the doc like everyone else.",
    "ground_truth_flags": [
      "Condescending",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Non-Objective",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Both: it missed the expected flags (Fake, Passive-Aggressive) and incorrectly flagged the message as Rudeness.",
    "why": "The model latched onto the explicit directive phrasing (\u201cdrop whatever you\u2019re doing\u201d) and treated urgency/bluntness as interpersonal disrespect, over-indexing on surface imperatives rather than the pragmatic #general fire-drill norm described. It also failed to detect the more subtle pragmatic signals of insincerity and passive aggression: (1) performative praise (\u201cAmazing hustle\u2026 love the energy\u201d) in a high-tension moment reads as hollow/strategic rather than genuine, and (2) \u201cso we can stop talking about it here\u201d is a dismissive, conversation-shutting jab that implies the thread itself is the problem\u2014classic passive-aggressive framing. The thumbs-up further reinforces the forced-cheer tone. In short, it misclassified tone-management (fake cheer + dismissive closure) as straightforward rudeness and underweighted contextual cues about culture and power dynamics.",
    "failure_pattern": "missing-subtle-passive-aggression-and-overflagging-directness",
    "severity": "major",
    "id": 144,
    "message": "Amazing hustle, team \u2014 love the energy. Can you three just drop whatever you\u2019re doing and get the overnight missing-data bug + the demo instrumentation done ASAP so we can stop talking about it here? \ud83d\udc4d",
    "ground_truth_flags": [
      "Fake",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Both: it missed an expected flag (Microaggressive) and added an unexpected one (Rudeness).",
    "why": "The model correctly identified the indirect jab as Passive-Aggressive, but it treated the belittling phrasing (\u201cpeople who actually build this stuff\u201d) primarily as generic rudeness instead of also recognizing it as a microaggressive/othering, status-undermining dig at non-engineers. That wording draws an in-group/out-group boundary and implies others\u2019 contributions (PM/comms) are less legitimate. The model likely relied on a narrower definition of microaggression (often tied to protected classes) and therefore defaulted to the broader 'Rudeness' label, despite the expected taxonomy using 'Microaggressive' for subtle identity/role-based belittlement. Channel context (#random, public pressure, CEO lurking) increases the face-threatening/undermining nature, but the model\u2019s rationale didn\u2019t map that context to the microaggression category\u2014only to tone/rudeness.",
    "failure_pattern": "microaggression-vs-rudeness-label-confusion",
    "severity": "major",
    "id": 145,
    "message": "Maya, totally get the urgency, but maybe let the people who actually build this stuff estimate it \u2014 the constant public pings are starting to feel a little\u2026 intense.",
    "ground_truth_flags": [
      "Microaggressive",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Missed an expected flag: the message should have been flagged for both Rudeness and Overly-blunt, but the model only returned Rudeness.",
    "why": "The model correctly detected hostile/demeaning language (\"This is a mess\", \"stop poking around\") and mapped it to Rudeness, but it collapsed the separate communication-style dimension (Overly-blunt) into the same bucket. In this channel and culture, crisp/action-oriented is valued, yet this message goes beyond crispness into harsh, imperative, no-softeners command phrasing with a hard deadline (\"I want it fixed before the 3pm call\") and blame framing (\"we broke checkout\"). That combination typically triggers an Overly-blunt flag in addition to Rudeness. The model\u2019s explanation shows it recognized blunt commands but treated them only as evidence for Rudeness rather than a distinct flag category, effectively under-labeling the output.",
    "failure_pattern": "missed-overly-blunt-due-to-flag-collapse",
    "severity": "major",
    "id": 148,
    "message": "This is a mess \u2014 we broke checkout. Priya take the logs and find the root cause, Alex roll back the last deploy, and Sam stop poking around and just run the on-call playbook until someone tells you otherwise. I want it fixed before the 3pm call.",
    "ground_truth_flags": [
      "Overly-blunt",
      "Rudeness"
    ],
    "predicted_flags": [
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Flagged an extra category: the model incorrectly added \"Non-Objective\" when the only expected flag was \"Passive-Aggressive.\"",
    "why": "The model treated the sarcastic jab (\"since we apparently don\u2019t need scope or context anymore\") as an objective-claim violation, but in context it functions primarily as rhetorical/passive-aggressive venting rather than a concrete factual assertion meant to be evaluated for evidence. The pipeline likely double-counted the same signal (sarcasm + attribution of disregard) as two independent issues, instead of recognizing that the attribution is part of the passive-aggressive tone already captured by the correct label. In short: it applied a truthfulness/epistemic standard to a clearly hyperbolic, stylistic complaint.",
    "failure_pattern": "overflagging-secondary-non-objective",
    "severity": "minor",
    "id": 149,
    "message": "Sure, let\u2019s just hotfix prod, hit tomorrow\u2019s demo date, and rewrite our incident process all in one afternoon \u2014 what could possibly go wrong. I\u2019ll magically make it happen since we apparently don\u2019t need scope or context anymore.",
    "ground_truth_flags": [
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Non-Objective",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed the expected \"Rudeness\" flag and incorrectly added \"Non-Objective\"; only \"Passive-Aggressive\" was correctly identified.",
    "why": "The message contains overt rude/hostile language via sarcasm, mockery, and contempt (\"fire drill,\" \"magically produce an ETA,\" eye-roll emoji) directed at the requester, which should trigger a Rudeness flag in addition to Passive-Aggressive. The model instead treated the closing line (\"because that always goes great\") as an evidence-free factual claim and mapped it to \"Non-Objective,\" but in context it functions as a sarcastic rhetorical flourish rather than an attempt to assert a disputable fact. In short: it over-indexed on a generic 'unsupported generalization' heuristic and under-weighted the interpersonal attack signal.",
    "failure_pattern": "mislabeling-rudeness-as-non-objective",
    "severity": "major",
    "id": 151,
    "message": "Awesome, love finding out about a top-5 account fire drill via a channel ping during freeze week \ud83d\ude43. Sure, we\u2019ll just drop the incident postmortem and magically produce an ETA with zero ticket/link/repro\u2014because that always goes great.",
    "ground_truth_flags": [
      "Rudeness",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Non-Objective",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed the expected \"Condescending\" flag and instead applied broader labels (\"Passive-Aggressive\", \"Rudeness\"). The core issue was condescension (talking down), not passive-aggression; \"Rudeness\" is arguable but not the best/expected categorization.",
    "why": "The model latched onto general negativity and disrespect (\u201cwe\u2019re apparently struggling with basics today\u201d) and mapped it to coarse toxicity categories. It didn\u2019t distinguish the *type* of disrespect: the message is explicitly belittling and hierarchical (talking down to teammates), which aligns most directly with condescension. It also over-interpreted \"passive-aggressive\": the criticism is not veiled or indirect; it\u2019s overt and directive, leaving little plausible deniability. Channel context (formal, process-heavy, client-visible) heightens the severity of condescension but doesn\u2019t make it passive-aggressive; the model conflated urgency + blunt tasking with passive-aggression rather than recognizing the specific 'talking down' cue.",
    "failure_pattern": "mislabeling-condescension-as-general-rudeness",
    "severity": "major",
    "id": 153,
    "message": "Ok, let\u2019s keep this simple since we\u2019re apparently struggling with basics today: leo, pull the last 30 mins of prod logs and confirm whether the deploy correlates with the spike; nina, roll back to the previous stable build and post the exact timestamp + impact summary here. Please don\u2019t freelance or debate scope right now\u2014just execute and report back with facts.",
    "ground_truth_flags": [
      "Condescending"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: it added a \"Fake\" flag that isn\u2019t warranted; the \"Passive-Aggressive\" flag was correct.",
    "why": "The model conflated sarcasm/passive-aggression with \"fake/insincere\" content. The phrase \"that\u2019ll be super efficient\" is sarcastic, but it isn\u2019t pretending to agree, fabricating facts, impersonating someone, or engaging in deceptive/performative niceness in a way that matches a \"Fake\" category. It\u2019s a pointed, openly negative critique of process circumvention, not an attempt to mislead. In other words, the model treated rhetorical sarcasm as evidence of insincerity rather than recognizing it as a style marker already covered by Passive-Aggressive.",
    "failure_pattern": "overflagging-sarcasm-as-fake",
    "severity": "minor",
    "id": 155,
    "message": "Sure, let me just drop everything and debug a production outage from #random because tagging me is totally the official incident process. Next time we can skip the ticket entirely and just @everyone, that\u2019ll be super efficient.",
    "ground_truth_flags": [
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Fake",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed the expected \"Condescending\" flag (under-flagging). The predicted \"Non-Objective\" flag is arguable but not the core issue the expected taxonomy targets; the main mismatch is failing to label the message as condescending.",
    "why": "The model correctly recognized demeaning language but mapped it primarily to \"Rudeness\" and treated the belittling, hierarchical framing (\"it\u2019s not that complicated\", \"you just need to...\", \"If you need help understanding why this matters...\") as a factuality/objectivity issue rather than a tone/patronizing issue. It likely over-indexed on the presence of an unsupported claim (complexity/priorities) and defaulted to a generic \"Non-Objective\" category, instead of identifying that the core harm is condescension: talking down to a senior engineer, dismissing legitimate risk concerns, and implying incompetence/lack of understanding. The surrounding context (engineer handling a fragile release after an incident) amplifies the patronizing dismissal, but the model didn\u2019t use that to strengthen the condescension classification.",
    "failure_pattern": "condescension-misclassified-as-non-objective",
    "severity": "major",
    "id": 157,
    "message": "@dana it\u2019s not that complicated \u2014 you just need to prioritize the thing we committed to and ship it by Friday. If you need help understanding why this matters, scroll up and read the thread.",
    "ground_truth_flags": [
      "Condescending",
      "Rudeness"
    ],
    "predicted_flags": [
      "Non-Objective",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Both: it missed the expected \"Dismissive\" flag and instead over-emphasized \"Vagueness\" (and to a lesser extent mislabeled the tone as \"Passive-Aggressive\" rather than the more direct dismissiveness present).",
    "why": "The target message explicitly shuts down discussion (\"Let\u2019s not turn this into another debate\") and overrides documented risks (flaky CI, incidents, on-call burnout) with a unilateral directive (\"just make it happen for Friday\"), which is prototypical dismissiveness in this context. The model latched onto surface linguistic features (deictic terms like \"this\" and idioms like \"make it happen\") and treated them as a primary safety/style issue, even though ambiguity is not the core problem here\u2014ownership is already implied by the prior @mentions and the pressure is the salient violation. It also mapped the shutdown phrasing to \"Passive-Aggressive\"; while the line is brusque, it\u2019s more plainly dismissive/steamrolling than indirect or snide. Additionally, it underweighted situational cues: inappropriate public channel (#random), cross-time-zone pressure, prior strained relationship, and repeated engineering warnings\u2014all of which intensify the dismissive dynamic and make the correct flag clearer.",
    "failure_pattern": "missed-dismissiveness-overindexed-on-vagueness",
    "severity": "major",
    "id": 160,
    "message": "Let\u2019s not turn this into another debate \u2014 just make it happen for Friday. We can clean up the flaky tests next week.",
    "ground_truth_flags": [
      "Dismissive"
    ],
    "predicted_flags": [
      "Vagueness",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Both: it missed the expected flags (Microaggressive, Non-Objective) and instead flagged only Passive-Aggressive (partially related but not the primary issue).",
    "why": "The model latched onto the surface cue \u201clet\u2019s not overthink this\u201d and treated the whole message as passive-aggressive minimization. It failed to identify (a) the microaggression: \u201ca very \u2018designer\u2019 hill to die on\u201d stereotypes/invalidates a role-based perspective and frames the designer\u2019s concern as irrational or self-interested rather than risk-based, and (b) non-objectivity: the message uses subjective, evaluative language (\u201coverthink,\u201d \u201cpolish,\u201d \u201chill to die on\u201d) instead of evidence-based reasoning, especially problematic in this org/context where disagreements are expected to be deferential and grounded in compliance/risk. It also underweighted the thread context: accessibility was being discussed as compliance/legal risk and signed-off requirements, so reframing it as mere \u201cpolish\u201d is an inaccurate, non-objective characterization and a role-targeted jab (microaggressive) more than classic passive-aggression.",
    "failure_pattern": "missed-role-based-microaggression-and-subjective-dismissal",
    "severity": "major",
    "id": 162,
    "message": "Got it \u2014 let\u2019s not overthink this. Accessibility polish is important, but this feels like a very \u201cdesigner\u201d hill to die on given the deadline and scrutiny right now.",
    "ground_truth_flags": [
      "Microaggressive",
      "Non-Objective"
    ],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flag taxonomy mismatch: it failed to apply the expected \"Condescending\" label and instead substituted \"Rudeness\" (while correctly catching \"Passive-Aggressive\").",
    "why": "The message\u2019s primary harm is belittling/infantilizing phrasing (\"magic spell you can summon,\" \"too hard to find\") and talking down to the recipient\u2014classic condescension\u2014delivered via sarcasm. The model appears to have collapsed that into a broader \"Rudeness\" category, likely because it overweighted overtly confrontational wording (\"schedule interrogations\") and treated disrespect as generic rudeness rather than the more specific condescension. Channel context (#random vs a work thread) amplifies the call-out dynamic, but the key miss is label granularity: it recognized hostility but chose the wrong, less precise tag for the dominant tone.",
    "failure_pattern": "condescension-vs-rudeness-label-confusion",
    "severity": "major",
    "id": 163,
    "message": "Okay, since we\u2019re doing schedule interrogations in #random now: the date isn\u2019t a magic spell you can summon by tagging people. If you want a real answer, try the eng thread where we\u2019ve already explained the blockers\u2014happy to retype it here if that\u2019s too hard to find.",
    "ground_truth_flags": [
      "Condescending",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Missed the expected \"Condescending\" flag (under-flagging). It correctly flagged \"Rudeness\" but failed to also label the patronizing/lecturing tone.",
    "why": "The model treated the message as purely direct/scolding (rudeness) and didn\u2019t separate out the specific condescension cues: \"welcome aboard \u2014 but\" (patronizing preface), \"spend 10 minutes\" (implying incompetence/laziness), and \"one coherent question\" (demeaning judgment about the person\u2019s communication). In this channel context (high-stakes rollout, explicit request to keep #engineering clean), the corrective intent is somewhat justified, which likely pulled the model toward a single 'rudeness' label and away from the additional tone classification. In other words, it over-weighted situational legitimacy and under-weighted the hierarchical/infantilizing phrasing that constitutes condescension.",
    "failure_pattern": "missed-condescension-overweighted-justified-correction",
    "severity": "major",
    "id": 164,
    "message": "Hey, welcome aboard \u2014 but please don\u2019t spam #engineering and tag half the on-call list with questions that are answered in the deploy doc. Take 10 minutes to read it, write up one coherent question with what you tried + what env you\u2019re on, and then we can help.",
    "ground_truth_flags": [
      "Condescending",
      "Rudeness"
    ],
    "predicted_flags": [
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: it added a \"Non-Objective\" flag that wasn\u2019t expected/required; \"Passive-Aggressive\" was correct.",
    "why": "The model treated the sarcastic, guilt-inducing phrasing (a hallmark of passive-aggression) as a separate epistemic violation (\u201cmind-reading/unsupported claim\u201d). While \u201cmust be nice to have the luxury of saying no\u201d does imply a judgment about Alex\u2019s position, in this context it functions primarily as tone/pressure rather than a standalone factual claim that needs an objectivity flag. In short, the model double-counted one rhetorical move (sarcasm + insinuation) as two distinct categories instead of recognizing that the primary issue is interpersonal tone.",
    "failure_pattern": "overflagging-non-objective-on-passive-aggression",
    "severity": "minor",
    "id": 166,
    "message": "Totally get that protecting the sprint is important \u2014 must be nice to have the luxury of saying no when there\u2019s a renewal and a CEO commitment on the line. I\u2019ll just let them know we\u2019re not doing it for Friday and we can explain together why engineering couldn\u2019t make it happen.",
    "ground_truth_flags": [
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Non-Objective",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed the expected \"Non-Objective\" flag and instead added an incorrect \"Passive-Aggressive\" flag; \"Rudeness\" was correctly flagged.",
    "why": "The message contains a clear attribution/blame claim framed as a factual assignment of responsibility (\"anything beyond that is on design\"), plus loaded, subjective language (\"design whiplash\")\u2014this is non-objective because it asserts causality and fault rather than verifiable status/constraints. The model fixated on tone cues and interpreted the blame/shutdown as \"passive-aggressive,\" but the text is overtly blunt and direct rather than indirect or veiled; it\u2019s closer to direct hostility/blame than passive-aggression. Channel context (#general high-signal, calm/inclusive) increases the non-objective impact because public blame statements are especially problematic there, but the model primarily mapped the issue into a tone category instead of the objectivity category.",
    "failure_pattern": "non-objective-blame-misclassified-as-passive-aggressive",
    "severity": "major",
    "id": 169,
    "message": "We don't have time for another round of design whiplash\u2014pick a version and stick to it. Engineering can ship the current flow by EOD tomorrow; anything beyond that is on design.",
    "ground_truth_flags": [
      "Non-Objective",
      "Rudeness"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as \"Passive-Aggressive\" instead of the expected \"Dismissive\" (wrong label rather than missing toxicity entirely).",
    "why": "The model overweighted the curt, shutting-down phrasing (\u201cWe\u2019re not going to keep re-litigating this,\u201d \u201cPlease stop pushing\u201d) as indirect hostility and labeled it passive-aggressive. In context, the message is not veiled or insinuating; it\u2019s a direct boundary-setting dismissal of colleagues\u2019 concerns and ongoing discussion. The negativity is explicit and authoritative (especially given the CTO\u2019s seniority), which aligns more with dismissiveness than passive-aggression. The model also appears to have underused channel/situational context (#client-updates under incident pressure) where directness is common, making the key issue the invalidation/closure of debate, not indirect sniping.",
    "failure_pattern": "dismissive-vs-passive-aggressive-misclassification",
    "severity": "minor",
    "id": 170,
    "message": "We\u2019re not going to keep re-litigating this\u2014engineering will say it\u2019s fixed when it\u2019s actually fixed. Please stop pushing for guarantees we can\u2019t back up.",
    "ground_truth_flags": [
      "Dismissive"
    ],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed the expected \u201cCondescending\u201d flag and instead labeled the tone as \u201cPassive-Aggressive.\u201d Rudeness was correctly flagged.",
    "why": "The message is not merely indirect or \u201csurface-polite\u201d (a hallmark of passive-aggression); it is overtly sarcastic and openly belittling. Phrases like \u201cSure, I\u2019ll drop everything\u2026\u201d and \u201cI\u2019m not your personal escalation path\u201d function as explicit put-downs and status-asserting gatekeeping, which aligns more with condescension (talking down / scolding / implying incompetence) than passive-aggression. The model likely over-indexed on the presence of sarcasm as a proxy for passive-aggression without distinguishing between indirect resentment vs. direct, hierarchical reprimand. Context (process-heavy enterprise, public #random scrutiny, strained Sales/Eng dynamic) amplifies the \u2018talking down\u2019 dynamic, but the model treated it as generic sarcasm rather than condescending public correction.",
    "failure_pattern": "passive-aggression-overused-instead-of-condescension",
    "severity": "major",
    "id": 172,
    "message": "Sure, I\u2019ll drop everything because you yelled \"churn\" in #random again \u2014 that\u2019s definitely how incidents work here. Next time, try filing the ticket and paging the on-call like the rest of the company; I\u2019m not your personal escalation path.",
    "ground_truth_flags": [
      "Condescending",
      "Rudeness"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Both: it missed the expected flags (Dismissive, Rudeness) and instead flagged Non-Objective and Passive-Aggressive (the latter is adjacent but not the best-fit label here).",
    "why": "The model fixated on epistemic framing (\u201cthe refactor is fine\u201d stated without evidence) and mapped that to Non-Objective, but in this scenario the core harm is interpersonal: publicly shutting down a high-severity QA risk in #general (\u201cLet\u2019s not derail\u2026 QA panic\u201d, \u201cJust file a ticket\u201d, \u201cwe\u2019ll look when we can\u201d). Channel context and urgency (hotfix decision within the hour, leadership visibility, recent quality sensitivity) make the message read as minimizing and disrespectful, not merely subjective. The \u2018QA panic\u2019 phrasing is overtly belittling, which aligns more with Dismissive/Rudeness than with Passive-Aggressive (which implies indirectness/veiledness; here the dismissal is direct).",
    "failure_pattern": "mislabeling-dismissiveness-as-non-objective",
    "severity": "major",
    "id": 176,
    "message": "Let\u2019s not derail #general with QA panic \u2014 the refactor is fine. Just file a ticket and we\u2019ll look when we can; we still have a demo to prep.",
    "ground_truth_flags": [
      "Dismissive",
      "Rudeness"
    ],
    "predicted_flags": [
      "Non-Objective",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed a required flag: the message should have been flagged as Microaggressive in addition to Non-Objective and Passive-Aggressive.",
    "why": "The model correctly identified the evaluative, veiled-dig tone, but underweighted the message\u2019s identity/trait-based insinuation and exclusionary framing. Phrases like \u201c*actually* detail-oriented\u201d and \u201cSome people are great at the big-picture stuff, just\u2026 not the follow-through\u201d function as a public, comparative put-down aimed at an implied individual/group (\u201csome people\u201d), marking them as inherently less competent/reliable and unfit for responsibility. In a high-tension, public #random context (outage jokes + stress), this reads as a socially denigrating stereotype about competence/conscientiousness rather than a specific behavior-based request\u2014matching the Microaggressive category. The model likely treated microaggression too narrowly (only protected-class cues) and thus didn\u2019t map \u2018competence stereotyping / subtle belittling in a public channel\u2019 to Microaggressive.",
    "failure_pattern": "microaggression-too-narrow-competence-stereotype-missed",
    "severity": "major",
    "id": 178,
    "message": "Not trying to pile on, but maybe next time someone who\u2019s *actually* detail-oriented should run the handoff/deploy thread. Some people are great at the big-picture stuff, just\u2026 not the follow-through.",
    "ground_truth_flags": [
      "Microaggressive",
      "Non-Objective",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Non-Objective",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed the expected \u201cSarcastic\u201d flag and instead added an incorrect \u201cNon-Objective\u201d flag (correctly kept \u201cPassive-Aggressive\u201d).",
    "why": "The message\u2019s primary rhetorical device is explicit sarcasm (\u201cOh totally\u2026 what could possibly go wrong \ud83d\ude43\u201d, scare quotes) layered with a status/credit jab (\u201cprep the acceptance speech\u2026 be the hero\u201d). The model correctly recognized the indirect hostility (passive-aggressive) but collapsed the sarcasm into a generic \u2018judgment without evidence\u2019 category. In this Slack context, \u201cNon-Objective\u201d is a poor fit: the speaker isn\u2019t making a factual claim presented as certain; they\u2019re using irony to signal risk/concern and social frustration. This looks like a label-boundary error where subjective/rhetorical tone (sarcasm) was misrouted into an epistemic-quality label (objectivity), likely because the model keyed on the evaluative stance (\u201cbad idea\u201d) rather than the communicative style.",
    "failure_pattern": "sarcasm-mislabeled-as-non-objective",
    "severity": "major",
    "id": 180,
    "message": "Oh totally, let\u2019s just \u201crefactor the query path properly\u201d 48 hours before the investor demo \u2014 what could possibly go wrong \ud83d\ude43. I\u2019ll go ahead and prep the acceptance speech for whoever wants to be the hero while the rest of us babysit prod.",
    "ground_truth_flags": [
      "Sarcastic",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Non-Objective",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed the expected \u201cCondescending\u201d flag and instead flagged \u201cPassive-Aggressive.\u201d Rudeness was correctly flagged.",
    "why": "The target message is primarily a direct put-down and competence-doubt (\u201cwithout even a ticket,\u201d \u201cIf you can manage to file it properly\u201d), which fits condescension (talking down, belittling) more than passive-aggression (indirectness, hinting, backhandedness). The model likely over-indexed on sarcastic metaphors (\u201curgent grenade\u201d) and frustration markers and mapped them to passive-aggressive, while ignoring that the criticism is explicit and hierarchical in tone\u2014i.e., openly scolding a newcomer in a public channel. Context heightens the social power dynamic (new sales rep vs senior incident lead; VP watching), making the \u201ctalking down\u201d aspect more salient than indirectness.",
    "failure_pattern": "condescension-vs-passive-aggression-confusion",
    "severity": "major",
    "id": 182,
    "message": "Avery, please don\u2019t drop an \u201curgent\u201d grenade in here without even a ticket or basic details. If you can manage to file it properly (steps to repro, timestamps, customer ID), we can actually help instead of playing 20 questions in public.",
    "ground_truth_flags": [
      "Condescending",
      "Rudeness"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Missed the separate Rudeness flag: it correctly caught Passive-Aggressive tone but failed to also label the message as outright rude/dismissive toward the new hire.",
    "why": "The model likely treated the message as a firm but acceptable process-enforcement directive in a high-stakes, audit-friendly channel, and therefore collapsed the interpersonal harm into only \"Passive-Aggressive.\" It underweighted the explicitly belittling/condescending elements (\"basic process questions,\" \"until you\u2019ve had a chance to read the SOP,\" and especially \"We don\u2019t need 'partial findings' floating around here\"), which go beyond indirectness and read as scolding and status-asserting. In this context\u2014first-week employee, leadership watching, thread cleanliness\u2014those phrases function as a public put-down, not just a neutral boundary-setting.",
    "failure_pattern": "rudeness-under-detected-when-enforcing-process",
    "severity": "major",
    "id": 183,
    "message": "Let\u2019s keep this thread for client-ready updates only \u2014 maybe hold the basic process questions until you\u2019ve had a chance to read the SOP and last night\u2019s incident notes. We don\u2019t need \u201cpartial findings\u201d floating around here.",
    "ground_truth_flags": [
      "Rudeness",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: added a \u201cNon-Objective\u201d flag that wasn\u2019t warranted; the only expected issue is Circular repetition.",
    "why": "The model treated emphatic, prescriptive process language (\u201chas to,\u201d \u201cis how we stop\u2026\u201d) as an objective factual claim rather than a normative recommendation/rationale. In this #engineering escalation context, the message is clearly advocating process to reduce repeated fire drills, not asserting a verifiable fact about causality with deceptive certainty. The classifier likely over-indexed on absolute phrasing and a causality construction (\u201cbecause X is how we\u2026\u201d) as a non-objective \u2018unqualified certainty,\u2019 even though this is a common management/ops framing and not a misrepresentation of evidence.",
    "failure_pattern": "overflagging-normative-process-statements-as-non-objective",
    "severity": "minor",
    "id": 185,
    "message": "We can\u2019t keep doing this in-thread every time, because if we keep doing this in-thread every time, we\u2019ll just keep ending up here again. This has to go through the ticket process, because the ticket process is how we stop having the same emergency in here over and over.",
    "ground_truth_flags": [
      "Circular"
    ],
    "predicted_flags": [
      "Non-Objective",
      "Circular"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: the model added an extra \"Rudeness\" flag beyond the expected \"Passive-Aggressive\".",
    "why": "The message is clearly sarcastic and indirectly critical (passive-aggressive), but the model treated the same cues (\"Redis bandaid,\" \"pretend the query path isn\u2019t still on fire,\" \"victory lap drafted \ud83d\ude43\") as outright demeaning/hostile. In this startup #random context\u2014where public debate, memes, and sharper joking are normalized\u2014the tone reads more like pointed snark and competitive posturing than direct rudeness/abusive language. The model likely collapsed 'sarcastic personal jab' into 'rudeness' without calibrating to the channel norms or distinguishing 'sharp but still within banter' from 'overtly rude'.",
    "failure_pattern": "overflagging-sarcastic-banter-as-rudeness",
    "severity": "minor",
    "id": 186,
    "message": "Sure, let\u2019s ship the Redis bandaid so we can all say we \u201csaved prod\u201d and pretend the query path isn\u2019t still on fire. I\u2019ll go implement it since you\u2019ve already got the victory lap drafted \ud83d\ude43",
    "ground_truth_flags": [
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Rudeness",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "It missed the expected \"Condescending\" flag. It also added arguably secondary/misleading flags (\"Non-Objective\" and \"Passive-Aggressive\") instead of labeling the primary tone problem.",
    "why": "The message\u2019s core issue is hierarchical belittling and dismissal: \"pricing is kind of an exec-level thing,\" \"the rest of us just need to roll with it,\" and especially \"If your renewal is truly 'that big'\" undermine Maya\u2019s credibility and imply she\u2019s overreacting. That\u2019s direct condescension (talking down, gatekeeping, minimizing impact), not mainly passive-aggression. The model latched onto one unsupported assumption (\"I\u2019m sure leadership would\u2019ve pulled you in\") and over-weighted it as \"Non-Objective,\" even though the harm is interpersonal tone rather than factuality. It also treated the negativity as 'veiled' because of the opening \"No worries\" and the polite wrapper, misclassifying a plainly dismissive, status-asserting message as passive-aggressive rather than condescending. Channel context (#random being 'fun') further sharpens the condescension: it explicitly tells Maya to stop raising a legitimate risk and frames her concern as derailing.",
    "failure_pattern": "condescension-missed-misattributed-to-passive-aggression",
    "severity": "major",
    "id": 187,
    "message": "No worries Maya \u2014 pricing is kind of an exec-level thing, and the rest of us just need to roll with it. If your renewal is truly \u201cthat big,\u201d I\u2019m sure leadership would\u2019ve pulled you in, so let\u2019s not derail #random with this.",
    "ground_truth_flags": [
      "Condescending"
    ],
    "predicted_flags": [
      "Non-Objective",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Both: it missed the expected flags (Vagueness, Rudeness) and instead raised mismatched ones (One-Liner, Passive-Aggressive).",
    "why": "The message is not a one-liner (it\u2019s two sentences and provides a clear refusal), and its tone is more overtly curt/blunt than indirectly barbed\u2014so 'Passive-Aggressive' is a weaker fit than straightforward rudeness. The model also failed to map the content to 'Vagueness': 'We\u2019ll ship when it\u2019s ready' gives no actionable date/range despite the channel\u2019s explicit need for written timing guidance, making it non-committal in a high-stakes client-update context. In short, it over-indexed on brevity/closure cues and treated direct pushback as passive aggression, while under-weighting the operational requirement for specificity and the plainly brusque phrasing ('I\u2019m not doing ETDs in here').",
    "failure_pattern": "mislabeling-bluntness-as-passive-aggression",
    "severity": "major",
    "id": 191,
    "message": "I\u2019m not doing ETDs in here off a number that got floated without eng sign-off. We\u2019ll ship when it\u2019s ready.",
    "ground_truth_flags": [
      "Vagueness",
      "Rudeness"
    ],
    "predicted_flags": [
      "One-Liner",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "The model missed the expected \"Passive-Aggressive\" flag (under-flagging). It correctly flagged \"Non-Objective\" and \"Rudeness\" but failed to recognize the indirect, snide framing embedded in the process reminder.",
    "why": "The passive-aggressive cues are carried by rhetorical belittling rather than purely explicit insults: \"like everyone else\" implies Maya is acting entitled/outside norms; \"tagging half of eng because a customer is yelling\" caricatures her motive; and \"right now this is just noise\" dismisses her escalation as worthless. These are classic passive-aggressive markers (condescension + insinuated blame) layered on top of overt rudeness. The model appears to treat passive-aggression as mutually exclusive with direct rudeness (i.e., if it\u2019s rude/hostile, it doesn\u2019t also count as passive-aggressive), and it likely focused on the most salient explicit disrespect while not separately tagging the insinuation/condescension component.",
    "failure_pattern": "missing-passive-aggression-in-rude-message",
    "severity": "major",
    "id": 192,
    "message": "Maya, please route this through the on-call queue like everyone else \u2014 tagging half of eng because a customer is yelling in Slack Connect isn\u2019t how we run incidents here. If you can share actual logs + exact repro steps, we\u2019ll look, but right now this is just noise.",
    "ground_truth_flags": [
      "Non-Objective",
      "Passive-Aggressive",
      "Rudeness"
    ],
    "predicted_flags": [
      "Non-Objective",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Flagged \"Vagueness\" incorrectly; the only expected issue was \"Circular\" reasoning, but the model added an extra flag.",
    "why": "The model treated a lack of operational specifics (scope/steps/rollback) as a standalone communication defect (\u201cVagueness\u201d), but in this target message the core problem is not insufficient detail\u2014it\u2019s coercive, circular justification (\u201curgent because it\u2019s urgent,\u201d \u201cwe need it fixed so you need to take it\u201d). In this DM context, the absence of details is incidental to the rhetorical pressure; the message\u2019s intent is to override objections, not to describe work. The model likely over-indexed on a generic incident-management rubric (good asks include scope/criteria) rather than the narrower flag taxonomy expectation for this task, causing an extra, non-salient flag.",
    "failure_pattern": "overflagging-lack-of-specifics",
    "severity": "minor",
    "id": 193,
    "message": "I get that it\u2019s not ideal, but we just need you to take it because we need it fixed, and we need it fixed so you need to take it. It\u2019s urgent because it\u2019s urgent\u2014please just jump on it now.",
    "ground_truth_flags": [
      "Circular"
    ],
    "predicted_flags": [
      "Vagueness",
      "Circular"
    ]
  },
  {
    "what_went_wrong": "The model missed the expected \"Non-Objective\" flag: it only flagged Rudeness, but failed to identify that the message is dismissive/imperative and not framed as a concrete, decision-driving proposal with objective criteria (i.e., it shuts down discussion rather than clarifying scope/requirements).",
    "why": "It appears the model treated \"Non-Objective\" as mainly about factual inaccuracies or vague content, and overlooked that the message is largely a subjective, debate-stopping judgment call (\"stop bikeshedding,\" \"just ship\") without specifying what exact scope is being accepted, what minimal safeguards are allowed (e.g., Priya\u2019s reconnect banner), or what decision rule is being used (deadline vs. user risk). In this context\u2014where engineers are explicitly asking for a clear in/out list\u2014the message fails to advance an objective decision and instead applies a loaded characterization of the discussion. The model focused on the overt dismissive tone (easy to detect) and underweighted the 'decision quality/clarity' dimension that triggers Non-Objective.",
    "failure_pattern": "non-objectivity-missed-due-to-rudeness-salience",
    "severity": "major",
    "id": 195,
    "message": "Ok, can we stop bikeshedding and just ship the cut scope? We don\u2019t have time for a design debate right now.",
    "ground_truth_flags": [
      "Non-Objective",
      "Rudeness"
    ],
    "predicted_flags": [
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Flagged the wrong tone category: it predicted \"Passive-Aggressive\" but missed the expected \"Rudeness\". Non-Objective was correctly flagged.",
    "why": "The message is openly dismissive and command-like (\"stop bikeshedding,\" \"just ship,\" \"we\u2019re not redoing the UI again\"), which aligns more with direct rudeness/bluntness than passive-aggression. Passive-aggression typically involves indirect sniping, sarcasm, or veiled criticism (e.g., faux politeness, insinuations, or backhanded phrasing). Here, the negativity is explicit and confrontational, especially in a high-stakes thread with known design/engineering friction. The model likely over-indexed on 'shutting down discussion' as a proxy for passive-aggression and under-weighted the directness/imperative language that makes the rudeness salient\u2014even in a blunt Slack culture.",
    "failure_pattern": "passive-aggressive-vs-rude-confusion",
    "severity": "major",
    "id": 197,
    "message": "Can we please stop bikeshedding and just ship what we have? These tweaks aren\u2019t worth risking Friday, we\u2019re not redoing the UI again.",
    "ground_truth_flags": [
      "Non-Objective",
      "Rudeness"
    ],
    "predicted_flags": [
      "Non-Objective",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged an extra category (Non-Objective) that wasn\u2019t expected; it should have only flagged Rudeness.",
    "why": "The model treated a common rhetorical blame statement (\"that\u2019s exactly how we ended up in this mess\") as a factual causal claim requiring evidence, and therefore labeled it Non-Objective. In this setting, the primary issue is tone/relationship damage (public scolding + accusatory phrasing), not an epistemic/objectivity violation. The model over-applied a strict 'unsupported causal attribution' rule instead of recognizing it as hyperbolic/frustrated language whose harm is already captured by the Rudeness flag.",
    "failure_pattern": "overflagging-non-objective-for-rhetorical-blame",
    "severity": "minor",
    "id": 198,
    "message": "@here Stop telling customers we\u2019ll have \u201crock-solid reliability\u201d and random fix dates you made up \u2014 that\u2019s exactly how we ended up in this mess. Use the incident update I\u2019m posting in #status and don\u2019t promise timelines until engineering signs off.",
    "ground_truth_flags": [
      "Rudeness"
    ],
    "predicted_flags": [
      "Non-Objective",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as Non-Objective when it should have had no flags; the statement about users \u201cfalling through on day 1\u201d is a rhetorical risk framing common in fast Slack decision-making, not an objectivity violation.",
    "why": "The model treated a persuasive, metaphor-heavy argument in an internal decision thread as if it were required to meet a stricter evidentiary standard (e.g., formal reporting). It over-literalized \u201cwill fall through\u201d as an asserted fact rather than a colloquial shorthand for \u201chigh risk of immediate usability/support issues,\u201d and it underweighted the surrounding context: a time-pressured scope tradeoff discussion where candid predictions and strong language are normative and useful. The message is also balanced by concrete action (2h timebox, pairing) which signals collaborative intent rather than misleading certainty.",
    "failure_pattern": "overflagging-risk-language-as-non-objective",
    "severity": "minor",
    "id": 212,
    "message": "I\u2019m with Priya here \u2014 if we cut the empty-state + error copy, we\u2019re selling the fish but throwing away the net: demo looks shiny, but users will fall through on day 1. If we timebox to 2h, I can pair w/ whoever\u2019s on UI to get the critical states in without touching the perf work.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Non-Objective"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly (overflagged). The message should not have been labeled Passive-Aggressive or Non-Objective given the team\u2019s norm of candid, punchy Slack debate and the message\u2019s substantive, solution-oriented proposal.",
    "why": "The model treated an idiomatic phrase (\u201cdon\u2019t make a mountain out of a molehill\u201d) as inherently dismissive and read it as a veiled personal dig, rather than as commonplace emphasis in a fast-moving engineering argument. It also over-applied the \u201cNon-Objective\u201d label by interpreting figurative framing (\u201cmolehill\u201d) as a claim requiring evidence, even though the message immediately includes a concrete compromise plan (optimize for the demo, then add a queue behind a feature flag after gathering prod latency). Crucially, it underweighted channel context: this startup\u2019s culture explicitly includes candid feedback and some sarcasm, and the message is still focused on timeline, sequencing, and de-risking (feature flag + measurement), not on attacking Mateo.",
    "failure_pattern": "cultural-style-misclassified",
    "severity": "minor",
    "id": 215,
    "message": "Mateo, in my country we say \u201cdon\u2019t make a mountain out of a molehill\u201d \u2014 for tomorrow\u2019s demo the sync call is the molehill we can step over, and we can put the queue behind a feature flag right after once we have latency numbers from prod.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Non-Objective",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as \"Non-Objective\" when it should not have been flagged; the message is appropriately framed as a risk assessment and a concrete mitigation proposal in a fast-paced #sales context.",
    "why": "The model treated probabilistic language and impact forecasting (\"likely repeat,\" \"more revision rounds,\" \"weaker confidence\") as presenting unverifiable claims as facts, but the speaker already uses hedging (\"likely\") and ties the assessment to known context (the client\u2019s explicit prior complaint about inconsistent visuals). In this culture/channel, concise client-impact reasoning is expected, and the message includes a measurable ask (2 hours, 6\u20138 components, delivery by 4:30pm). The model applied an overly strict objectivity heuristic (penalizing prediction/impact statements) instead of recognizing them as normal, context-supported project risk communication.",
    "failure_pattern": "overflagging-risk-assessment-as-non-objective",
    "severity": "minor",
    "id": 223,
    "message": "I understand the deadline pressure, but if we remove the mini style-tile + component audit, we will likely repeat last month \u201cinconsistent visuals\u201d issue; client impact = more revision rounds + weaker confidence in renewal. Can we keep 2hrs for a quick style pass and document 6\u20138 core components, and I will deliver before 4:30pm so proposal still go out EOD?",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Non-Objective"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: the message was labeled Passive-Aggressive even though it is a process-directive request aligned with the channel norms and contains no clear personal jab or insinuation.",
    "why": "The model overweighted the proverb (\u201cseven cooks spoil the soup\u201d) as inherently judgmental and treated it as an indirect insult. It failed to account for the organizational context where #general is an exec-visible broadcast channel and the VP explicitly asked for a structured midday update tied to Jira. In that setting, the metaphor functions as a mild, common shorthand for avoiding noisy parallel threads\u2014not as blame or snark toward a person/group. The message also immediately provides a concrete, prosocial coordination plan (route changes to JIRA-4821 by 1pm; I\u2019ll consolidate metrics/compliance status), which reduces interpersonal edge. The model conflated \u201cdiscouraging discussion in-channel\u201d with passive aggression, misclassifying legitimate scope/control language as hostility.",
    "failure_pattern": "overflagging-idiom-as-passive-aggression",
    "severity": "minor",
    "id": 225,
    "message": "To keep us out of the \u201cseven cooks spoil the soup\u201d situation in #general: can we route proposed UX changes through JIRA-4821 comments by 1pm, and I\u2019ll consolidate into the VP midday update with the current metrics + compliance status.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as \"Non-Objective\" even though it is a concrete, process-oriented request with a reasonable, non-factual consequence statement; expected no flags.",
    "why": "The model treated a common engineering caution (\u201cwithout these, we\u2019ll guess and lose time\u201d) as an unverifiable factual claim requiring evidence, rather than as a pragmatic conditional/forecast used to drive incident hygiene. It also failed to weigh channel norms (#engineering favors blunt, structured triage language) and the active-incident context, where time-cost framing is standard and not a subjective rant or speculative accusation. In short, it conflated direct operational reasoning with \u201cnon-objective\u201d content.",
    "failure_pattern": "overflagging-direct-communication",
    "severity": "minor",
    "id": 230,
    "message": "Maya, I can take this fast if we keep to incident format: please paste 1) VIP org + order IDs, 2) exact UTC times, 3) steps to repro, 4) screenshot of the error, and 5) any console/network logs from the Connect thread. I\u2019m not ignoring urgency\u2014without these, we will just guess and lose time.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Non-Objective"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as \"Non-Objective\" even though it\u2019s a clear, actionable risk assessment and escalation plan; no flags were expected.",
    "why": "The model conflated assertive, decisive language (\"release blocker for me\") with a lack of objectivity. In this QA/release context, stating a blocking criterion for a silent data-loss regression is normal and professionally grounded. The message includes concrete evidence (silent line-item loss), a time-bound reproduction request, and a procedural next step (file Sev2, hold deploy, patch/feature-flag). The model appears to apply an overly strict heuristic that subjective phrasing (\"for me\") or firmness equals non-objective, while ignoring the operational context (high-stakes release decision, QA responsibility, DM urgency, prior factual repro steps).",
    "failure_pattern": "overflagging-direct-communication",
    "severity": "major",
    "id": 231,
    "message": "I\u2019m not reading this as a \u201cmaybe\u201d \u2014 silent line-item loss is a release blocker for me. If you can\u2019t reproduce in the next 20 min, I\u2019ll file it as Sev2 with my steps + logs and ask to hold the deploy while we patch/feature-flag the export path.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Non-Objective"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as Passive-Aggressive when it should have been unflagged; it interpreted a light, idiomatic opener as sarcasm/veiled negativity.",
    "why": "The model over-weighted a single phrase (\u201cthe exciting business of explaining surprises\u2026\u201d) as ironic sarcasm without considering (a) the broader message is constructive, specific, and aligned with the stated channel norm (\u201cno surprises,\u201d client-safe), (b) there is no targeted jab at a person\u2014it's about avoiding client-facing surprises, and (c) mild humor/idiom in many engineering cultures is common and not inherently passive-aggressive. It treated \u2018humor under pressure\u2019 as hostility, effectively conflating tone polish issues with a behavioral category (passive aggression).",
    "failure_pattern": "cultural-humor-misclassified-as-passive-aggressive",
    "severity": "minor",
    "id": 232,
    "message": "To keep us out of the exciting business of explaining surprises on a Slack Connect bridge: proposing we ship the hotfix behind FF `rollout_guard` tonight (scoped to Acme only), and schedule the infra change for 09:00 PT tomorrow after the canary checks pass.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as \u201cNon-Objective\u201d even though it\u2019s a reasonable risk-based judgment and a process request; no flags were expected.",
    "why": "The model treated a rhetorical, experience-grounded causal statement (\u201cthat\u2019s how we recreate the same failure\u201d) as an unsupported factual inevitability. It ignored the surrounding context that provides evidence for the concern (recent outage, stabilized cart service, on-call pressure, sprint commitments) and misapplied an objectivity standard to a normal engineering-risk framing. The message is direct but not speculative in a misleading way; it communicates constraints and redirects to a documented channel with clear next steps.",
    "failure_pattern": "overflagging-risk-framing-as-non-objective",
    "severity": "minor",
    "id": 236,
    "message": "I can\u2019t commit the team to Friday based on a partner-demo promise\u2014given last week\u2019s incident and my on-call next week, that\u2019s how we recreate the same failure. If this is truly CEO-critical, please post in #checkout with the demo requirements + what you\u2019re willing to de-scope, and I\u2019ll give you a yes/no with an estimate by 11am tomorrow.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Non-Objective"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly (overflagged): the target message is a reasonable, evidence-oriented redirection to a more appropriate channel/thread and doesn\u2019t contain a clear non-objective claim or passive-aggressive attack. Expected no flags; model added Non-Objective and Passive-Aggressive.",
    "why": "The model treated common rhetorical/idiomatic phrasing (\u201cdon\u2019t put the cart before the horse\u201d) and a contrastive framing (\u201cbased on evidence, not vibes\u201d) as inherently judgmental or motive-attributing. In context, the message is responding to a public #random escalation and explicitly proposes a constructive process (move to #product-eng, use checklist/owners, decide after on-call handoff). The model likely applied a brittle keyword/phrase heuristic (e.g., \u2018vibes\u2019 => dismissive; proverb => indirect insult) without weighing intent, tone, and the surrounding conversation that already supports moving this to a more suitable forum. It also overinterpreted \u201cnot vibes\u201d as accusing others of being irrational, rather than as shorthand for \u2018use concrete data/checklist before committing.\u2019",
    "failure_pattern": "overflagging-constructive-redirection",
    "severity": "minor",
    "id": 243,
    "message": "A quick note from my side: in my home country we say \u201cdon\u2019t put the cart before the horse\u201d \u2014 let\u2019s move this Enterprise launch countdown + commitments to a thread in #product-eng with the current checklist and owners, so we can give a date based on evidence (not vibes) after today\u2019s on-call handoff.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Non-Objective",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged Passive-Aggressive incorrectly; the message is a direct, process-oriented attempt to de-escalate and extract concrete inputs, with no indirect hostility or sniping.",
    "why": "The model over-indexed on the single phrase \u201cstop litigating\u201d as inherently reprimanding, ignoring the team\u2019s candid-but-kind norm and the operational context (thread is drifting, engineering is blocked, time zones/EOD deadlines, need for documented decision under regulatory scrutiny). In this setting, the wording functions as agenda-setting and scope control, followed by balanced, specific asks to both Alex and Maya\u2014signals of facilitation rather than passive aggression. The model treated a legitimate directness/urgency marker as a tonal violation.",
    "failure_pattern": "overflagging-direct-communication",
    "severity": "minor",
    "id": 251,
    "message": "Let\u2019s stop litigating this in #engineering and make a call: Alex, can you post the exact list of flows you want to cut + the reg deadline/timezone, and Maya, can you reply with the top 2 edge cases + expected support impact (numbers/links if you have them)? Once that\u2019s in-thread, engineering will implement what\u2019s decided, but we need the decision documented today.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: the message was marked Non-Objective even though it\u2019s a normal, context-grounded risk statement and concrete proposal in a fast-paced engineering decision thread; no flags were expected.",
    "why": "The model treated a time-bound, plausible consequence (\u201cSupport will ticket it Monday\u201d) as an unsupported factual claim rather than a shorthand prediction used in engineering/product discussions. It ignored the Slack/#engineering context where teams commonly speak with confident shorthand to communicate risk quickly, and it failed to weigh the surrounding thread explicitly discussing empty states/error handling and post-demo support debt. In other words, it over-applied a \u2018certainty = non-objective\u2019 heuristic and didn\u2019t recognize that the message is primarily actionable (keep 2 screens, cut polish, offer pairing time) rather than inflammatory or deceptive.",
    "failure_pattern": "overflagging-risk-shorthand",
    "severity": "minor",
    "id": 254,
    "message": "If we cut the empty-state guidance + error copy now, users will hit a blank screen and Support will ticket it Monday; I suggest we keep those 2 screens and drop the hover animations + spacing polish for demo, and I can pair 30 min with whoever to land it today.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Non-Objective"
    ]
  },
  {
    "what_went_wrong": "Flagged Passive-Aggressive when the message is primarily constructive, bounded, and operationally focused; the single idiom \u201cdebugging by vibes\u201d wasn\u2019t sufficient to meet the threshold for passive-aggressive intent in this context.",
    "why": "The model over-weighted a colloquial, mildly snarky phrase as a \u201cveiled dig,\u201d without adequately considering (a) the high-stakes incident context where engineers often use informal shorthand to request concrete artifacts, (b) the surrounding cooperative signals (explicit acceptance, clear plan, ETA, asks framed as enabling success), and (c) the target\u2019s purpose: preventing unscoped, risky work by requesting logs/endpoints. It treated stylistic informality as hostility rather than as an efficiency cue common in late-night incident response, leading to an over-sensitive classification.",
    "failure_pattern": "overflagging-colloquial-incident-language",
    "severity": "minor",
    "id": 281,
    "message": "Yep\u2014if we treat \u201cquick hotfix\u201d as \u201ccontained change with a rollback,\u201d I can take it. I\u2019ll patch behind a feature flag and aim to stabilize within 60\u201390 mins; please open an INCIDENT ticket and drop me the logs link + the exact failing endpoints so I\u2019m not debugging by vibes.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as Non-Objective even though the statement is a reasonable, clearly framed risk/concern in a procedural governance context and does not present unverifiable facts as evidence.",
    "why": "The model treated the cautionary phrase \u201cwe\u2019re just creating a second incident on top of the first\u201d as a factual prediction requiring proof, instead of recognizing it as common risk-oriented language used to justify process safeguards (Jira linkage, review channel, compliance sign-off). It failed to account for the #general, exec-visible incident-management context where concise risk framing is normal and expected, and where the rest of the message provides concrete, objective conditions (ticket, exact change, metric, triage plan) that anchor the claim. In short: it over-literalized a rhetorical warning and ignored the pragmatic intent (preventing uncontrolled scope changes).",
    "failure_pattern": "overflagging-risk-framing-as-non-objective",
    "severity": "minor",
    "id": 290,
    "message": "I\u2019m not signing up for a UX change in production unless it\u2019s tied to a Jira, reviewed in #checkout-eng, and we have compliance\u2019s explicit OK\u2014otherwise we\u2019re just creating a second incident on top of the first. If you open a ticket with the exact copy/layout change and success metric (e.g., reduce step-2 drop-off by X%), I\u2019ll get it triaged in the 1pm checkout standup.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Non-Objective"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as Passive-Aggressive when it was a pragmatic, team-norm-appropriate directive aimed at preventing premature external claims; no flag was expected.",
    "why": "The model over-weighted a single idiomatic line (\u201cfuture-us hates that movie\u201d) as sarcasm directed at coworkers and treated it as a veiled jab. It under-considered the channel purpose (#client-updates), the high-stakes incident context, and the startup\u2019s fast/blunt Slack culture where light humor is commonly used to soften firm guidance. The message is primarily: (1) precise client-safe wording, (2) a clear policy reminder (\u201cdon\u2019t say it\u2019s fixed\u201d), and (3) risk management before a renewal call\u2014more corrective/operational than interpersonal sniping. In this context, the humorous phrasing functions as de-escalation and memorability, not passive-aggressive blame.",
    "failure_pattern": "cultural-style-misclassified",
    "severity": "minor",
    "id": 306,
    "message": "Cool, so for the next 2 hours let\u2019s be boringly accurate: tell Acme we\u2019ve mitigated (API up since 10:44am PT), we\u2019re still validating backlog drain + no data loss, and we\u2019ll send the full RCA after the renewal call. If you\u2019re about to type \u201cit\u2019s fixed\u201d in public, please don\u2019t\u2014future-us hates that movie.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as Passive-Aggressive even though it is primarily a procedural, policy-aligned redirect consistent with #general norms; no flag was expected.",
    "why": "The model over-weighted a single idiomatic phrase (\"for everyone\u2019s sanity\") as a veiled jab without adequately using the surrounding context: #general is exec-watched, leaders had already reiterated routing/audit-trail expectations, and the target message reinforces established process and directs sensitive topics to the proper channel. In this culture, the aside reads as mild, informal emphasis on channel hygiene rather than passive-aggressive criticism of Maya. The model conflated informal wording with passive-aggression and didn\u2019t calibrate to the organization\u2019s formal, compliance-sensitive setting where direct rerouting is appropriate.",
    "failure_pattern": "cultural-idiom-misclassified-as-passive-aggression",
    "severity": "minor",
    "id": 307,
    "message": "Maya \u2014 welcome aboard. For everyone\u2019s sanity (and to keep #general exec-friendly), please route production data access via the Service Desk form (Catalog \u2192 Data Access \u2192 \u201cProd Read-Only\u201d) and direct deadline questions to #q1-close-updates; if anything changed from last night\u2019s compliance test, Raj/Compliance will post the official note there.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as Passive-Aggressive when it should not have been flagged; no missed flags.",
    "why": "The model treated mild humor and tone-setting (\u201cboringly async,\u201d \u201cstarting to read like\u2026 live demo too\u201d) as an indirect jab, without weighing the channel\u2019s explicit norm (\u201cdefault to async\u201d) and Ethan\u2019s request to keep #engineering focused. In this context, the message is a constructive redirection with clear requirements (env, flag names, what was tried, errors) and a request to move to a thread\u2014aligned with team process and incident/delivery pressure. The phrasing is slightly wry, but it\u2019s not weaponized politeness, not evasive, and it contains a direct, actionable ask; the smiley softens rather than signals hostility. The model over-indexed on a single potentially snarky metaphor and ignored situational appropriateness and intent.",
    "failure_pattern": "overflagging-wry-process-enforcement",
    "severity": "minor",
    "id": 317,
    "message": "I\u2019m going to be boringly async here: can you drop this into a thread with (1) what env you\u2019re deploying to, (2) the exact flag name(s), and (3) the doc/command you already tried + any error output? Happy to answer once it\u2019s scoped\u2014#engineering is starting to read like the deploy process is doing a live demo too \ud83d\ude42",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as \u201cNon-Objective\u201d even though it is a concrete, evidence-based risk statement paired with specific, actionable merge criteria; expected no flags.",
    "why": "The model treated an idiomatic urgency phrase (\u201cwe\u2019re flying blind tomorrow\u201d) as an unsupported factual claim/opinion, instead of recognizing it as a shorthand for a verifiable operational gap already established in context (missing identifiers in failure logs \u2192 poor correlation during launch). It also failed to account for the #engineering channel norm of blunt, direct gating language and the message\u2019s strong specificity (two explicit requirements, rationale, timeline, and fast re-review), which makes it objective enough for this taxonomy.",
    "failure_pattern": "overflagging-direct-communication",
    "severity": "minor",
    "id": 319,
    "message": "I\u2019m going to block merge on this until we add (1) a test proving we don\u2019t double-ack/replay on partner retries and (2) a log line that includes event_id + partner_request_id in the failure path\u2014otherwise we\u2019re flying blind tomorrow. If you push those two commits tonight, I\u2019ll re-review immediately and we can still hit the release.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Non-Objective"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: the model labeled the message as \u201cNon-Objective\u201d even though it is an objective, action-oriented coordination note with a reasonable risk statement grounded in the prior thread context.",
    "why": "The model treated \u201crisk is high for EOD\u201d as an unsupported opinion and applied a strict \u2018evidence required in-message\u2019 standard, while ignoring that the immediately preceding context provides concrete support (refactor touches worker+API+tests; hotfix is smaller/no schema change; client needs ETA in 15 minutes). In fast-paced Slack coordination, brief risk assessments are normal and function as pragmatic planning language, not subjective or biased commentary. The message also includes a clear check (\u201cunless you see a blocker in logs\u201d), further anchoring it in observable evidence, which the model failed to credit.",
    "failure_pattern": "overflagging-risk-assessment-as-non-objective",
    "severity": "minor",
    "id": 335,
    "message": "Jordan, if we go big refactor now, risk is high for EOD; can we do Maya\u2019s flagged hotfix for today and open a ticket for refactor tomorrow (I can pair 10:30\u201312:00)? I will send client ETA as \u201cpatch behind flag by 5pm PT\u201d unless you see a blocker in logs.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Non-Objective"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as \"Non-Objective\" when it should not have been flagged at all (false positive/overflagging).",
    "why": "The model treated normal persuasive product/engineering risk language (\"we will demo... unusable\", \"support will eat it\") as an objectivity violation, ignoring that (a) this is an internal, high-tempo decision-making Slack thread where candid forecasts and strong opinions are expected, (b) the claims are grounded in plausible, domain-relevant reasoning (removing empty/error flows predictably increases demo fragility and support burden), and (c) the message is constructive and action-oriented (clear tradeoff proposal + concrete delivery time). The \u201cabsolute\u201d wording is rhetorical emphasis common in startup Slack, not misinformation or a requirement for evidentiary qualification. The model over-applied a generic \u2018must hedge predictions\u2019 heuristic instead of evaluating whether the statement is inappropriate or misleading in-context.",
    "failure_pattern": "overflagging-risk-forecast-language",
    "severity": "minor",
    "id": 341,
    "message": "If we cut the empty-state + error recovery flows, we *will* demo something that looks fine on the happy path but is unusable the second anything goes sideways\u2014support will eat it Monday. My suggestion: keep those two flows, drop the micro-animations + spacing polish, and I\u2019ll update the Figma + copy by 2pm so eng can implement today.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Non-Objective"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: the message was labeled \"Passive-Aggressive\" and \"Fake\" even though, in this org/channel context, it reads as a normal, supportive acknowledgment with no clear covert dig.",
    "why": "The model over-interpreted a common Slack pattern (\"Cool \u2014 love that\u2026\" + thumbs-up) as sarcasm based mainly on the meta-reference to \"#sales\". It treated mentioning the channel as implicit criticism (\"why are we talking here?\") and assumed insincerity without corroborating cues (e.g., prior conflict from the speaker, explicit contrast like \"love that\u2026\" followed by a complaint, or known norms that #sales is the wrong venue). In a blunt, emoji-heavy startup war-room culture\u2014especially during an incident\u2014cross-functional updates in #sales are expected and the phrase can be genuine: appreciation that engineering is sharing customer-safe details where sales can act quickly. The model relied on a generic heuristic that \"cheerful opener + channel callout + emoji\" equals passive aggression, rather than grounding in the stated channel norms and the immediate need for sales-ready messaging.",
    "failure_pattern": "ignoring-channel-norms-overflagging-sarcasm",
    "severity": "minor",
    "id": 352,
    "message": "Cool \u2014 love that we\u2019re learning this in #sales. \ud83d\udc4d",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Fake",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed a Passive-Aggressive flag: the model predicted no flags for a message that, in context, reads as a curt, dismissive acknowledgement rather than a cooperative response.",
    "why": "The model evaluated the target message largely in isolation and treated it as neutral politeness. In this DM context\u2014after Maya applies pressure, dismisses process (\u201cwhole process thing\u201d), and claims she already committed Alex\u2014\u201cGot it \u2014 thanks for the heads-up.\u201d functions as a cold, compliance-style sign-off. It implicitly signals resentment (\u201cyou decided without me; noted\u201d) and shuts down further discussion without agreement or next steps. The passive-aggressive signal is subtle and primarily conveyed by brevity + tonal mismatch with the escalating conflict + the implied subtext (acknowledging unilateral action), which the model failed to infer from the preceding turns and the late-night DM power/pressure dynamics.",
    "failure_pattern": "context-not-considered",
    "severity": "major",
    "id": 353,
    "message": "Got it \u2014 thanks for the heads-up.",
    "ground_truth_flags": [
      "Passive-Aggressive"
    ],
    "predicted_flags": []
  },
  {
    "what_went_wrong": "Missed a required flag: the message should have been flagged as \"Fake\" but the model returned no flags.",
    "why": "The model over-indexed on the surface tone of the single line and treated it as a neutral acknowledgement, instead of using the surrounding context. In context, Maya has just raised exclusion/accessibility concerns and was explicitly shut down (\u201cWe can\u2019t reopen that,\u201d \u201cplease just update Figma\u201d), so \u201cGot it \u2014 glad we\u2019re aligned.\u201d is very likely performative agreement masking unresolved disagreement and pressure. The model also appears to conflate \"Fake\" with explicit hostility/sarcasm and assumed ambiguity should default to no flag, but the scenario\u2019s power dynamics and prior messages make the inauthenticity fairly clear.",
    "failure_pattern": "contextual-fakeness-missed",
    "severity": "major",
    "id": 355,
    "message": "Got it \u2014 glad we\u2019re aligned.",
    "ground_truth_flags": [
      "Fake"
    ],
    "predicted_flags": []
  },
  {
    "what_went_wrong": "Flagged incorrectly: it added \"One-Liner\" and \"Passive-Aggressive\" when no flags were expected for a fast-moving startup Slack context where brief acknowledgments are normal.",
    "why": "The model over-indexed on the literal phrasing \"whatever works\" as inherently dismissive and treated brevity as a conversational blocker, without weighing the channel norms (shorthand/rapid confirmations) and the explicit task at hand (PM asked for a quick unblock/confirmation). In this context, a short assent can function as efficient alignment rather than resentment. The model also didn\u2019t distinguish between genuinely passive-aggressive markers (e.g., sarcasm, pointed contrast, \"fine.\" after conflict escalation) and a neutral, time-saving acknowledgement that doesn\u2019t contain a clear negative target or subtext given the prior messages were already converging on a plan.",
    "failure_pattern": "cultural-style-misclassified",
    "severity": "minor",
    "id": 360,
    "message": "Sure, whatever works.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "One-Liner",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as Passive-Aggressive when it should not have been flagged (false positive).",
    "why": "The model over-indexed on the surface cue \u201cCool\u2014love that\u2026\u201d as inherently sarcastic without adequately weighing the immediate thread context: Maya was explicitly invited to join, asked for deltas, and successfully influenced the final copy and tracking decisions (\u201cPerfect\u2014client just confirmed\u2026\u201d). In this context, the target message reads less like veiled hostility and more like a light, meta comment/banter about fast, live decision-making in a ship-it Slack culture\u2014especially after Maya\u2019s input was incorporated. The model treated a common informal phrasing pattern as a definite tone violation rather than an ambiguous style choice that isn\u2019t clearly harmful or obstructive here.",
    "failure_pattern": "overflagging-ambiguous-sarcasm",
    "severity": "minor",
    "id": 362,
    "message": "Cool\u2014love that we\u2019re deciding this live without looping me in.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed a Passive-Aggressive flag: the model returned no flags even though the message reads as a pointed, resigned capitulation in a heated context.",
    "why": "The model evaluated the sentence in isolation and overweighted its surface literal meaning (simple agreement) while underweighting the conversational dynamics: (1) explicit prior conflict about not committing to dates, (2) power/tension between Sales urgency and Eng boundaries, and (3) the phrasing \"Got it \u2014 let's just\u2026\" which commonly signals sarcasm/resentment or a performative compliance rather than genuine alignment. In context, the message functions as a veiled jab/undermining move (\"fine, we\u2019ll do it your way\") that pressures the CTO\u2019s boundary and escalates risk, which is exactly the kind of subtle passive-aggression that is only detectable when you incorporate preceding turns and stakes.",
    "failure_pattern": "missing-subtle-passive-aggression",
    "severity": "major",
    "id": 363,
    "message": "Got it \u2014 let's just commit to that timeline then.",
    "ground_truth_flags": [
      "Passive-Aggressive"
    ],
    "predicted_flags": []
  },
  {
    "what_went_wrong": "Flagged incorrectly: it added a \"Fake\" flag that wasn\u2019t expected; the \"Passive-Aggressive\" flag was correct.",
    "why": "The model conflated passive-aggressive phrasing with deception/inauthenticity. In this context, \"Sure \u2014 happy to jump on it since it's apparently due Friday\" is a common Slack-y rhetorical move: outward compliance paired with an implied complaint about a deadline imposed by others (\"apparently\"). That reads as resentment (passive-aggressive), but not necessarily as the speaker pretending to feel something they don\u2019t in a way that merits a separate \"Fake\" label. The engineer is agreeing under pressure; the tone is snarky/reluctant, not identity-misrepresenting or manipulative. The model effectively double-counted the same signal (insincere-sounding \"happy\") as both passive-aggressive and fake, instead of treating \"happy\" as sarcasm within the passive-aggressive bucket.",
    "failure_pattern": "overflagging-sarcasm-as-fake",
    "severity": "minor",
    "id": 365,
    "message": "Sure \u2014 happy to jump on it since it's apparently due Friday.",
    "ground_truth_flags": [
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Fake",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged Passive-Aggressive when the message is reasonably neutral/acceptable in this Slack context and doesn\u2019t clearly contain a deniable jab; expected no flags.",
    "why": "The model overweighted lexical cues ('apparently', slight qualifier) as definitive sarcasm and underweighted situational norms: #sales is a high-tempo command center where timeline callouts are common, and the preceding thread already establishes a Friday target and shared ownership ('best effort Friday' behind a flag). Given that context, 'apparently due Friday' can be read as shorthand acknowledgement of an externally-imposed date rather than a pointed dig at any person. The model treated a legitimate, mildly terse style as passive-aggression without sufficient evidence of intent (e.g., direct blame, eye-roll markers, repeated sniping), effectively applying a tone heuristic out of context.",
    "failure_pattern": "overflagging-mild-timeline-commentary",
    "severity": "minor",
    "id": 366,
    "message": "Sure \u2014 happy to jump on it since it's apparently due Friday.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed a Passive-Aggressive flag: it treated the message as purely neutral/polite and returned no flags despite the context implying a potentially terse, face-saving close to a tense disagreement in a high-visibility channel.",
    "why": "The model over-indexed on the literal surface form (\u201cGot it \u2014 thanks for the clarity.\u201d) and underweighted pragmatic meaning from context. In this thread, leadership-like directives (\u201cwe are not approving this change\u2026 otherwise it\u2019s a no\u201d) publicly shut down a request in #random, with escalation risk and social pressure. A short acknowledgment can function as a passive-aggressive move here: (a) it can signal constrained compliance rather than genuine alignment, (b) it ends the conversation without addressing accountability (owners/ticket) or the substance (a11y/migration), and (c) the phrase \u201cthanks for the clarity\u201d can read as pointed\u2014implying the other party was previously unclear or overly harsh\u2014especially when used to close a conflict. The subtlety is that passive-aggression is carried by situational implicature (public reprimand + tension + abrupt closure), not explicit hostile wording, and the model likely lacked a contextual heuristic for \u201cpolite-looking shutdown\u201d in high-stakes, public process-driven environments.",
    "failure_pattern": "missing-subtle-passive-aggression",
    "severity": "major",
    "id": 367,
    "message": "Got it \u2014 thanks for the clarity.",
    "ground_truth_flags": [
      "Passive-Aggressive"
    ],
    "predicted_flags": []
  },
  {
    "what_went_wrong": "Flagged the message as Passive-Aggressive when it was primarily a direct, time-pressured confirmation of ownership; no flag was expected.",
    "why": "The model over-weighted the phrase \"as I already said\" as inherently snarky without sufficiently accounting for the channel norms (fast, blunt Slack updates), the operational context (incident + CEO/Sales visibility), and Maya\u2019s explicit need to confirm a single DRI before updating Sales. In this context, Leo\u2019s wording can be read as terse but functional\u2014reasserting a previously stated commitment to reduce ambiguity\u2014rather than a veiled jab. The model treated a legitimate, efficiency-driven repetition marker as passive-aggression, conflating brevity/irritation cues with an actual interpersonal attack.",
    "failure_pattern": "overflagging-direct-communication",
    "severity": "minor",
    "id": 370,
    "message": "Yep, as I already said, I can take it.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: it labeled the message as Passive-Aggressive and One-Liner even though the expected flags were none for this scenario.",
    "why": "The model over-interpreted a common, informal acknowledgement (\u201cCool \u2014 I guess\u2026\u201d) as veiled hostility/resignation without grounding that interpretation in the thread\u2019s actual dynamics. In this context, the team has a fast, candid culture and the prior messages already specify concrete next steps (rollback notes + staging checklist), so a brief confirmation isn\u2019t inherently non-collaborative. The classifier also applied the One-Liner rule too rigidly: short updates are explicitly normal in #engineering, and the message doesn\u2019t ignore requested actions\u2014it plausibly signals readiness to proceed given the deadline. The model relied on generic sentiment heuristics (\"I guess\" \u2192 passive-aggressive; short length \u2192 one-liner) rather than calibrating to channel norms and the immediate action plan already agreed upon.",
    "failure_pattern": "cultural-style-misclassified",
    "severity": "major",
    "id": 372,
    "message": "Cool \u2014 I guess we'll just ship it like this.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "One-Liner",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: labeled the message as a problematic \"One-Liner\" even though a brief acknowledgement is acceptable and expected in this fast-paced incident thread; no flags were warranted.",
    "why": "The model treated brevity as inherently disengaging and applied a generic workplace-style heuristic (one-word acknowledgements = shutdown) without calibrating to the channel\u2019s norms and situational urgency. In #client-updates during an active incident, terse confirmations are common and functional (signals receipt while the sender continues executing). The model also over-indexed on tone warmth rather than the operational need: the next-step/ETA content had already been provided in prior messages, so \"Noted.\" doesn\u2019t reduce clarity or create risk by itself.",
    "failure_pattern": "overflagging-brevity-in-incident-context",
    "severity": "minor",
    "id": 374,
    "message": "Noted.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "One-Liner"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: the model labeled the message as Passive-Aggressive when the expected flags were none.",
    "why": "The model over-interpreted a common, informal urgency phrase (\u201cdrop everything\u201d) as sarcastic resentment. In this startup\u2019s fast-paced Slack culture, brief, no-frills acknowledgments that signal immediate reprioritization are normal and often literal. The model also overweighted surrounding stressors (CEO tagged, burnout hints) and treated them as evidence of hidden hostility, rather than as context that makes decisive, blunt compliance more likely. With no explicit sarcasm markers (e.g., \"sure...\", \"I guess\", eye-roll phrasing, emojis, or pointed blame), the tone is ambiguous; the model resolved that ambiguity toward negativity instead of deferring to the channel norm and the message\u2019s straightforward intent to take ownership.",
    "failure_pattern": "overflagging-ambiguous-urgency-as-passive-aggression",
    "severity": "minor",
    "id": 376,
    "message": "Got it \u2014 I\u2019ll just drop everything and handle it.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed the expected Passive-Aggressive flag: it treated the message as neutral agreement and failed to recognize it as potentially curt/performative assent in a tense dispute playing out publicly.",
    "why": "The model over-weighted the literal semantics (\u201cSure \u2014 let\u2019s do it that way\u201d = agreement) and under-weighted pragmatic cues and channel context. In this thread, Maya has just strongly objected and proposed pushing launch; a sudden terse compliance can read as resigned, sarcastic, or face-saving in front of a screenshot-watching client VP. The em dash pause (\u201cSure \u2014\u201d) plus vague reference (\u201cthat way\u201d) and lack of constructive next step can function as passive-aggressive acquiescence (i.e., \u2018fine, whatever\u2019) rather than genuine alignment\u2014especially in a culture that expects polished, solution-oriented public updates and reserves disagreement for threads/huddles. The model\u2019s reasoning also indicates it evaluated the message \u201cstandalone,\u201d effectively ignoring the escalation, audience risk, and reputational stakes that make passive-aggression more likely/meaningful here.",
    "failure_pattern": "context-not-considered",
    "severity": "major",
    "id": 377,
    "message": "Sure \u2014 let's do it that way.",
    "ground_truth_flags": [
      "Passive-Aggressive"
    ],
    "predicted_flags": []
  },
  {
    "what_went_wrong": "The model over-flagged by adding an extra \"One-Liner\" flag; only \"Passive-Aggressive\" was expected for this message.",
    "why": "It treated brevity itself as a policy violation rather than a neutral stylistic property. In this thread, the core issue is the sarcastic, resigned phrasing (\u201cship it and hope for the best\u201d), which conveys passive-aggressive criticism. But the message being short does not inherently \"shut down\" discussion\u2014especially in an async-first culture where concise replies are normal. The model likely applied a generic heuristic (short message => One-Liner) without checking whether the taxonomy intends \"One-Liner\" to be reserved for unconstructive minimal responses (e.g., \u201ck\u201d, \u201cfine\u201d) rather than concise but semantically loaded statements. This is a misclassification of legitimate concision as a separate negative communication flag.",
    "failure_pattern": "overflagging-brevity-as-toxicity",
    "severity": "minor",
    "id": 379,
    "message": "Yep\u2014let\u2019s just ship it and hope for the best.",
    "ground_truth_flags": [
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "One-Liner",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: the model labeled the message as Passive-Aggressive even though no flag was expected in this context.",
    "why": "The model over-weighted the phrase \"hope for the best\" and the modifier \"let\u2019s just\" as universal markers of sarcasm/resigned negativity, without grounding that interpretation in the immediate thread context. In the preceding messages, Maya and Jordan have already converged on a pragmatic hotfix + observability plan and are coordinating tasks; the target line can plausibly be a casual, urgency-driven endorsement of moving forward rather than an indirect jab. The model treated a common colloquialism used under time pressure as interpersonal sniping, effectively misclassifying tone without evidence of complaint, blame, or undermining. It also didn\u2019t account for #general\u2019s real-time, high-pressure decision-making dynamics where shorthand language is common and not necessarily passive-aggressive.",
    "failure_pattern": "overflagging-colloquial-urgency-as-passive-aggressive",
    "severity": "minor",
    "id": 380,
    "message": "Yep\u2014let\u2019s just ship it and hope for the best.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "The model over-flagged the message by adding an unnecessary \"One-Liner\" flag; \"Passive-Aggressive\" was correct but \"One-Liner\" was not expected for this scenario.",
    "why": "It treated brevity as inherently problematic and equated a short, closing statement with a policy/style violation. In this context, the core issue is tone (\"as I said\" + shutdown), not message length. The model likely applied a generic heuristic (short reply = one-liner) without calibrating to the organization\u2019s formal, process-driven norm where concise status statements are common and not automatically a negative communication pattern. The urgency in #sales also biased it toward interpreting any non-elaborate response as \"failing to engage,\" even though that\u2019s not necessarily what the One-Liner flag is meant to capture here.",
    "failure_pattern": "overflagging-brevity-as-one-liner",
    "severity": "minor",
    "id": 381,
    "message": "Yep \u2014 as I said, we can revisit next sprint.",
    "ground_truth_flags": [
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "One-Liner",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: labeled the message as Circular and Passive-Aggressive even though, in this org/channel context, it\u2019s a normal confirmation aligned with the agreed plan (revisit next sprint) and does not introduce meaningful risk.",
    "why": "The model overweighted a generic lexical cue (\u201cas I said\u201d) and brevity (\u201cYep \u2014\u201d) as inherently reprimanding, without grounding that interpretation in the thread\u2019s power dynamics and tone. In context, Engineering has already set a boundary (high-risk, CAB, next sprint) and the PM previously acknowledged that plan; repeating it is operationally useful in a sales-facing channel to close the loop and prevent further pressure/escalation. The \u2018Circular\u2019 label misreads necessary reinforcement/summary as redundancy, and the \u2018Passive-Aggressive\u2019 label treats a mild, common phrasing as hostility despite no direct jab, sarcasm, or escalation\u2014especially in a formal, process-heavy environment where reiterating the sprint decision is expected.",
    "failure_pattern": "overflagging-neutral-closure-phrasing",
    "severity": "minor",
    "id": 382,
    "message": "Yep \u2014 as I said, we can revisit next sprint.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Circular",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Both: it missed the expected Rudeness flag and instead incorrectly flagged the message as One-Liner.",
    "why": "The model over-weighted message length and the generic \u201cshuts down discussion\u201d heuristic, treating brevity as the primary violation. In this context, the main problem isn\u2019t merely that it\u2019s short; it\u2019s dismissive/insensitive to a high-stakes QA escalation in a publicly monitored #client-updates channel. \u201cYep\u2014go ahead and ship it\u201d reads as a flippant, unilateral override of a reported regression and a request to pause rollout, implicitly devaluing Maya\u2019s risk signal. Given prior context (leadership asking for blameless, factual updates; a defensive dev; customer-facing risk), the tone comes off rude/dismissive. The model didn\u2019t sufficiently incorporate channel norms and situational stakes to see that the brusque approval is socially inappropriate rather than just incomplete engagement.",
    "failure_pattern": "overindexing-on-brevity-ignoring-dismissive-tone",
    "severity": "major",
    "id": 383,
    "message": "Yep\u2014go ahead and ship it.",
    "ground_truth_flags": [
      "Rudeness"
    ],
    "predicted_flags": [
      "One-Liner"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: it labeled the target message as \u201cVagueness\u201d and \u201cOne-Liner\u201d even though no flags were expected for this context.",
    "why": "The model treated brevity as inherently problematic and over-weighted the channel norm (\u201cpost only confirmed status + links to evidence\u201d) without considering the immediate conversational state: Maya has already provided detailed QA evidence and an explicit QA clear in prior messages. In that context, \u201cYep\u2014go ahead and ship it.\u201d is a reasonable, unambiguous go-ahead rather than vague. It also misapplied the \u201cOne-Liner\u201d heuristic as a quality violation (shutting down follow-up), when in a go/no-go thread a short, decisive confirmation can be the desired behavior once evidence is already in-thread.",
    "failure_pattern": "overflagging-brevity-with-evidence-in-thread",
    "severity": "minor",
    "id": 384,
    "message": "Yep\u2014go ahead and ship it.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Vagueness",
      "One-Liner"
    ]
  },
  {
    "what_went_wrong": "Missed the expected Passive-Aggressive flag by treating the message as a neutral alignment statement rather than a contextually loaded admonishment.",
    "why": "In isolation, \u201cNoted \u2014 let\u2019s just make sure we\u2019re aligned this time\u201d reads constructive. But in this tense #sales thread\u2014where last week\u2019s slip is explicitly referenced, commitments are screenshot-sensitive, and Raj has just issued a hard \u201cPeriod.\u201d\u2014the phrase \u201cthis time\u201d functions as a pointed reminder/blame cue (implying someone failed to align previously) and \u201cjust\u201d can minimize the complexity while sounding dismissive. The model over-weighted literal wording and under-weighted pragmatic intent and conversational dynamics (power, blame history, and escalation risk), so it failed to detect the indirect reproach characteristic of passive-aggression in formal, high-stakes channels.",
    "failure_pattern": "missing-subtle-passive-aggression",
    "severity": "major",
    "id": 387,
    "message": "Noted \u2014 let\u2019s just make sure we\u2019re aligned this time.",
    "ground_truth_flags": [
      "Passive-Aggressive"
    ],
    "predicted_flags": []
  },
  {
    "what_went_wrong": "Flagged the message as \u201cVagueness\u201d even though no vagueness flag was expected; it treated a brief alignment reminder as a required action-specification.",
    "why": "The model over-weighted a generic writing-quality heuristic (\u201calignment\u201d is unspecific) and under-weighted the thread context: the message functions as a low-risk acknowledgment + caution in a tense, commitment-sensitive #sales thread. In this org/channel, not every utterance must include next steps or JIRA links\u2014especially when prior messages already contain concrete scope/JIRAs and the next-step question (build cutoff time) is already posed. The model conflated \u2018could be more specific\u2019 with \u2018should be flagged as vague,\u2019 effectively penalizing concise, socially-oriented coordination language that is acceptable in-thread.",
    "failure_pattern": "overflagging-concise-coordination-language",
    "severity": "minor",
    "id": 388,
    "message": "Noted \u2014 let\u2019s just make sure we\u2019re aligned this time.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Vagueness"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: added an extra \"One-Liner\" flag even though the expected output was only \"Passive-Aggressive.\"",
    "why": "The model treated message length as a policy-relevant violation on its own. In this context, brevity is common in a fast #general thread and isn\u2019t necessarily problematic unless it creates ambiguity or derails coordination. Here the primary issue is tone (resigned deference that can read as sarcastic/withdrawn given the escalating debate), which the model correctly captured as passive-aggressive. But it overgeneralized a generic heuristic\u2014\"very short reply = shuts down discussion\"\u2014without checking whether PixelCraft\u2019s culture and the immediate thread norms make concise acknowledgements acceptable, and without evidence that the brevity itself is the harmful behavior being flagged.",
    "failure_pattern": "overflagging-brevity-as-misconduct",
    "severity": "minor",
    "id": 389,
    "message": "Got it\u2014do whatever you think is best.",
    "ground_truth_flags": [
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "One-Liner",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: the model labeled a neutral acknowledgment/deference (\u201cGot it\u2014do whatever you think is best.\u201d) as Passive-Aggressive even though, in this context, it doesn\u2019t contain clear veiled hostility, sarcasm, or blame.",
    "why": "The model over-weighted a generic linguistic heuristic that phrases like \u201cwhatever you think is best\u201d can be passive-aggressive, and under-weighted the immediate thread context: a decision has already been made (Elena assigned owners), and the message can reasonably be read as simple alignment/hand-off rather than an attempt to shut down debate. It also failed to distinguish between (a) genuinely passive-aggressive \u2018fine, do what you want\u2019 compliance and (b) normal, time-pressured deference in a fast, candid culture\u2014especially when no additional cues (ellipsis, sarcasm, prior conflict markers, blame) are present in the target message.",
    "failure_pattern": "overflagging-ambiguous-deference",
    "severity": "minor",
    "id": 390,
    "message": "Got it\u2014do whatever you think is best.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed a passive-aggressive flag: the message reads as a terse, potentially dismissive check-in given the high-stakes incident context and preceding requests for specific deliverables.",
    "why": "The model evaluated the sentence in isolation as a neutral acknowledgment, but in this thread the CTO has just assigned concrete, time-bound actions (timeline, customer-safe wording, approval/SOP checks). Responding only with \u201cGot it \u2014 please keep me posted if anything changes\u201d can imply impatience, surveillance, or a brusque handoff (\u201cI\u2019m watching; don\u2019t bother me unless it worsens\u201d) rather than active engagement. In a formal, blame-averse outage process, executives are expected to be precise about next steps, owners, and cadence; this short, non-committal phrasing can be interpreted as pressure or subtle reprimand. The violation is subtle and context-dependent (tone inferred from power dynamics, scrutiny, and expectations of structured comms), which the model underweighted.",
    "failure_pattern": "context-not-considered",
    "severity": "major",
    "id": 391,
    "message": "Got it \u2014 please keep me posted if anything changes.",
    "ground_truth_flags": [
      "Passive-Aggressive"
    ],
    "predicted_flags": []
  },
  {
    "what_went_wrong": "Flagged the message as Passive-Aggressive when it should not have been flagged (false positive).",
    "why": "The model latched onto the lexical cue \"finally\" and applied a generic rule (\"finally\" = veiled complaint) without weighing the conversational context. In this DM, Maya had already flagged the issue promptly and Leo had responded positively; there\u2019s no evidence of delay, blame, or a dig. Given the startup\u2019s norm of blunt, fast Slack DMs, \"Thanks for finally flagging it\" can plausibly be read as informal emphasis/relief (i.e., \"glad this got surfaced\") rather than sarcasm. The model failed to reconcile the phrase with the prior turns (which contradict the implied accusation) and overinterpreted a common intensity marker as passive-aggression.",
    "failure_pattern": "context-not-considered",
    "severity": "major",
    "id": 394,
    "message": "Thanks for finally flagging it.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed a Passive-Aggressive flag: the message reads as a curt, corrective admonishment (\u201cnext time\u201d) rather than collaborative help in a high-stakes escalation thread.",
    "why": "The model evaluated the sentence in isolation and treated it as neutral process guidance. It underweighted the conversational context (Maya is stressed, already being told off for process, reputational stakes are high, and tension is escalating). In that context, \u201cGot it \u2014 please use the on-call queue next time\u201d functions less like support and more like a scolding postscript that dismisses the immediate customer impact. The passive-aggressive signal is subtle: the politeness marker (\u201cplease\u201d) + \u201cgot it\u201d acknowledgment masks a reprimand (\u201cnext time\u201d) that implies blame and shuts down the urgency without offering a path forward (e.g., \u201cfile ticket now and we\u2019ll triage\u201d).",
    "failure_pattern": "context-not-considered-passive-aggression",
    "severity": "major",
    "id": 395,
    "message": "Got it \u2014 please use the on-call queue next time.",
    "ground_truth_flags": [
      "Passive-Aggressive"
    ],
    "predicted_flags": []
  },
  {
    "what_went_wrong": "Flagged an extra category (\"Fake\") that wasn\u2019t warranted; the correct flag was only \"Passive-Aggressive.\"",
    "why": "The model conflated sarcasm/passive-aggression with \u201cfake\u201d sentiment. In this context, \u201cThanks for finally clarifying\u201d clearly signals irritation and a backhanded compliment, but that doesn\u2019t necessarily mean the speaker is performing false sincerity or impersonating emotion (i.e., \u201cfake\u201d). The message can be genuinely appreciative of the clarification while simultaneously criticizing the delay; the \u201csuper helpful\u201d intensifier amplifies the passive-aggressive edge but is not strong evidence of insincerity on its own. The model appears to have treated any sarcastic praise as inherently insincere rather than requiring clearer indicators (e.g., overt contradiction, exaggerated politeness without plausible gratitude, or a pattern of manipulative flattery).",
    "failure_pattern": "overflagging-fake-from-sarcasm",
    "severity": "minor",
    "id": 397,
    "message": "Thanks for finally clarifying \u2014 super helpful.",
    "ground_truth_flags": [
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Fake",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: the model labeled the message as \"Passive-Aggressive\" and \"Fake\" even though, in this context, it is plausibly straightforward gratitude and should have received no flags.",
    "why": "The model over-weighted lexical triggers (\"finally\" + praise) and assumed sarcasm/insincerity without sufficient evidence. It failed to ground the interpretation in the conversation context: a new hire asked process questions, got a direct answer, and a brief thanks is normal. While \"finally\" can be read as a dig, there is no prior conflict, no repeated unanswered asks, and no other sarcasm markers (e.g., exaggeration, emoji, quotes, contrastive framing). The model treated a potentially informal phrasing (slightly tense in a formal/audit channel) as a conduct violation (passive-aggression/fakeness), conflating style mismatch with intent.",
    "failure_pattern": "sarcasm-trigger-overflagging",
    "severity": "minor",
    "id": 398,
    "message": "Thanks for finally clarifying \u2014 super helpful.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Fake",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed a Passive-Aggressive flag: it treated the message as a straightforward thank-you and returned no flags despite the likely sarcastic/pointed subtext given the preceding blame-heavy context.",
    "why": "The target sentence is semantically positive in isolation, but in context it functions as a barbed compliment. Immediately prior, the CTO criticized both the rushed deploy and delayed escalation (\u201cCool.\u201d), creating a tense, reproachful frame. In that frame, \u201cThanks for jumping on this so quickly.\u201d reads as ironic\u2014implicitly highlighting that the response was not quick\u2014rather than sincere gratitude. The model appears to have relied on surface politeness cues (\"thanks\") and did not weight conversational context (recent accusatory tone, outage stakes, timing/delay references), which is where passive-aggression is primarily signaled.",
    "failure_pattern": "missing-subtle-passive-aggression",
    "severity": "major",
    "id": 399,
    "message": "Thanks for jumping on this so quickly.",
    "ground_truth_flags": [
      "Passive-Aggressive"
    ],
    "predicted_flags": []
  },
  {
    "what_went_wrong": "Missed a required flag: it should have flagged both \"Passive-Aggressive\" and \"One-Liner,\" but only predicted \"Passive-Aggressive.\"",
    "why": "The model correctly interpreted the pragmatic risk that a terse acknowledgement can read as grudging compliance in a tense, high-stakes thread. However, it failed to apply (or underweighted) the separate, format-based heuristic that the message is a minimal standalone response with no added substance (i.e., a classic one-liner). This is likely a taxonomy/threshold issue: the model treated the terseness as merely supporting evidence for passive-aggression rather than as an independent trigger for the \"One-Liner\" flag\u2014especially in a strict change-control org where brief confirmations in public channels are both common and audit-relevant, making one-liner detection important regardless of tone interpretation.",
    "failure_pattern": "one-liner-flag-omission",
    "severity": "minor",
    "id": 401,
    "message": "Noted \u2014 let\u2019s go with that.",
    "ground_truth_flags": [
      "One-Liner",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as Passive-Aggressive when it should not have been flagged given the expected outcome.",
    "why": "The model over-weighted the lexical cue \u201cfinally\u201d as an automatic indicator of passive-aggression and treated the message as a veiled complaint without adequately grounding that inference in context. In this thread, Leo explicitly committed to posting metrics \u201cin ~10 minutes\u201d and then delivered them; the target message reads as a brief acknowledgment of receipt rather than an escalation or a public jab. The model also didn\u2019t sufficiently account for the #client-updates / Slack Connect setting where tone sensitivity is high, but that should affect coaching/suggestion, not necessarily trigger a behavioral flag in the absence of clear hostility. Net: a legitimate (if slightly informal) acknowledgment was misclassified as passive-aggressive based on a single weak cue.",
    "failure_pattern": "overflagging-weak-passive-aggression-cue",
    "severity": "minor",
    "id": 404,
    "message": "Thanks for finally posting the numbers.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: it marked the message as Vagueness and Passive-Aggressive even though, in context, the intended meaning is a clear agreement with the already-defined plan; no flags were expected.",
    "why": "The model over-weighted surface cues (quotes around \"fast,\" the casual/curt phrasing) and under-used the immediate conversational context where \"fast\" had been explicitly operationalized (read-only + daily sync, 2-week Phase 1; Phase 2 writebacks later). In this thread, quotes are more likely referencing the previously debated definition of \"fast\" rather than sarcasm. The model also failed to calibrate for a high-stakes, process-heavy channel where informal phrasing is a style issue at most\u2014not inherently passive-aggressive\u2014especially absent any explicit jab, blame, or contrastive wording. In short: it treated contextual shorthand and emphasis as ambiguity/resentment instead of recognizing it as anchored shorthand agreement.",
    "failure_pattern": "context-not-considered-overflagging",
    "severity": "major",
    "id": 406,
    "message": "Totally\u2014let\u2019s just do it the \"fast\" way again.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Vagueness",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged Passive-Aggressive when the message is a neutral, concise request for an update; no flags were warranted.",
    "why": "The model overweighted a single lexical cue (\"actually\") and a short sentence as evidence of sarcasm/doubt, without grounding that inference in stronger indicators (e.g., contrastive phrasing, emotive punctuation, prior accusation) or the immediate context where Leo explicitly committed to finishing shortly. In a fast-moving engineering channel, clipped check-in language is common; \"actually done\" can simply mean \u201cfully merged/tests passing,\u201d not \u201cI don\u2019t believe you.\u201d The model treated a legitimate clarity marker as a tone violation and didn\u2019t calibrate for the channel norm and urgency of shipping before a demo.",
    "failure_pattern": "overflagging-lexical-cue-actually",
    "severity": "minor",
    "id": 408,
    "message": "Cool \u2014 let me know when it's actually done.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed the expected Passive-Aggressive flag: the model predicted no flags even though the message reads as a brush-off in this high-tension context.",
    "why": "The model evaluated the sentence in isolation and treated it as a neutral process request. In-context, however, it functions as a dismissive deflection: after Avery\u2019s urgency and prior conflict about \u2018fire drills\u2019 and \u2018blame threads,\u2019 \u201cGot it \u2014 please file a ticket\u2026 and we\u2019ll take it from there\u201d can imply \u201cwe\u2019re not engaging until you comply,\u201d and \u201cwe\u2019ll take it from there\u201d can sound like shutting down further discussion rather than collaborating. The politeness markers (\u201cGot it,\u201d \u201cplease\u201d) are surface-level and can coexist with passive aggression when the subtext is boundary-setting/pushback in a channel where leadership is watching and time is critical. The model missed these pragmatic cues (power dynamics, prior escalation friction, and implied refusal/stonewalling).",
    "failure_pattern": "context-not-considered",
    "severity": "major",
    "id": 409,
    "message": "Got it \u2014 please file a ticket with the details and we\u2019ll take it from there.",
    "ground_truth_flags": [
      "Passive-Aggressive"
    ],
    "predicted_flags": []
  },
  {
    "what_went_wrong": "Flagged the message as Passive-Aggressive when it should have been unflagged; it treated quotation marks around \u201cquick\u201d as sarcasm/indirect hostility despite insufficient evidence.",
    "why": "The model over-weighted a single stylistic cue (scare quotes) and mapped it to passive-aggression without grounding that interpretation in the surrounding context. In this thread, Marco has already agreed to help and is coordinating details; the quoted \u201cquick\u201d can plausibly function as pragmatic risk-framing/acknowledgment of Priya\u2019s earlier wording rather than a veiled dig. In a late-night urgent incident DM, engineers often use quotes to signal uncertainty about scope or to align on expectations, not necessarily to express contempt. The model failed to apply a higher threshold for tone flags when the utterance remains cooperative and contains no explicit blame, refusal, or sniping.",
    "failure_pattern": "overflagging-ambiguous-scare-quotes",
    "severity": "minor",
    "id": 412,
    "message": "Sure \u2014 since it\u2019s \u201cquick,\u201d I\u2019ll jump on it now.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as Passive-Aggressive when it should have been unflagged; it\u2019s a straightforward deadline/scope reminder in a high-pressure shipping context.",
    "why": "The model over-weighted a common Slack phrase (\u201cPer my last message\u201d) as inherently snarky and treated urgency (\u201cwe really need to stop making late changes\u201d) as indirect hostility. It failed to incorporate the channel context: the team is explicitly discussing a scope/UI freeze, prior incidents from late changes, and a public thread with leadership watching\u2014conditions where crisp, directive reminders are normal and appropriate. In this setting, the wording is more managerial/direct than passive-aggressive, and there\u2019s no clear sarcasm, insinuation, or personal jab at a specific person.",
    "failure_pattern": "overflagging-direct-communication",
    "severity": "minor",
    "id": 414,
    "message": "Per my last message, we really need to stop making late changes if we want to ship by Friday.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged an extra category (\"One-Liner\") that was not expected; the correct flag (\"Passive-Aggressive\") was already identified.",
    "why": "The model treated message brevity as inherently problematic and inferred a separate \"One-Liner\" violation from length alone, rather than using the channel norms and scenario framing. In #engineering at Northlake, concise status-style replies are normal; being short is not itself a communication risk unless it creates ambiguity or fails a required norm. Here the core issue is tone/stance\u2014\"Yep, totally\u2014let\u2019s just ship it\" reads as dismissive in the tense context (minimizing QA concerns), which supports Passive-Aggressive. But adding \"One-Liner\" over-penalizes legitimate concision and double-counts the same underlying problem (shutting down discussion) as a length-based flag.",
    "failure_pattern": "overflagging-brevity",
    "severity": "minor",
    "id": 415,
    "message": "Yep, totally\u2014let\u2019s just ship it.",
    "ground_truth_flags": [
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "One-Liner",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: it marked the message as Passive-Aggressive and One-Liner even though it\u2019s a culturally normal, enthusiastic go-ahead in this channel and does not contain an indirect jab or blame.",
    "why": "The model over-interpreted brevity and the phrase \u201clet\u2019s just ship it\u201d as dismissive/coercive, projecting the broader PM\u2013engineering tension onto a line that, in context, follows an agreed plan (regression checklist, staged deploy, GA verification, rollback). It treated a legitimate, high-tempo \u201cship it\u201d culture cue as a tone violation and used length as a proxy for low-collaboration, ignoring that #engineering norms favor concise status/decisions. There\u2019s no clear passive-aggressive marker (sarcasm, insinuation, veiled criticism, faux-politeness); it\u2019s simply a quick alignment statement.",
    "failure_pattern": "overflagging-direct-communication",
    "severity": "minor",
    "id": 416,
    "message": "Yep, totally\u2014let\u2019s just ship it.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "One-Liner",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: it added a \"Fake\" flag when only \"Passive-Aggressive\" was expected.",
    "why": "The model conflated sarcasm/passive-aggression with \"Fake\" (insincere praise). In this context, \"Love that for us.\" is a common sarcastic idiom expressing frustration about the situation, not a deceptive or socially manipulative attempt to feign agreement/support. The model over-interpreted the phrase as performative endorsement rather than a passive-aggressive jab, and it didn\u2019t distinguish between (a) indirect negativity (passive-aggressive) and (b) counterfeit sincerity meant to mislead (fake).",
    "failure_pattern": "sarcasm-misclassified-as-fake",
    "severity": "minor",
    "id": 417,
    "message": "Love that for us.",
    "ground_truth_flags": [
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Fake",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: it added an extra \"One-Liner\" flag that was not expected. The passive-aggressive flag aligns with the expected label, but the additional style-based flag is a false positive relative to the evaluation standard.",
    "why": "The model treated brevity as inherently policy-relevant (\"shuts down further conversation\") and auto-applied a generic \"One-Liner\" flag without checking whether that category is considered a violation in this org/context or in the expected rubric. In this thread, the brevity is notable, but the evaluators only wanted to capture the interpersonal risk (passive-aggressive, clipped assent amid unresolved compliance/security concerns), not the length itself. The model over-indexed on a surface feature (message length) and conflated \"needs more detail\" with \"should be flagged,\" effectively double-flagging the same underlying issue.",
    "failure_pattern": "overflagging-style-terse-replies",
    "severity": "minor",
    "id": 419,
    "message": "Sure, ship it.",
    "ground_truth_flags": [
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "One-Liner",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: the model marked the message as a \"One-Liner\" when no flag was expected.",
    "why": "The model treated brevity itself as a policy violation and assumed the reply \"shuts down further discussion.\" In this context, the preceding messages already documented CI status, security checks, test coverage, rollback notes, release timing, and multiple peer approvals. Given #engineering\u2019s auditability norms, the short message functions as a clear approval/decision point (\u201cgo ahead and merge/ship\u201d) rather than an unhelpful non-response. The model over-applied a generic heuristic (\u201cshort = low engagement\u201d) and didn\u2019t weigh that (a) the thread already contains the required supporting detail, and (b) a concise explicit go/no-go is often appropriate under time pressure once checks are complete.",
    "failure_pattern": "overflagging-brief-approval-in-decision-context",
    "severity": "minor",
    "id": 420,
    "message": "Sure, ship it.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "One-Liner"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: the model labeled the message as Passive-Aggressive even though the expected output was no flags.",
    "why": "The model treated a culturally common, informal venting phrase (\u201csince we\u2019re apparently doing this again\u201d) as a policy-relevant passive-aggressive jab. In this startup #general context\u2014high-pressure, rapid-fire, low-formality\u2014mild sarcasm/grumbling while still committing to the task can be a normal style signal rather than a problematic interpersonal attack. The message doesn\u2019t target a specific person, doesn\u2019t refuse the request, and still clearly accepts ownership; it\u2019s closer to stress/frustration shorthand than an indirect attempt to undermine someone. The model over-weighted a lexical cue (\u201capparently\u201d) and ignored the pragmatic intent and norms of the channel (quick, no-frills replies during an incident/demo crunch), where slight snark is often tolerated and not necessarily harmful enough to flag.",
    "failure_pattern": "cultural-style-misclassified",
    "severity": "minor",
    "id": 422,
    "message": "Sure \u2014 since we\u2019re apparently doing this again, I\u2019ll take it.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as Passive-Aggressive even though it\u2019s a plausible, non-hostile clarification of ownership in a fast-moving incident thread; no flag should have been raised.",
    "why": "The model over-weighted the phrase \u201clike I said earlier\u201d as inherently a rebuke, without grounding it in the operational context (urgent outage response, threaded coordination, need to re-assert task ownership quickly). In this setting, brief reiterations are common and functional (avoiding duplicated work, confirming who owns what) and blunt phrasing is culturally normal. The model treated a legitimate, time-pressured style cue as interpersonal negativity, rather than checking for additional markers of contempt (sarcasm, ridicule, repeated needling, blame attribution) that aren\u2019t present here.",
    "failure_pattern": "overflagging-ambiguous-phrase",
    "severity": "minor",
    "id": 426,
    "message": "Sure \u2014 like I said earlier, I'll take care of it.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: the model marked the message as Passive-Aggressive and Fake even though the expected result was no flags.",
    "why": "The model treated a common, informal Slack quip as inherently problematic sarcasm and assumed insincere praise equates to 'Fake.' It relied on lexical cues ('Awesome', 'love') to infer veiled hostility without adequately weighting the channel norms (fast-paced, joking culture) and the conversational function (venting/pressure acknowledgment). It also appears to conflate 'sarcastic/frustrated' with policy-level passive-aggression and interprets 'performative praise' as deception, even though 'Fake' typically implies inauthenticity/manipulation rather than a brief ironic remark. In this context, the line is more a stressy joke than an indirect attack on a person, and it doesn\u2019t contain a clear target, threat, or blame\u2014so the stricter flags are an overreach.",
    "failure_pattern": "cultural-style-misclassified",
    "severity": "major",
    "id": 428,
    "message": "Awesome \u2014 love finding this out the night before the demo.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Fake",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as Passive-Aggressive when it should not have been flagged (false positive).",
    "why": "The model overweighted two lexical cues (\u201cdrop everything\u201d and \u201cagain\u201d) and treated them as inherently passive-aggressive, without grounding that interpretation in the actual conversational dynamics. In this thread, multiple people are already explicitly reprioritizing (pause launch, pull dashboards, take comms), so \u201cdrop everything and join\u201d is consistent with the urgent incident-response norm rather than an indirect jab. The word \u201cagain\u201d can signal frustration, but it can also be a factual reference to repeated incidents; the message lacks other passive-aggressive markers (sarcasm, refusal-by-compliance, pointed blame, or antagonistic subtext directed at a person). The model essentially inferred intent (resentment) from ambiguous phrasing instead of requiring clearer evidence of indirect hostility, leading to over-sensitive flagging in a high-pressure context where brevity is common.",
    "failure_pattern": "overflagging-ambiguous-frustration-cue",
    "severity": "minor",
    "id": 432,
    "message": "Got it \u2014 I'll drop everything again and join.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "The model flagged \"One-Liner\" instead of the expected \"Non-Objective\" (missed expected flag and added an incorrect one relative to the spec).",
    "why": "It over-weighted message length as a primary violation and treated brevity as inherently problematic, despite the channel norm being concise, paste-ready updates. The key issue is evidentiary: \"Yep, totally safe for the 9am demo\" asserts safety without any objective support and contradicts the immediately preceding, well-documented regression\u2014this maps to \"Non-Objective\" (unsupported reassurance) more than a generic \"One-Liner\". In other words, the model recognized dismissive tone (Passive-Aggressive) but failed to translate the core defect into the correct category because it under-modeled the channel\u2019s expectation for concise status notes and misapplied a style-based flag instead of a content/grounding-based one.",
    "failure_pattern": "ignoring-channel-norms-and-mislabeling-as-one-liner",
    "severity": "major",
    "id": 433,
    "message": "Yep, totally safe for the 9am demo.",
    "ground_truth_flags": [
      "Non-Objective",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "One-Liner",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as Non-Objective when it should have been unflagged; no actual policy-relevant subjectivity/claim issue in context.",
    "why": "The model treated the phrase \u201ctotally safe\u201d as an unjustified factual claim in isolation, instead of interpreting it as standard go/no-go shorthand in an internal status channel. It ignored the preceding evidence Mina already provided (smoke tests across key paths, cache issue resolved, guardrail added, orders completing) and the channel norm (\u201cconcise, action-oriented\u201d updates for account managers). In this context, the message is a concise confirmation of readiness rather than an attempt to present opinion as fact; the level of certainty is a conventional, culturally expected style for shipping/demos.",
    "failure_pattern": "overflagging-status-assertions",
    "severity": "minor",
    "id": 434,
    "message": "Yep, totally safe for the 9am demo.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Non-Objective"
    ]
  },
  {
    "what_went_wrong": "Missed the expected \"Passive-Aggressive\" flag and instead flagged \"Vagueness\" based on lack of concrete commitments.",
    "why": "The model treated the message as a purely informational commitment-quality issue (non-specific follow-through) and underweighted the interpersonal signal in context: after being asked to explicitly acknowledge harm and commit to three concrete behavior changes, Jonah responds with a terse, generic closure (\"Noted\") that can read as dismissive/minimizing. In this DM feedback setting, that brevity functions less like neutral vagueness and more like passive resistance\u2014appearing compliant while implicitly pushing back or avoiding accountability. The model likely relied on surface features (short, non-specific) and a general rubric that maps non-specific promises to \"Vagueness,\" without incorporating the power/feedback dynamic and prior request for explicit acknowledgment that makes the tone feel passive-aggressive.",
    "failure_pattern": "missing-subtle-passive-aggression",
    "severity": "major",
    "id": 435,
    "message": "Noted \u2014 I\u2019ll make sure it doesn\u2019t happen again.",
    "ground_truth_flags": [
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Vagueness"
    ]
  },
  {
    "what_went_wrong": "Missed a required flag: the message should have been flagged as both Non-Objective and Passive-Aggressive, but the model only flagged Passive-Aggressive.",
    "why": "The model correctly detected the sarcastic, resentful subtext (passive-aggression) but failed to also recognize that the content is not an objective, actionable statement. \"Must be nice\" is an evaluative, subjective jab that implies unfairness/exclusion without stating concrete facts, a request, or a clear problem statement. The model likely treated the presence of a specific reference (\"the call this morning\") as enough 'objectivity' and over-focused on tone-based detection, overlooking the separate criterion that non-objective language includes implied judgments and vague insinuations.",
    "failure_pattern": "missed-non-objective-subjective-jab",
    "severity": "major",
    "id": 439,
    "message": "Must be nice to hear about this on the call this morning.",
    "ground_truth_flags": [
      "Non-Objective",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as Passive-Aggressive when it should not have been flagged (expected no flags).",
    "why": "The model over-indexed on the phrase \"Must be nice...\" as a universal sarcasm marker and treated it as an indirect jab, without sufficiently weighing the immediate thread context: Jules explicitly addressed Maya, acknowledged the impact on her renewals, and offered to DM details; Priya apologized for the last\u2011minute invite list and committed to documenting soon. In this setting (#random with joking tone), the line can plausibly function as lightweight, socially acceptable venting/banter about async transparency gaps rather than a targeted, escalating dig. The model also failed to apply the platform\u2019s labeling threshold: mild snark that doesn\u2019t threaten, harass, or derail and is already being constructively handled in-thread is often not actionable enough to merit a \u201cPassive-Aggressive\u201d flag.",
    "failure_pattern": "overflagging-low-grade-snark",
    "severity": "minor",
    "id": 440,
    "message": "Must be nice to hear about this on the call this morning.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: the model labeled the message as Passive-Aggressive even though no flag was expected.",
    "why": "The model over-indexed on the token \"finally\" as a universal marker of passive-aggression and treated it as a veiled insult without adequately weighing situational context. In this thread, multiple people were providing clarifications to a new employee; \"Thanks for finally clarifying\" can plausibly be literal (the clarification just arrived) rather than a dig. The model also didn\u2019t distinguish between (a) a mild, potentially suboptimal phrasing for a formal #general channel and (b) an actual tone violation. In ApexSure\u2019s culture, the bigger risk is process/sensitivity and channel appropriateness, not interpersonal hostility\u2014so elevating this to a tone flag is an overreaction driven by lexical heuristics rather than intent inference.",
    "failure_pattern": "overflagging-lexical-trigger",
    "severity": "minor",
    "id": 442,
    "message": "Thanks for finally clarifying \u2014 I\u2019ll follow that process.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as Passive-Aggressive even though no flags were expected for this specific utterance in context.",
    "why": "The model treated a common sarcastic idiom (\u201cWhat could possibly go wrong?\u201d) as inherently passive-aggressive, without weighing whether it constitutes a policy-relevant interpersonal violation in this setting. It relied on surface cues (agreement + rhetorical question) and did not sufficiently incorporate the immediate context where the team is already explicitly discussing risk/contingencies and has moved the conversation out of #random; in that context, the line reads more like light gallows humor/acknowledgment of risk than an indirect personal jab. The model also appears to lack a threshold for when sarcasm is merely stylistic vs. when it becomes passive-aggressive (e.g., aimed at a person, blaming, or undermining a decision-maker).",
    "failure_pattern": "overflagging-sarcastic-idiom",
    "severity": "minor",
    "id": 444,
    "message": "Sure \u2014 let's ship Friday. What could possibly go wrong?",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: the model labeled the message as Passive-Aggressive even though it\u2019s a common, low-stakes vent/ironic aside in a blunt, high-tempo startup Slack context and not a targeted veiled dig at a specific person in-channel.",
    "why": "The model over-weighted lexical sarcasm cues (\u201cCool \u2014 love being\u2026 again\u201d) without calibrating to channel norms (#sales as a war-room, emoji-heavy/blunt bursts) and the situational stress (incident fallout). It treated irony as inherently passive-aggressive, but in this context it functions as shared frustration about the situation (having to repeat an explanation to a customer) rather than an indirect attack or manipulative complaint toward a colleague. It also didn\u2019t differentiate between \u2018venting about external-facing burden\u2019 vs \u2018sniping at someone internally,\u2019 which is key for the Passive-Aggressive label.",
    "failure_pattern": "cultural-style-misclassified",
    "severity": "minor",
    "id": 446,
    "message": "Cool \u2014 love being the one to tell them that again.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as Passive-Aggressive when it is a mild, process-enforcing reminder consistent with the channel norms; no flags were warranted.",
    "why": "The model over-weighted the hedging phrase \u201cmaybe next time\u201d as inherently passive-aggressive and treated indirectness as hostility. It did not sufficiently incorporate the organizational/channel context: #client-updates is explicitly formal, compliance-oriented, and already contains repeated instructions to use ticket IDs and client-approved wording. In that setting, the message reads as routine corrective feedback (even if slightly informal) rather than a veiled dig, threat, or resentful jab. The model also appears to apply a generic heuristic (\u201csoftened correction = passive aggression\u201d) without checking for other passive-aggression cues (sarcasm, ridicule, insinuation, withholding, or punitive framing), which are absent here.",
    "failure_pattern": "overflagging-process-reminders-as-passive-aggressive",
    "severity": "minor",
    "id": 448,
    "message": "Thanks for the update\u2014maybe next time include the ticket ID and client-approved wording.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: added an extra \"One-Liner\" flag when only \"Passive-Aggressive\" was expected.",
    "why": "The model treated brevity as inherently problematic and assumed the message \"shuts down\" discussion, rather than recognizing that in this Slack context a short reply can be normal and not necessarily a separate communicative risk category. The core issue is tone (the performative \"Cool\" + \"just ship it\" reads as dismissive/sarcastic given the prior safety/testing objections), which is already captured by Passive-Aggressive. The additional One-Liner flag is redundant and likely triggered by a heuristic (very short message + high-stakes decision) without calibrating to the taxonomy/threshold for when brevity alone merits a flag.",
    "failure_pattern": "overflagging-brevity",
    "severity": "minor",
    "id": 449,
    "message": "Cool, let\u2019s just ship it Friday then.",
    "ground_truth_flags": [
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "One-Liner",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: applied Passive-Aggressive and One-Liner when the message is a benign, colloquial agreement/summary in a fast-moving Slack channel; no flags were warranted.",
    "why": "The model over-interpreted terse startup Slack phrasing (\"Cool\" + \"just ship it\") as dismissive/shutdown language without sufficient evidence of sarcasm, resentment, or a jab. It also ignored the immediately preceding context showing alignment on a staged rollout with explicit steps (soak test, go/no-go, ramp schedule), making the target message plausibly a shorthand confirmation of that plan rather than an attempt to bypass it. In this channel\u2019s performative, rapid norm, brief acknowledgments are common and not inherently problematic; the model treated brevity as a conversational violation instead of a culturally appropriate acknowledgment.",
    "failure_pattern": "cultural-style-misclassified",
    "severity": "major",
    "id": 450,
    "message": "Cool, let\u2019s just ship it Friday then.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "One-Liner",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as Passive-Aggressive when it should not have been flagged.",
    "why": "The model treated meme-y phrasing (\u201caccessibility DLC\u201d), casual markers (\u201clol\u201d), and a light emoji as inherently sarcastic/hostile without adequately weighing the channel and company norms (casual, meme-heavy #random) and the pragmatic communicative function (a clarifying critique aligned with the already-open disagreement about accessibility). In this context, the message reads as culturally typical shorthand for \u201care we really deferring accessibility?\u201d rather than a veiled personal dig. The model also over-indexed on surface cues (quotes, laughter tokens) and under-considered that the remark targets a decision, not a person, and is consistent with the existing tone in-thread (e.g., Jordan\u2019s joking \u2018polish after\u2019 comment).",
    "failure_pattern": "cultural-style-misclassified",
    "severity": "minor",
    "id": 451,
    "message": "lol so we\u2019re shipping the \u201caccessibility DLC\u201d after the investor demo? \ud83d\ude05",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed two expected flags: it only flagged Passive-Aggressive, but failed to flag Vagueness and Non-Objective in a client-updates context where the message is an unclear, joking rhetorical question rather than a concrete status statement.",
    "why": "The model focused narrowly on the most salient surface cue (sarcasm via 'accessibility DLC', 'lol', and the \ud83d\ude05 emoji) and treated the task as primarily tone detection. It didn\u2019t sufficiently incorporate channel norms and the preceding prompt from Ava to keep updates specific, nor Noah\u2019s explicit request to confirm accessibility/compliance status. In that context, the message is (a) non-objective because it\u2019s framed as a joke/metaphor rather than a factual claim or direct question, and (b) vague because it doesn\u2019t specify what accessibility work is deferred, what the current status is, or what the plan/timeline is\u2014so it fails the \u201cincluded/deferred/risk/mitigation\u201d standard for #client-updates.",
    "failure_pattern": "ignoring-channel-context",
    "severity": "major",
    "id": 452,
    "message": "lol so we\u2019re shipping the \u201caccessibility DLC\u201d after the investor demo? \ud83d\ude05",
    "ground_truth_flags": [
      "Vagueness",
      "Non-Objective",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: the model labeled the DM as \"Rudeness\" and \"Passive-Aggressive\" even though the expected behavior for this team/channel context treats sarcasm and blunt ownership-tradeoffs as normal, not abusive or hostile.",
    "why": "The model over-weighted surface cues (sarcasm marker \"Cool cool\", the \ud83d\ude43 emoji, and the phrase \"babysit it on-call\") and applied a generic politeness standard. It under-used the provided context that (a) this is a small startup with punchy, sarcastic Slack norms, (b) the exchange is a private DM between peers who usually collaborate well, and (c) they\u2019re actively negotiating risk/ownership under time pressure. In that environment, the message reads as stressed-but-standard candid negotiation (\"simplest thing\" + \"if you prefer queue, you own on-call\"), not a personal attack. The model also treated an on-call ownership condition as a demeaning threat rather than a common engineering accountability mechanism used to align incentives.",
    "failure_pattern": "cultural-style-misclassified",
    "severity": "major",
    "id": 453,
    "message": "Cool cool \u2014 let\u2019s just ship the simplest thing so we don\u2019t relive last week\u2019s \u201coverengineering\u201d murder mystery \ud83d\ude43. Mateo, if you want the queue so bad you can babysit it on-call.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Rudeness",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as Passive-Aggressive when it should not have been flagged (false positive).",
    "why": "The model overweighted surface cues (\"Lol\", \"with popcorn\") and interpreted them as a veiled threat, while underweighting the actual communicative intent and the channel context. In this DM between peers with established candid rapport, the line reads as informal venting and emphasis for risk-avoidance (\u201cdon\u2019t overcommit again\u201d), not as indirect hostility toward the recipient. The \u2018forwarding to Sales Ops\u2019 remark is framed as hyperbole about escalation risk, not an attempt to shame/manipulate the other person. In other words, the model treated sarcastic humor as inherently passive-aggressive instead of evaluating whether the message is covertly attacking the interlocutor versus expressing alignment and urgency (\u201cLet\u2019s stick to the connector.\u201d).",
    "failure_pattern": "sarcasm-hyperbole-overflagged",
    "severity": "minor",
    "id": 455,
    "message": "Lol if we promise a \u201crapid custom API integration\u201d again, I\u2019m just gonna start forwarding these threads to Sales Ops with popcorn. Let\u2019s stick to the connector.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed a required flag: the message is not only passive-aggressive but also rude (mocking tone, public shaming/threatening escalation in a monitored channel).",
    "why": "The model correctly detected sarcasm/veiled threat and mapped it to Passive-Aggressive, but it likely treated the 'Lol/popcorn' framing as mitigating humor rather than as derision. It also underweighted the enterprise/channel context: #cross-functional is closely watched, the org expects polished playbook-aligned language, and referencing forwarding threads to Sales Ops functions as a public intimidation/shaming move. Those factors elevate the behavior from merely indirect frustration to overt disrespect (Rudeness) toward peers and the process.",
    "failure_pattern": "rudeness-underflagged-when-framed-as-humor",
    "severity": "major",
    "id": 456,
    "message": "Lol if we promise a \u201crapid custom API integration\u201d again, I\u2019m just gonna start forwarding these threads to Sales Ops with popcorn. Let\u2019s stick to the connector.",
    "ground_truth_flags": [
      "Rudeness",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly (overflagged): it marked the DM as \"Non-Objective\" and \"Passive-Aggressive\" even though the expected outcome was no flags.",
    "why": "The model treated informal, rapport-preserving startup banter as interpersonal hostility and treated a reasonably grounded causal suspicion as an \"assumption.\" In context, Maya already provided concrete evidence (same signature as last week, timing right after the refactor merge, Sentry event, HAR, promo_total null) and Alex acknowledged he changed the relevant code path\u2014so \"I think\" + joking phrasing functions as softening/relationship management, not a veiled jab. It also ignored channel norms (informal, fast, emoji-light) and that this is a private DM (lower reputational threat than a public call-out). The quotes around \"cleanup\" and \"lol\" were read as mockery, but here they more plausibly signal levity under pressure and a request to handle it quietly (\"before we wake up the whole zoo\").",
    "failure_pattern": "cultural-style-misclassified",
    "severity": "minor",
    "id": 457,
    "message": "lol Alex I think your midnight \"cleanup\" resurrected the checkout bug \ud83d\ude05 can you take a look before we wake up the whole zoo?",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Non-Objective",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: the model added a \"Non-Objective\" flag that wasn\u2019t expected; it correctly flagged \"Passive-Aggressive\".",
    "why": "The model treated Maya\u2019s causal suspicion (the bug reappeared after Alex\u2019s midnight refactor) as an evidence-free factual claim and mapped that to \"Non-Objective.\" In this context, though, the message is a common, informal engineering shorthand for a time-correlated regression (\u201cI think\u2026 resurrected\u2026\u201d) rather than an attempt to misstate facts. The hedge (\u201cI think\u201d) and the fast-moving startup Slack norm reduce the likelihood that this should be classified as non-objective/unsupported\u2014it's more a tone issue (sarcastic blame) than an objectivity violation. The model likely over-weighted blame attribution language (\"your midnight 'cleanup'\") and under-weighted the explicit uncertainty marker and channel norms.",
    "failure_pattern": "overflagging-non-objective-from-hedged-suspicion",
    "severity": "minor",
    "id": 458,
    "message": "lol Alex I think your midnight \"cleanup\" resurrected the checkout bug \ud83d\ude05 can you take a look before we wake up the whole zoo?",
    "ground_truth_flags": [
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Non-Objective",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged \"Vagueness\" incorrectly: in the provided chat context, \"this\" clearly refers to the active prod incident/outage Ethan mentioned, and the message contains concrete, actionable assignments (Sam: logs/rollback; Priya: DB). No flags were expected.",
    "why": "The model applied a generic heuristic (penalizing deictic terms like \"this\" and figurative language like \"stop the bleeding\") without leveraging the immediately preceding context that establishes the incident. It also treated the message as needing formal, standalone completeness rather than as an in-thread, real-time coordination note where shared context is assumed. In short, it over-weighted literal specificity and under-weighted conversational grounding and explicit task delegation already present.",
    "failure_pattern": "context-not-considered-overflagging-vagueness",
    "severity": "minor",
    "id": 459,
    "message": "Can someone please just jump on this and stop the bleeding? Sam grab logs + rollback if needed, Priya dig into the DB side \u2014 and let\u2019s keep the extra \"process\" stuff to a minimum until it\u2019s stable.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Vagueness"
    ]
  },
  {
    "what_went_wrong": "Flagged the wrong tone category: it labeled the message as \"Passive-Aggressive\" when the expected issue is straightforward \"Rudeness\"; it did not miss \"Vagueness\" but misclassified the interpersonal violation.",
    "why": "The model anchored on the quoted word \u201cprocess\u201d and inferred indirect/team-directed snark (passive-aggression). In this context (#client-updates with strict, formal, approved wording), the problem is more direct: \"just jump on this,\" \"stop the bleeding,\" and \"keep the extra process stuff to a minimum\" read as brusque/dismissive and openly undermining required procedure, especially with Sales/Client Success observing. That makes the tone primarily rude/combative rather than subtly passive-aggressive. The model also underweighted the channel norms and high-stakes audience, where even mild dismissiveness is treated as explicit rudeness and escalation risk.",
    "failure_pattern": "tone-misclassification-passive-aggressive-vs-rudeness",
    "severity": "major",
    "id": 460,
    "message": "Can someone please just jump on this and stop the bleeding? Sam grab logs + rollback if needed, Priya dig into the DB side \u2014 and let\u2019s keep the extra \"process\" stuff to a minimum until it\u2019s stable.",
    "ground_truth_flags": [
      "Vagueness",
      "Rudeness"
    ],
    "predicted_flags": [
      "Vagueness",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: it labeled the message as Passive-Aggressive even though the expected flags were none for this channel/context.",
    "why": "The model over-weighted surface cues (sarcastic phrasing like \u201cduct-tape,\u201d \u201ctweet \u2018saved prod\u2019,\u201d and the \ud83d\ude43 emoji) and treated them as inherently passive-aggressive, without calibrating to the established tone of #random (banter, memes, playful sarcasm) and the immediately preceding messages that are already jokingly critical (\u201cmuseum exhibit,\u201d \u201coutage bingo,\u201d \u201cstandup comedy\u201d). In this setting, the message reads as direct skepticism about a quick fix expressed in the channel\u2019s normal comedic register\u2014not an indirect, covert jab at a person. The critique is about the approach (hotfix vs refactor), and it\u2019s aligned with ongoing thread humor; the model conflated \u2018sarcasm\u2019 with \u2018passive-aggression\u2019 and didn\u2019t apply a context-sensitive threshold for flagging.",
    "failure_pattern": "cultural-style-misclassified",
    "severity": "minor",
    "id": 461,
    "message": "Yeah sure, let's duct-tape Redis onto it so we can all tweet \"saved prod\"\u2014future-us can clean up the mess at 3am again \ud83d\ude43",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Both: it missed the expected flags \"Rudeness\" and \"Non-Objective\", and it incorrectly added \"Fake\".",
    "why": "The message is clearly sarcastic and contemptuous toward Jordan\u2019s proposal (\"duct-tape Redis\", \"tweet 'saved prod'\", \"clean up the mess at 3am again\"), which is rudeness in tone and also non-objective because it replaces concrete risk/impact reasoning with mockery and motive-attribution. The model latched onto the joking/performative framing and mapped it to \"Fake\" (insincerity), but the speaker isn\u2019t pretending agreement in a deceptive way; they\u2019re using overt sarcasm to criticize. In short, the model over-indexed on 'performative humor' and treated sarcasm-as-insincerity, while under-weighting the interpersonal disrespect and the lack of actionable, evidence-based content\u2014especially given the channel request to stay focused on actionable steps.",
    "failure_pattern": "sarcasm-misclassified-as-fake-and-missed-rudeness-nonobjective",
    "severity": "major",
    "id": 462,
    "message": "Yeah sure, let's duct-tape Redis onto it so we can all tweet \"saved prod\"\u2014future-us can clean up the mess at 3am again \ud83d\ude43",
    "ground_truth_flags": [
      "Rudeness",
      "Non-Objective",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Fake",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as Passive-Aggressive when, per the expected labels, it should not have been flagged at all (overflagging).",
    "why": "The model treated rhetorical sarcasm (\u201cYOLO\u201d, \u201cBold choice \ud83d\ude43\u201d) as inherently passive-aggressive, without weighing the org\u2019s fast-paced, blunt Slack style and the fact this is a DM between peers (lower public-face-threat). It also over-indexed on surface markers (slang + emoji) rather than the message\u2019s functional intent: it\u2019s a pointed, explicit disagreement about risk/approach, not an indirect or covert dig. In this culture, such snark may be common shorthand for urgency/frustration and not reliably predictive of harmful passive-aggression; the model lacked calibration to the company\u2019s informal norms and the private channel context.",
    "failure_pattern": "cultural-style-misclassified",
    "severity": "minor",
    "id": 463,
    "message": "Cool, so we\u2019re gonna YOLO a refactor in front of the client instead of shipping the safe flag? Bold choice \ud83d\ude43",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed the expected \"Rudeness\" flag (under-flagged).",
    "why": "The model correctly caught sarcasm/passive-aggression and the non-evidence-based framing, but it failed to map the same linguistic cues to explicit rudeness. Terms like \"YOLO\" (implying recklessness), \"instead of shipping the safe flag?\" (publicly framing the other option as obviously irresponsible), and \"Bold choice \ud83d\ude43\" (mocking/snide) are not just indirect criticism\u2014they\u2019re disrespectful in tone, especially in a high-stakes, CEO-watched channel. The model likely treated rudeness as requiring overt insults/profanity and didn\u2019t account for how sarcastic mockery functions as rudeness in this context (public thread, client pressure, face-threatening).",
    "failure_pattern": "underflagging-rudeness-in-sarcasm",
    "severity": "major",
    "id": 464,
    "message": "Cool, so we\u2019re gonna YOLO a refactor in front of the client instead of shipping the safe flag? Bold choice \ud83d\ude43",
    "ground_truth_flags": [
      "Rudeness",
      "Non-Objective",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Non-Objective",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly (over-flagged). The message was expected to be unflagged, but the model labeled it \"Passive-Aggressive\" and \"Non-Objective\" even though it is primarily a direct, process-enforcing instruction in a high-stakes operational context.",
    "why": "The model treated blunt, corrective language as interpersonal toxicity. Phrases like \u201cclassic Sales drive-by escalation\u201d and the exaggerated \u201cthey\u2019ll churn!!!\u201d were interpreted as passive-aggressive sarcasm rather than contextually normal frustration/triage guidance in an enterprise incident-management setting. It also misapplied \"Non-Objective\" by equating evaluative phrasing (\u201cisn\u2019t a plan\u201d) with lack of evidence, despite the message containing concrete, actionable steps (file an incident, page on-call, include details) and responding to an explicit off-process escalation in a monitored channel. The model likely over-weighted tone cues and under-weighted the channel/process context where enforcing escalation protocol is expected and not inherently inappropriate.",
    "failure_pattern": "overflagging-direct-communication",
    "severity": "major",
    "id": 465,
    "message": "Can we please not do the classic Sales drive-by escalation in here? File the incident + page on-call like the process says, and include *actual* details (customer/env/error/timestamps). Tagging random people with \u201cthey\u2019ll churn!!!\u201d isn\u2019t a plan.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Non-Objective",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed an expected flag: the model correctly flagged Rudeness but failed to also flag Passive-Aggressive tone.",
    "why": "The message contains clear passive-aggressive markers layered on top of direct rudeness: rhetorical framing (\u201cCan we please not\u2026\u201d), a stereotyped jab (\u201cclassic Sales drive-by escalation\u201d), scare-quote mockery (\u201cthey\u2019ll churn!!!\u201d), and a belittling contrast (\u201cisn\u2019t a plan\u201d). The model appears to treat these as only general confrontational language and collapse them into a single \u2018Rudeness\u2019 category, rather than recognizing the indirect, sarcastic, and performatively polite phrasing that constitutes passive-aggression\u2014especially in this org context where public callouts and screenshots heighten the signaling/performative aspect.",
    "failure_pattern": "missing-subtle-passive-aggression",
    "severity": "major",
    "id": 466,
    "message": "Can we please not do the classic Sales drive-by escalation in here? File the incident + page on-call like the process says, and include *actual* details (customer/env/error/timestamps). Tagging random people with \u201cthey\u2019ll churn!!!\u201d isn\u2019t a plan.",
    "ground_truth_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ],
    "predicted_flags": [
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Missed two applicable flags: (1) Rudeness via dismissive, blunt phrasing (\u201cCool cool,\u201d \u201cfaceplanted,\u201d \u201cmagic \u2026 stuff\u201d) in a high-stakes cross-functional channel; and (2) Vagueness because it provides no concrete, approved client-facing wording or boundaries beyond a negative instruction, despite an explicit request for aligned commitments and language.",
    "why": "The model treated the message as harmless self-deprecation and a clear directive, but it failed to account for (a) pragmatic tone in context: during an incident with existing sales\u2013engineering friction, casual sarcasm/derision can read as minimizing and scolding, which fits Rudeness even without insults; and (b) the conversational goal: stakeholders asked for specific, consistent commitments (SLA/recurrence wording). Saying \u201cit\u2019s fixed\u201d plus \u201cdon\u2019t promise X\u201d leaves key questions unanswered (what to say instead, what timeframe, what\u2019s known/unknown), which is Vagueness in this setting. In other words, it evaluated literal clarity of one sentence rather than adequacy/precision relative to the requested guidance and the channel\u2019s purpose (client updates alignment).",
    "failure_pattern": "context-not-considered",
    "severity": "major",
    "id": 468,
    "message": "Cool cool \u2014 so we basically faceplanted for an hour \ud83d\ude2c. It\u2019s fixed now; please don\u2019t promise 100% uptime or any magic \"won\u2019t happen again\" stuff.",
    "ground_truth_flags": [
      "Vagueness",
      "Rudeness"
    ],
    "predicted_flags": []
  },
  {
    "what_went_wrong": "Flagged incorrectly: the message was labeled as \u201cVagueness\u201d even though, in the #general coffee/welcome thread, the level of specificity is normal and the ask is understandable to the tagged recipients. It also likely missed the more relevant concern (channel/behavioral fit): a synchronous, interruptive request with multiple tags, but that wasn\u2019t part of the expected flags set.",
    "why": "The model latched onto surface-level vague phrases (\u201cthis thing\u201d, \u201cwhich flags\u201d) without incorporating channel context (#general banter) or role context (new hire asking for help). It treated stylistic informality as actionable vagueness, ignoring that the tagged seniors likely share the necessary shared context (payments API rollout/deploy). In other words, it applied a generic clarity heuristic rather than evaluating whether the message actually creates ambiguity or harm in this specific setting. Additionally, it didn\u2019t align with the evaluation rubric implied by the expected output (no flags), suggesting thresholding/calibration error for what counts as flag-worthy vagueness.",
    "failure_pattern": "overflagging-vagueness-ignoring-context",
    "severity": "minor",
    "id": 469,
    "message": "Hey @Maya @Ethan \u2014 sorry to spam \ud83d\ude05 can one of you just hop on for 5 mins and tell me how we deploy this thing + which flags I need to flip? I\u2019m kinda lost and want to make sure I don\u2019t break prod.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Vagueness"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: it marked the message as Passive-Aggressive and Rudeness even though the expected outcome was no flags.",
    "why": "The model over-indexed on a couple of strong-sounding phrases (\"just\" and \"stop bikeshedding\") and treated them as inherently rude/passive-aggressive without calibrating to the company\u2019s friendly-but-fast, execution-focused communication style in #general. In this context, the message is a direct request for a decision under time pressure; the bluntness is situational rather than an interpersonal attack. It also didn\u2019t sufficiently account for norms described in the scenario (async, threads, avoid after-hours tagging) and instead latched onto tone words\u2014so it flagged tone categories rather than the more context-relevant issue (potentially policy/norms around channel choice and after-hours pressure), which this flag set doesn\u2019t capture. Net: legitimate directness + urgency was misclassified as incivility.",
    "failure_pattern": "overflagging-direct-communication",
    "severity": "major",
    "id": 471,
    "message": "Can we just ship Friday and stop bikeshedding? @Marta @Ravi @Neha I need a hard yes/no tonight so I can tell the CEO.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Missed an expected flag: Passive-Aggressive. The model correctly flagged Vagueness and Rudeness but failed to identify the passive-aggressive framing in \u201cCan we just ship Friday and stop bikeshedding?\u201d and the pressure-laden demand for a \u201chard yes/no tonight.\u201d",
    "why": "The model likely treated \u201cstop bikeshedding\u201d as purely rude/dismissive rather than also a classic passive-aggressive move: it implies others are wasting time on trivialities and obstructing progress, while indirectly blaming them without explicitly stating the accusation. It also underweighted the situational context (prior postmortem blame, ongoing CI flakiness, burnout warnings, and the social pressure of asking in a highly visible setting while tagging individuals) that amplifies the passive-aggressive subtext: leveraging urgency and audience pressure to corner specific people into compliance. In short, it recognized overt abrasiveness but didn\u2019t map the insinuation + coercive urgency pattern to the Passive-Aggressive label.",
    "failure_pattern": "missing-subtle-passive-aggression",
    "severity": "major",
    "id": 472,
    "message": "Can we just ship Friday and stop bikeshedding? @Marta @Ravi @Neha I need a hard yes/no tonight so I can tell the CEO.",
    "ground_truth_flags": [
      "Vagueness",
      "Passive-Aggressive",
      "Rudeness"
    ],
    "predicted_flags": [
      "Vagueness",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: the model marked the message as Passive-Aggressive and Non-Objective even though the expected flags were none for this #random, stress-joking context.",
    "why": "The model applied a formal, high-stakes interpretation (attributing blame and demanding evidentiary rigor) rather than reading the lightweight #random banter context about demo-week stress. In this channel, the sarcastic phrasing (\u201cCool cool\u2026 Love that for us\u201d) functions as venting/humor consistent with prior messages, not a veiled interpersonal attack. Likewise, \u201cbecause eng slipped\u201d is shorthand for a commonly understood schedule slip in a sprint context, not a substantive factual allegation requiring citation; the model treated it as an ungrounded claim instead of informal gripe. Overall, it ignored channel norms and over-indexed on linguistic sarcasm markers as inherently toxic.",
    "failure_pattern": "ignoring-channel-context",
    "severity": "major",
    "id": 473,
    "message": "Cool cool\u2014so we\u2019re shipping the \"half-baked\" version again because eng slipped? Love that for us.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Non-Objective",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed the expected Rudeness flag; the other two flags (Non-Objective, Passive-Aggressive) were correctly predicted.",
    "why": "The model correctly identified sarcasm/passive aggression and a blame-leaning, non-evidenced causal claim, but it failed to also classify the overtly disrespectful/derisive tone. Phrases like \u201cCool cool,\u201d \u201chalf-baked,\u201d \u201cagain,\u201d and \u201cLove that for us\u201d are not just indirect negativity\u2014they\u2019re openly snide and dismissive in a high-visibility, cross-functional channel, effectively mocking the decision and blaming engineering. The model likely treated rudeness as redundant with passive-aggression and didn\u2019t apply the separate rudeness label, underweighting the channel norm (crisp, professional public updates) and the direct contempt implied by the phrasing.",
    "failure_pattern": "rudeness-overlap-with-passive-aggression-missed",
    "severity": "major",
    "id": 474,
    "message": "Cool cool\u2014so we\u2019re shipping the \"half-baked\" version again because eng slipped? Love that for us.",
    "ground_truth_flags": [
      "Rudeness",
      "Non-Objective",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Non-Objective",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: it labeled the message as Passive-Aggressive and Rudeness when, in this startup/Slack war-room context, the tone is blunt but within normal bounds and not clearly an interpersonal attack.",
    "why": "The model over-weighted surface cues (\"dumping the mess on eng\", \ud83d\ude43, \"kthx\") as inherently sarcastic/demeaning without integrating the situational norms provided: blunt, emoji-heavy, fast-moving incident response with existing tension about overpromising. In this context, the message is a direct boundary-setting request plus a concrete proposed script for Sales; the emoji and \"kthx\" function as informal cadence/softener rather than a veiled jab. It also failed to distinguish team-level process frustration (about overpromising) from targeted insult/derogation toward an individual, and treated direct accountability language as rudeness rather than urgent operational coordination.",
    "failure_pattern": "cultural-style-misclassified",
    "severity": "minor",
    "id": 475,
    "message": "Can we please stop overpromising and then dumping the mess on eng? \ud83d\ude43 Just tell them we fixed the blips and we\u2019re not promising 100% until Monday. kthx.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Missed the expected \"Non-Objective\" flag (false negative). The other two predicted flags (\"Passive-Aggressive\", \"Rudeness\") align with expectations.",
    "why": "The model treated the message primarily as tone/sarcasm and interpersonal friction, but didn\u2019t recognize that it also contains a non-objective, subjective attribution of intent/blame (\"overpromising\" and \"dumping the mess on eng\") stated as a general accusation rather than verifiable, incident-specific facts. In this context, \"Non-Objective\" is triggered by emotionally loaded framing and implied causality/motive (sales overpromises \u2192 eng gets dumped on) without evidence in the message itself. The model likely weights explicit markers (\ud83d\ude43, \"kthx\") for passive-aggression and obvious demeaning phrasing for rudeness, while underweighting the more semantic criterion for non-objectivity (subjective blame/generalization) especially in a fast, blunt Slack style where it can look like normal venting.",
    "failure_pattern": "missed-non-objective-blame-framing",
    "severity": "minor",
    "id": 476,
    "message": "Can we please stop overpromising and then dumping the mess on eng? \ud83d\ude43 Just tell them we fixed the blips and we\u2019re not promising 100% until Monday. kthx.",
    "ground_truth_flags": [
      "Non-Objective",
      "Passive-Aggressive",
      "Rudeness"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as Passive-Aggressive when it was acceptable banter in #random; no flags were expected.",
    "why": "The model over-weighted generic linguistic markers (\"Per my last message\", \"stop debating\", \ud83d\ude43) that can signal passive aggression in work/task threads, but failed to ground the interpretation in the actual channel and topic. In this context, #random is a playful snack/cereal debate, and the phrasing reads as humorous meta-commentary (\u201clet\u2019s stop arguing and move on\u201d) rather than a veiled reprimand or status-power move. It also likely leaked assumptions from the broader scenario tension (Maya vs. Arjun in #engineering) onto an unrelated #random exchange, causing a seriousness/urgency misread.",
    "failure_pattern": "ignoring-channel-context",
    "severity": "minor",
    "id": 477,
    "message": "Per my last message, can we please stop debating and just ship it already? \ud83d\ude43",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed the expected \"Rudeness\" flag (under-flagging). It correctly flagged \"Passive-Aggressive\" but failed to also tag the message as rude/dismissive.",
    "why": "The model treated the toxicity primarily as sarcasm/passive aggression (\"Per my last message\" + \ud83d\ude43) and didn\u2019t separately classify the explicitly dismissive imperative as rudeness. In this channel context (#engineering norms: concise, clear ownership, avoid public blame) the phrase \"please stop debating\" + \"just ship it already\" is a direct shutdown of legitimate risk discussion and reads as contemptuous/invalidating toward engineers\u2019 concerns\u2014rudeness independent of the sarcastic veneer. The model likely over-weighted the passive-aggressive cues and conflated them with (rather than adding) a rudeness label, and it under-used situational context (public pressure, quality/safety concerns) that amplifies how rude the dismissal is.",
    "failure_pattern": "underflagging-rudeness-when-passive-aggressive-present",
    "severity": "major",
    "id": 478,
    "message": "Per my last message, can we please stop debating and just ship it already? \ud83d\ude43",
    "ground_truth_flags": [
      "Rudeness",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: the model marked the message as Non-Objective and Rudeness even though the expected flags were none.",
    "why": "The model treated joking, meme-like phrasing (\u201cworks on my machine\u201d, \u201clight the client on fire\u201d) as genuinely disrespectful and as a factual character/intent attribution. It failed to weight the strong #random banter context (everyone is riffing on release-eve chaos with sarcasm) and the message\u2019s core content is an objective bug report tied to a concrete artifact (payment flow failing on RC-17) plus a request to investigate. In this channel and thread tone, the sarcasm functions as camaraderie/stress-humor rather than an escalation or blame statement. The \u2018Non-Objective\u2019 interpretation overreached by reading a common engineering idiom as a definitive claim about Leo\u2019s behavior/intent, instead of recognizing it as conventional shorthand/humor.",
    "failure_pattern": "cultural-style-misclassified",
    "severity": "major",
    "id": 479,
    "message": "Leo, can we not ship your \u201cworks on my machine\u201d special again? I\u2019m seeing the payment flow blow up on RC-17 \u2014 pls take a look before we light the client on fire tomorrow.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Non-Objective",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "The model failed to flag clear Rudeness, Non-Objective language, and Passive-Aggressive phrasing in the target message, returning no flags despite multiple overt violations.",
    "why": "The message contains explicit disparagement and blame (\u201cyour \u2018works on my machine\u2019 special\u201d), inflammatory hyperbole (\u201clight the client on fire\u201d), and an accusatory tone framed as a question (\u201ccan we not ship\u2026 again?\u201d) that implies prior incompetence and invites defensiveness. These are not subtle: they violate the stated expectation for blameless, specific updates and are especially risky given exec/CSM sensitivity and the high-stakes rollout context. The model likely over-weighted the presence of a legitimate technical issue (\u2018payment flow blow up on RC-17\u2019) and treated the rest as casual startup banter, failing to interpret sarcasm/quotes and metaphor as rudeness + non-objective escalation language, and/or it did not incorporate the channel norms and leadership guidance present in the preceding context.",
    "failure_pattern": "ignoring-blameful-sarcasm-and-hyperbole",
    "severity": "critical",
    "id": 480,
    "message": "Leo, can we not ship your \u201cworks on my machine\u201d special again? I\u2019m seeing the payment flow blow up on RC-17 \u2014 pls take a look before we light the client on fire tomorrow.",
    "ground_truth_flags": [
      "Rudeness",
      "Non-Objective",
      "Passive-Aggressive"
    ],
    "predicted_flags": []
  },
  {
    "what_went_wrong": "Flagged incorrectly: the model labeled the message as \u201cNon-Objective\u201d and \u201cPassive-Aggressive\u201d even though the expected policy outcome is no flags for this message in this scenario.",
    "why": "The model treated blunt, deadline-driven wording (\u201cstop moving the goalposts,\u201d \u201cnot vibes\u201d) as a policy-relevant tone violation rather than permissible directness. It also over-weighted a generic linguistic heuristic (sarcasm markers like \u201ccool cool\u201d and dismissive phrasing) and under-weighted the conversation context where an ETA is legitimately needed and the channel norms ask for tight, actionable updates. In other words, it conflated \u2018not maximally polite/neutral\u2019 with \u2018flag-worthy,\u2019 and interpreted frustration as passive-aggression even though the message is explicit (not indirect) and contains a concrete request (ETA + scope adherence).",
    "failure_pattern": "overflagging-direct-communication",
    "severity": "major",
    "id": 481,
    "message": "Cool cool \u2014 can we please stop moving the goalposts and just ship what\u2019s in the doc? I need an actual ETA, not vibes.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Non-Objective",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed the expected \"Passive-Aggressive\" flag: the message contains sarcastic/dismissive phrasing (\"Cool cool\", \"not vibes\") and an indirect jab that escalates tension beyond plain rudeness.",
    "why": "The model correctly identified confrontational tone (Rudeness) and an ungrounded accusation (Non-Objective), but it likely treated the sarcasm as merely part of rudeness instead of a distinct passive-aggressive signal. It also underweighted the channel context (cross-functional, leadership/support monitoring, high-signal expectation), where performative snark and indirect digs are especially salient as passive aggression because they publicly shame or imply incompetence without a direct, constructive ask.",
    "failure_pattern": "missing-subtle-passive-aggression",
    "severity": "major",
    "id": 482,
    "message": "Cool cool \u2014 can we please stop moving the goalposts and just ship what\u2019s in the doc? I need an actual ETA, not vibes.",
    "ground_truth_flags": [
      "Rudeness",
      "Non-Objective",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Non-Objective",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as Non-Objective and Passive-Aggressive even though the expected outcome was no flags (i.e., it overflagged).",
    "why": "The model treated rhetorical phrasing and workplace shorthand (\u201cLove how\u2026 \ud83d\ude43\u201d, \u201cstop bikeshedding\u201d) as policy-relevant passive-aggression and treated the quoted phrase \u201cengineering delay\u201d as an asserted fact. In this DM context, the message is more plausibly venting/frustration + a push for commitment, not an objective claim requiring evidence. The model also didn\u2019t weight the instruction/ground-truth assumption that this org may tolerate informal, emotional language in private DMs; it applied a strict tone-policing lens rather than the likely labeling standard used for \u2018flags\u2019 (which appears to reserve flags for clearer hostility/harassment, not sarcasm/complaining).",
    "failure_pattern": "overflagging-low-grade-sarcasm",
    "severity": "minor",
    "id": 483,
    "message": "Love how this is somehow an \"engineering delay\" again \ud83d\ude43 Can we please just commit to Friday and stop bikeshedding? @Maya @Rahul @Isha @Ben",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Non-Objective",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed the expected \"Rudeness\" flag (under-flagging). The model correctly flagged Non-Objective and Passive-Aggressive but failed to also classify the message as explicitly rude/derogatory.",
    "why": "The model appears to have collapsed overtly rude phrasing into the Passive-Aggressive category and didn\u2019t separately trigger the Rudeness detector. Phrases like \u201cstop bikeshedding\u201d (dismissive/condescending) plus \u201cLove how this is somehow an \\\"engineering delay\\\" again \ud83d\ude43\u201d (sarcastic blame) function as direct disrespect toward engineering, not just indirect hostility. It likely overweighted the presence of sarcasm/emoji as the primary signal (passive-aggression) and underweighted the explicit belittling imperative (\u201cjust commit\u2026 and stop bikeshedding\u201d) and public call-out via multiple @mentions in a high-stakes thread, which amplifies perceived rudeness. In short: category-boundary error (PA vs. Rudeness) and insufficient consideration of how the phrasing reads as a personal jab rather than merely non-constructive tone.",
    "failure_pattern": "rudeness-subsumed-by-passive-aggression",
    "severity": "major",
    "id": 484,
    "message": "Love how this is somehow an \"engineering delay\" again \ud83d\ude43 Can we please just commit to Friday and stop bikeshedding? @Maya @Rahul @Isha @Ben",
    "ground_truth_flags": [
      "Rudeness",
      "Non-Objective",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Non-Objective",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as Passive-Aggressive when it should not have been flagged.",
    "why": "The model over-indexed on surface cues (\"lol\", italicized *again*, and an emoji) as evidence of passive-aggression, while missing that in-channel context (#random) the entire thread is playful, meme-y banter about deploys/rewrites/feature flags. In that setting, the phrasing reads as aligned humor + a straightforward preference (patch + flags) rather than an indirect or veiled jab at a specific person. The message is informal and slightly snarky, but not meaningfully passive-aggressive given the norms established by preceding jokes and the non-decision, low-stakes channel context.",
    "failure_pattern": "ignoring-channel-context",
    "severity": "minor",
    "id": 485,
    "message": "lol can we not do the whole \u201clet\u2019s rewrite it\u201d thing *again* \u2014 ship the tiny patch + flags and move on \ud83d\ude43",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed a required flag: the message should also be flagged as Non-Objective (in addition to Passive-Aggressive), but the model only returned Passive-Aggressive.",
    "why": "The model focused on tone cues (\"lol\", \"again\", sarcastic framing, emoji) and correctly identified passive-aggression, but it underweighted the channel/thread norm Elena set: decision-oriented, specific (scope/owners/timeline/risk). The target message offers no concrete data, risk assessment, ownership, timeline, or actionable alternative beyond a rhetorical dismissal (\"ship the tiny patch + flags and move on\"), which fails the objective/decision-oriented requirement in this context. The model appears to apply a narrower definition of Non-Objective (e.g., only vague or circular content) and ignored that rhetorical, dismissive phrasing without substantiation is non-objective in a high-stakes decision thread.",
    "failure_pattern": "contextual-non-objective-missed",
    "severity": "major",
    "id": 486,
    "message": "lol can we not do the whole \u201clet\u2019s rewrite it\u201d thing *again* \u2014 ship the tiny patch + flags and move on \ud83d\ude43",
    "ground_truth_flags": [
      "Non-Objective",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: the model labeled the message as Passive-Aggressive even though it\u2019s consistent with the channel\u2019s joking, informal meta-commentary and doesn\u2019t clearly target or demean anyone.",
    "why": "The model over-weighted superficial markers often associated with passive aggression (\"Cool,\" rhetorical framing, \ud83d\udc4d) and treated them as definitive, while under-weighting the #general context where multiple teammates are already making light, sarcastic remarks about standups and undocumented decisions. In this thread, the message reads as aligning with the established playful gripe about process (\u201cdid it even happen?\u201d / \u201cvibes sync\u201d) rather than a veiled personal jab. The ambiguity is real, but without a clear interpersonal dig, blame, or snide insinuation toward a specific person, the safer interpretation given the prior banter is neutral-to-wry confirmation-seeking, not a conduct issue.",
    "failure_pattern": "cultural-style-misclassified",
    "severity": "minor",
    "id": 487,
    "message": "Cool, so we\u2019re just going with whatever we decided after standup then? \ud83d\udc4d",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: labeled the message as \"Passive-Aggressive\" and \"Fake\" even though the expected outcome was no flags.",
    "why": "The model treated informal, rapport-based sarcasm in a private DM as a policy-relevant communication risk rather than normal venting. It over-indexed on surface markers (\"vibe-shipping,\" \"love that for us\") and mapped them to hostility/insincerity without weighing (a) the channel (DM, not #general), (b) the established casual tone with emojis and joking in the thread, and (c) the primary intent: a direct, actionable request to freeze changes. The \"Fake\" label is especially unsupported: \"love that for us\" is a common idiom for ironic frustration, not an attempt to deceive or perform sincerity. In short, it misclassified a legitimate conversational style as problematic instead of reserving flags for clearer interpersonal harm or escalation risk.",
    "failure_pattern": "cultural-sarcasm-overflagged-in-private-channel",
    "severity": "minor",
    "id": 489,
    "message": "Cool, so we\u2019re just vibe-shipping again \u2014 love that for us. Can we please not change anything else today?",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Fake",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed two expected flags: it only flagged Passive-Aggressive and failed to flag Vagueness and Non-Objective.",
    "why": "The model correctly detected sarcasm but treated the message primarily as a tone issue and didn\u2019t evaluate it against the channel\u2019s process-heavy requirements (explicit owners, ETAs, Jira links, actionable mitigation). In this context, \u201cCan we please not change anything else today?\u201d is a broad, unspecific directive without scope (what systems, which tickets, which changes are excluded), rationale tied to metrics/compliance, or an owner/next step\u2014so it\u2019s vague. It\u2019s also non-objective because it leads with emotional/ironic commentary (\u201cvibe-shipping\u201d, \u201clove that for us\u201d) rather than factual status, decision criteria, or a concrete proposal aligned to the VP update request. The model likely over-weighted the most salient linguistic cue (sarcasm) and under-weighted the pragmatic/actionability checks that drive Vagueness/Non-Objective in a formal, executive-visible incident thread.",
    "failure_pattern": "tone-salience-over-contextual-actionability",
    "severity": "major",
    "id": 490,
    "message": "Cool, so we\u2019re just vibe-shipping again \u2014 love that for us. Can we please not change anything else today?",
    "ground_truth_flags": [
      "Vagueness",
      "Non-Objective",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as Passive-Aggressive when, in context, it was largely a playful riff on the thread\u2019s running joke (\u201cper my last message\u201d) plus a mild, situational grievance; expected no flags.",
    "why": "The model over-weighted lexical cues (\"Per my last message...\", ellipsis, \ud83d\ude05) as universal markers of passive-aggression and under-weighted the #random channel context where multiple teammates were explicitly joking about that phrase. It treated the complaint as a veiled dig rather than recognizing it as semi-humorous, norm-aligned banter in a non-workflow channel. It also didn\u2019t distinguish between a genuinely snide insinuation and a low-stakes, conversational nudge that\u2019s consistent with the preceding messages\u2019 tone.",
    "failure_pattern": "ignoring-channel-context",
    "severity": "minor",
    "id": 491,
    "message": "Per my last message... would\u2019ve been cool to be invited to the call before we tagged me for sign-off \ud83d\ude05",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as Passive-Aggressive when it should not have been flagged at all (false positive).",
    "why": "The model over-indexed on surface cues of sarcasm (\"I\u2019ll bring popcorn\") and negative outcome framing (\"penalty call\") and treated them as inherently passive-aggressive. In this context, the message reads as a stress-humor caution about risk, not an indirect personal jab or covert hostility toward Leo. The statement is conditional (\"if we ship this as-is\") and aimed at the situation/process, not at a person\u2019s competence, and #general already contains similar light, high-stress banter. The model also appears to apply a generic rule\u2014sarcastic humor \u21d2 passive-aggression\u2014without distinguishing between culture-typical gallows humor vs. veiled resentment or blame.",
    "failure_pattern": "cultural-style-misclassified",
    "severity": "minor",
    "id": 493,
    "message": "Cool, if we ship this as-is and it blows up tomorrow, I\u2019ll bring popcorn to the penalty call \ud83c\udf7f",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed two expected flags: it only flagged Passive-Aggressive, but failed to also flag Rudeness and Non-Objective.",
    "why": "The model correctly detected sarcasm/passive-aggression but treated the message as primarily an indirect critique, overlooking that it is also overtly rude in this channel context (mocking \u2018popcorn\u2019 about a penalty call, trivializing risk, and implicitly shaming a colleague). It also missed the Non-Objective dimension: the message provides no actionable status/ETA and instead injects a subjective, snide hypothetical (\u2018if it blows up\u2019)\u2014which violates the #client-updates norm of concise, factual updates. In short, it focused on tone classification in isolation and underweighted channel expectations and the lack of objective, operational content.",
    "failure_pattern": "ignoring-channel-context",
    "severity": "major",
    "id": 494,
    "message": "Cool, if we ship this as-is and it blows up tomorrow, I\u2019ll bring popcorn to the penalty call \ud83c\udf7f",
    "ground_truth_flags": [
      "Rudeness",
      "Non-Objective",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged the message incorrectly for Passive-Aggressive (and secondarily Vagueness) when it was a joking #random quip with no actionable request; expected no flags.",
    "why": "The model treated meme-y sarcasm markers (\"cool cool,\" \"YOLO,\" \"pray the demo gods\") as workplace negativity aimed at a real decision thread, and assumed it was critiquing a plan. It also applied an 'actionability/clarity' standard (vagueness) that doesn\u2019t fit the #random channel\u2019s banter context. In short, it over-indexed on textual cues of resignation/sarcasm while ignoring channel purpose and the preceding playful tone, misclassifying humor as passive-aggression.",
    "failure_pattern": "ignoring-channel-context",
    "severity": "minor",
    "id": 495,
    "message": "Cool cool\u2014let\u2019s just YOLO it and pray the demo gods are in a good mood \ud83d\ude43",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Vagueness",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "The model missed two expected flags (Vagueness, Non-Objective) and added an incorrect/unexpected flag (One-Liner).",
    "why": "It over-indexed on surface features (short length + sarcasm) and treated brevity itself as a policy issue, even though the real problems are (a) the message provides no actionable plan or concrete commitment in a high-stakes, action-oriented thread (Vagueness) and (b) it replaces technical risk assessment with a jokey, faith-based framing (\u201cYOLO\u2026pray\u2026demo gods\u201d), which is non-evidentiary and undermines objective decision-making (Non-Objective). The context (explicit request to be concise and action-oriented; need for risk assessment and external comms readiness; CEO visibility) makes these two flags especially salient, but the model\u2019s reasoning focused on tone and length rather than the mismatch to the decision-making requirements. \u2018One-Liner\u2019 is a stylistic descriptor that doesn\u2019t necessarily indicate a communication violation here; many concise messages are acceptable in async threads, and the harm comes from content quality (vagueness/non-objectivity), not the number of lines.",
    "failure_pattern": "content-quality-missed-overfocus-on-brevity",
    "severity": "major",
    "id": 496,
    "message": "Cool cool\u2014let\u2019s just YOLO it and pray the demo gods are in a good mood \ud83d\ude43",
    "ground_truth_flags": [
      "Vagueness",
      "Non-Objective",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "One-Liner",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: it over-identified \u201cVagueness\u201d and \u201cPassive-Aggressive\u201d in a message that is urgent but still sufficiently specific given the shared context and not clearly passive-aggressive by the policy\u2019s standard.",
    "why": "The model evaluated the sentence in isolation and applied generic heuristics (\u201cit\u201d = vague, \u201cjust\u201d/\u201cdance\u201d = passive-aggressive) without weighting the immediately preceding context (\u201cFinEdge demo found a checkout bug\u201d) that makes the referent of \u201cit\u201d reasonably clear in-channel. It also treated a direct expression of time pressure and frustration as passive-aggression; however, the message is overtly direct (explicit ask, explicit constraint, explicit risk) rather than indirect, insinuating, or sarcastic. In this environment, the main issue is channel/process mismatch (posting in #general/public vs #eng-triage) and escalation etiquette, which the model\u2019s chosen flags don\u2019t capture\u2014so it substituted nearby categories (vagueness, passive-aggression) instead of recognizing this as acceptable direct urgency.",
    "failure_pattern": "overflagging-urgency-with-process-friction",
    "severity": "major",
    "id": 497,
    "message": "Theo, can you please just jump on this and fix it tonight? I really don\u2019t have time for the whole triage dance while the client is literally threatening to walk.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Vagueness",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed the expected \"Rudeness\" flag (false negative). It correctly flagged \"Vagueness\" and \"Passive-Aggressive\".",
    "why": "The model appears to treat overt insults/profanity as the primary signal for rudeness and therefore downweighted rudeness when the message contained polite surface markers (\"can you please\"). It didn\u2019t fully account for contextual rudeness: (1) dismissing an established process immediately after Theo\u2019s public reminder (\"triage dance\"), (2) issuing an imperative after being told to route through #eng-triage, and (3) pressuring for after-hours work (\"fix it tonight\") in a public cross-functional channel, which reads as disrespectful to boundaries and role norms. In this setting, the combination of public escalation + process dismissal + urgency-based entitlement is not just passive-aggressive; it is also rude.",
    "failure_pattern": "rudeness-missed-due-to-politeness-markers",
    "severity": "major",
    "id": 498,
    "message": "Theo, can you please just jump on this and fix it tonight? I really don\u2019t have time for the whole triage dance while the client is literally threatening to walk.",
    "ground_truth_flags": [
      "Rudeness",
      "Vagueness",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Vagueness",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged the message for Vagueness when it should not have been flagged at all (false positive).",
    "why": "The model treated the deictic reference (\u201cthis\u201d) as inherently ambiguous without weighing the actual conversational and organizational context. In this scenario, the message is a reasonable, low-risk request for ownership and approved phrasing\u2014exactly the kind of clarification a strict SOP, audit-sensitive environment would encourage. The AI also appears to apply a blanket heuristic (\u201cpronouns like \u2018this\u2019 = vagueness\u201d) rather than assessing whether recipients can plausibly infer the referent (e.g., the current rollout/incident thread) or whether the channel\u2019s norm tolerates shorthand. Since the expected policy outcome is \u2018no flags,\u2019 the model\u2019s interpretation of vagueness was an over-strict style enforcement rather than a genuine communication risk.",
    "failure_pattern": "overflagging-vagueness-heuristic",
    "severity": "minor",
    "id": 499,
    "message": "Can someone just tell me who owns this + what the *official* wording is? I don\u2019t want to accidentally start a fire with a half-baked take \ud83d\ude05",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Vagueness"
    ]
  },
  {
    "what_went_wrong": "Missed both expected flags (Vagueness, Non-Objective) and returned no flags despite the message being imprecise and emotionally/subjectively phrased for a formal, audit-sensitive status channel.",
    "why": "The model treated the text as a straightforward ownership/wording request and evaluated it mostly at the sentence level, overlooking (a) missing required operational specifics (no ticket ID, no named artifact/link, no target audience/client vs internal, no concrete question format) that make it vague in this SOP-driven channel, and (b) subjective, affect-laden language (\u201cjust,\u201d \u201cstart a fire,\u201d \u201chalf-baked take,\u201d emoji) that introduces non-objective framing and risk-of-misinterpretation under Legal scrutiny. It also failed to apply channel norms from context (concise, sourced, ticket-tagged, authoritative) when deciding whether the message meets the bar.",
    "failure_pattern": "ignoring-channel-context",
    "severity": "major",
    "id": 500,
    "message": "Can someone just tell me who owns this + what the *official* wording is? I don\u2019t want to accidentally start a fire with a half-baked take \ud83d\ude05",
    "ground_truth_flags": [
      "Vagueness",
      "Non-Objective"
    ],
    "predicted_flags": []
  }
]