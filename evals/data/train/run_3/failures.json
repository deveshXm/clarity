[
  {
    "what_went_wrong": "Flagged incorrectly: it added a \"Non-Objective\" flag even though the main issue is simply lack of specifics (Vagueness).",
    "why": "The model treated an urgent, colloquial stop-ship statement (\"we can\u2019t ship like this\") as an unjustified factual claim rather than a standard QA escalation signal. In this context, it functions as a subjective risk call/decision trigger, not an attempt to assert an objective truth. The real problem is that the message doesn\u2019t provide actionable details (steps, scope, severity, evidence) appropriate for a high-stakes public channel, which is already captured by the Vagueness flag.",
    "failure_pattern": "overflagging-non-objective-urgent-qa-language",
    "severity": "minor",
    "id": 2,
    "message": "leo can you take a look at this asap? something seems off in the payment flow and we can\u2019t ship like this.",
    "ground_truth_flags": [
      "Vagueness"
    ],
    "predicted_flags": [
      "Vagueness",
      "Non-Objective"
    ]
  },
  {
    "what_went_wrong": "Flagged the wrong tone category: it labeled the message as Passive-Aggressive instead of Rudeness (while correctly flagging Vagueness).",
    "why": "The model appears to have treated a blunt, imperative shutdown (\u201ccalm down and stop speculating\u201d) as passive-aggression, but the content is not indirect, veiled, or sarcastic\u2014it\u2019s overtly dismissive and scolding, which aligns more cleanly with Rudeness. It also underweighted the enterprise/ITIL outage-comms context where executives are expected to provide calm, protocol-aligned guidance (DRI, next update time, approved customer language) rather than chastising the audience; that context makes the tone breach more clearly \u2018rude/unprofessional\u2019 than \u2018passive-aggressive.\u2019",
    "failure_pattern": "tone-label-confusion-passive-aggressive-vs-rude",
    "severity": "major",
    "id": 3,
    "message": "Everyone just calm down and stop speculating. We\u2019ll handle it and you\u2019ll get an update when there\u2019s something worth sharing.",
    "ground_truth_flags": [
      "Vagueness",
      "Rudeness"
    ],
    "predicted_flags": [
      "Vagueness",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed the expected \"Microaggressive\" flag (under-flagging); it only flagged \"Passive-Aggressive.\"",
    "why": "The model correctly identified the curt/shut-down tone as passive-aggressive, but failed to recognize that the second sentence contains a status-based microaggression toward remote work: it frames Maya\u2019s remote status as the problem (\"If remote is making it hard to keep up\") and implies she is less capable/less legitimate than co-located teammates, with an exclusionary threat (\"we\u2019ll loop in someone local next time\"). That\u2019s othering + conditional inclusion tied to location, dismissing a reasonable accessibility question and leveraging power dynamics in front of leadership. The model treated that line primarily as indirect reprimand rather than identity/status-based bias and exclusion, likely due to over-weighting explicit slurs/identity markers and under-weighting structural/role-based microaggressions (remote vs local) and the implicit penalty.",
    "failure_pattern": "missed-remote-work-microaggression",
    "severity": "major",
    "id": 4,
    "message": "Maya, we don\u2019t have time to re-litigate the basics \u2014 just take what\u2019s in the Figma and make it look clean. If remote is making it hard to keep up with the quick decisions, just say so and we\u2019ll loop in someone local next time.",
    "ground_truth_flags": [
      "Microaggressive",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed the expected flags (Dismissive, Rudeness) and instead flagged Passive-Aggressive, which is not the primary behavior in the text.",
    "why": "The message is overtly dismissive and rude: it explicitly tells the requester to \u201cplease stop\u201d and deprioritizes their urgent need (\u201cwe\u2019ll look when we can\u201d), in a public channel, with a scolding tone (\u201clike everyone else\u201d). Passive-aggression typically involves indirectness, insinuation, or veiled criticism; here the criticism is direct and unambiguous. The model likely treated any curt reprimand as passive-aggressive and underweighted the explicit devaluation of the request and the shaming/commanding phrasing. It also appears to over-index on the coordination/process intent (ticket workflow) and misclassify blunt enforcement as passive-aggression rather than rudeness/dismissiveness.",
    "failure_pattern": "overflagging-passive-aggression-instead-of-rudeness",
    "severity": "major",
    "id": 5,
    "message": "We\u2019re in a postmortem + code freeze \u2014 please stop dropping urgent sales stuff in here. File a ticket with repro + links like everyone else and we\u2019ll look when we can.",
    "ground_truth_flags": [
      "Dismissive",
      "Rudeness"
    ],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "It missed the expected \"Dismissive\" and \"One-Liner\" flags, and instead added an extra (unexpected) \"Non-Objective\" flag.",
    "why": "The model correctly identified rudeness, but treated the message\u2019s belittling/invalidating tone (\"stop overthinking,\" \"isn\u2019t that complicated,\" \"we don\u2019t need a bunch of questions\") as merely an evidence-free assertion (Non-Objective) rather than the core issue: dismissing a new hire\u2019s legitimate process questions in a high-stakes, audit-sensitive channel. It also failed to apply the taxonomy consistently: the message is short and functionally a terse shutdown, which should map to \"One-Liner\" and \"Dismissive\" given the channel norms (strict, authoritative, thread kept clean) and the power/tenure dynamic. In short, it over-indexed on propositional content (is it evidenced?) and under-indexed on interpersonal intent + brevity signals that the expected labels capture.",
    "failure_pattern": "missed-dismissive-and-length-style-overflagged-non-objective",
    "severity": "major",
    "id": 6,
    "message": "Just follow the template and stop overthinking it\u2014this isn\u2019t that complicated. We don\u2019t need a bunch of questions in here right now.",
    "ground_truth_flags": [
      "Dismissive",
      "One-Liner",
      "Rudeness"
    ],
    "predicted_flags": [
      "Non-Objective",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Missed a required flag: the message is clearly sarcastic as well as passive-aggressive, but the model only returned Passive-Aggressive.",
    "why": "The model\u2019s own rationale correctly identified explicit sarcasm cues (rhetorical question, ironic framing, quotes around \u201cfigure it out,\u201d and the \ud83d\ude43 emoji), but it appears to have treated sarcasm as merely evidence for Passive-Aggressive rather than an independent flag to output. In other words, it collapsed a multi-label case into a single label, likely due to label-overlap confusion (sarcasm used as a feature of passive-aggression) or a threshold/selection behavior that prioritizes one \u2018dominant\u2019 category instead of emitting all applicable categories in a multi-flag taxonomy. The context (formal, compliance-heavy org; DM that can be screenshotted) amplifies the sarcastic intent rather than making it ambiguous.",
    "failure_pattern": "multi-label-collapse-sarcasm-into-passive-aggression",
    "severity": "major",
    "id": 8,
    "message": "Totally fine\u2014who needs the handoff checklist when we can just give Risk another fun reason to hover \ud83d\ude43. Keep pushing that board KPI deck and we\u2019ll just \u201cfigure it out\u201d later, I guess.",
    "ground_truth_flags": [
      "Sarcastic",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: the model added a \"Non-Objective\" flag that wasn\u2019t expected/justified; only \"Circular\" should have been flagged.",
    "why": "The model treated a rhetorical, blame-leaning emphasis (\u201cif we can\u2019t ship it\u2019s because ownership wasn\u2019t clear\u201d) as an evidence-free factual claim, mapping it to \"Non-Objective.\" In this Slack incident context, the main issue is repetition without actionable content (circularity), not lack of objectivity: the statement is more of a stress/framing move than a testable assertion requiring support. The model likely over-relied on a generic heuristic (absolute causal phrasing => non-objective) and underweighted the channel norms (fast, blunt, informal) and the taxonomy intent (\"Non-Objective\" should capture unverifiable claims presented as facts, not rhetorical emphasis during triage).",
    "failure_pattern": "overflagging-non-objective-rhetorical-causality",
    "severity": "minor",
    "id": 11,
    "message": "We just need clearer ownership here\u2014if we don\u2019t have clear ownership, we can\u2019t ship, and if we can\u2019t ship it\u2019s because ownership wasn\u2019t clear. So let\u2019s make ownership clear by being clear about who owns what, because the only issue is unclear ownership.",
    "ground_truth_flags": [
      "Circular"
    ],
    "predicted_flags": [
      "Non-Objective",
      "Circular"
    ]
  },
  {
    "what_went_wrong": "Missed the Passive-Aggressive flag (under-flagged). It correctly flagged Rudeness but failed to recognize the veiled, sarcastic/conditional barbs layered on top of the direct confrontational wording.",
    "why": "The model anchored on overt rudeness markers (\"stop gambling with production\", direct accusation) and treated the rest as merely confrontational rather than identifying passive-aggressive structure: (1) a false-softener opener (\"as gently as possible\") followed by a sharp put-down, (2) insinuations about the other person\u2019s competence/understanding (\"isn\u2019t a substitute for actually understanding the failure mode\"), and (3) a taunting permission/ultimatum framed as choice (\"If you want to keep debating theory in a client-facing channel, go ahead\u2014otherwise...\") that implicitly shames Leo and signals dominance. The client-visible #client-updates context amplifies the passive-aggressive social function (public face-saving vs. blame) but the model\u2019s explanation focused on explicit disrespect and didn\u2019t model the indirect, performative jab component.",
    "failure_pattern": "missing-subtle-passive-aggression",
    "severity": "major",
    "id": 15,
    "message": "Leo, I\u2019m going to say this as gently as possible: shipping a hotfix behind a flag isn\u2019t a substitute for actually understanding the failure mode. If you want to keep debating theory in a client-facing channel, go ahead\u2014otherwise let\u2019s do the infra change and stop gambling with production.",
    "ground_truth_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ],
    "predicted_flags": [
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Missed two expected flags (Rudeness, Non-Objective) and only flagged Passive-Aggressive, under-classifying the message\u2019s direct incivility and subjective/insulting phrasing toward Maya.",
    "why": "The model correctly recognized sarcasm/passive-aggression but treated the rest as merely informal or \u201ctone light\u201d rather than explicitly rude. It likely over-indexed on the presence of sarcasm markers (\u201clol\u201d, \ud83d\ude43) and interpreted the content as a productivity-oriented request, missing that it contains a direct put-down (\u201cwe already landed this in the room\u201d), a belittling generalization (\u201cnot everything needs a full design sync\u201d), and a personal/timezone jab (\u201ccatch up when you wake up\u201d). It also under-weighted the context: Maya raised a legitimate coordination risk (demo Monday, investor update), so the dismissive response isn\u2019t just indirect negativity\u2014it\u2019s an objectively discourteous dismissal and uses non-objective, patronizing language instead of specific actionable information (what changed, where documented, what exact assets are needed).",
    "failure_pattern": "underflagging-rudeness-and-subjectivity",
    "severity": "major",
    "id": 17,
    "message": "lol Maya, we already landed this in the room \u2014 not everything needs a full design sync. Just catch up when you wake up and drop whatever assets you can so we can ship \ud83d\ude43",
    "ground_truth_flags": [
      "Rudeness",
      "Non-Objective",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "The model over-flagged by adding \"Vagueness\" when only \"Circular\" was expected; the message is repetitive but not meaningfully ambiguous in this context.",
    "why": "It latched onto a generic heuristic (\"engineering\" is an imprecise actor; \"confirm\" is an underspecified action) without grounding in the thread where the recipients are already clearly identified (Marcus/Priya/Devon/Liam) and the requested action is implicitly binary (commit to a Friday release). In other words, it evaluated the target sentence in isolation and treated rhetorical repetition as also implying unclear ownership/action, even though the surrounding context resolves those details. The actual issue is purely circular reasoning/pressure-by-repetition, not lack of specificity.",
    "failure_pattern": "context-not-considered-overflagging-vagueness",
    "severity": "minor",
    "id": 18,
    "message": "We need to ship Friday because the launch is Friday, and since the launch is Friday we need engineering to commit to shipping Friday. So can we just confirm Friday so we can hit the Friday launch?",
    "ground_truth_flags": [
      "Circular"
    ],
    "predicted_flags": [
      "Vagueness",
      "Circular"
    ]
  },
  {
    "what_went_wrong": "It over-flagged by adding \"Rudeness\" when the expected taxonomy called for \"Fake\" + \"Passive-Aggressive\". Passive-aggressive was correct; the miss was failing to identify the \"Fake\" (insincere/performative praise) component and substituting it with a harsher category.",
    "why": "The model latched onto the sarcastic contempt and treated it as straightforward interpersonal insult (rudeness), rather than decomposing the message into (a) insincere praise/performative agreement (\"Love the confidence\u2026 truly inspiring\u2026 I\u2019ll go update the proposal\u2026\") which fits \"Fake\" and (b) indirect hostility via sarcasm and scare quotes which fits \"Passive-Aggressive.\" In this org/channel context (polished, leadership-watched, post-incident), the rhetorical \"polite\" wrapping is a key signal: it\u2019s not blunt direct aggression so much as faux-positivity used to shame Jordan publicly. The model effectively collapsed \"Fake\" into \"Rudeness\" because it prioritized surface negativity over the communicative tactic (fake sincerity) the expected label set emphasizes.",
    "failure_pattern": "fake-missed-rudeness-substituted",
    "severity": "major",
    "id": 21,
    "message": "Love the confidence here, Jordan \u2014 truly inspiring how you can just declare a 2\u2011week custom API integration in #sales right after the timeline incident. I\u2019ll go update the proposal to match your \u201cvision\u201d unless anyone wants to stick to the approved connector playbook.",
    "ground_truth_flags": [
      "Fake",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Missed the expected flags (Dismissive, Rudeness) and instead applied a non-expected label (Passive-Aggressive), partially capturing tone but misclassifying the primary violation.",
    "why": "The message is overtly dismissive and rude: it shuts down substantive discussion (\u201cnot turn this into an architecture seminar\u201d), issues a directive (\u201cJust ship the hotfix\u201d), and uses scare quotes around \u201cclean refactor\u201d to belittle Maya\u2019s proposal. Those are direct, on-the-nose behaviors that map more cleanly to Dismissive/Rudeness than to Passive-Aggressive (which typically involves indirectness, veiled digs, or ambiguity). The model over-indexed on the quotation marks as \u2018sarcasm\u2019 and treated that as passive-aggression, while underweighting the explicit discussion-stopping and belittling phrasing. It also didn\u2019t fully incorporate the #general, CEO-watching, high-pressure context where public shutdowns are particularly rude/dismissive (even if the core intent\u2014prioritize hotfix\u2014is legitimate).",
    "failure_pattern": "mislabeling-dismissive-as-passive-aggressive",
    "severity": "major",
    "id": 22,
    "message": "Can we not turn this into an architecture seminar right now? Just ship the hotfix and we\u2019ll deal with the \u201cclean refactor\u201d later.",
    "ground_truth_flags": [
      "Dismissive",
      "Rudeness"
    ],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "The model missed the expected Rudeness flag and instead flagged Passive-Aggressive (likely misclassified) and Vagueness (weak/incorrect).",
    "why": "The primary issue in the target message is overt dismissiveness and belittling phrasing (\u201coverthinking/\u2018what if\u2019 stuff,\u201d \u201cjust run with this patch\u201d), which is more directly rudeness than passive-aggression (there\u2019s no indirectness; it\u2019s a blunt shutdown). The model appears to overweight certain lexical cues (quotes, \u2018overthinking,\u2019 curt imperatives) as \u2018passive-aggressive\u2019 even though the intent is explicit, and it underweighted the interpersonal impact (publicly minimizing a teammate\u2019s risk concerns in a high-stakes channel with leadership watching). The Vagueness flag is also context-insensitive: \u201cthis patch / what I asked for\u201d is clearly anchored to Maya\u2019s immediately preceding, specific instruction about a guard + fallback shipping method on the promo path, so ambiguity is minimal.",
    "failure_pattern": "rudeness-missed-passive-aggression-overattributed",
    "severity": "major",
    "id": 24,
    "message": "Jordan, I need you to just run with this patch \u2014 we don\u2019t have time for the overthinking/\u201cwhat if\u201d stuff right now. Let\u2019s keep it simple and ship what I asked for by 6; I\u2019ll handle the client narrative.",
    "ground_truth_flags": [
      "Rudeness"
    ],
    "predicted_flags": [
      "Vagueness",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged \"Passive-Aggressive\" incorrectly; only \"Vagueness\" was expected for this message in this culture/context.",
    "why": "The model over-interpreted a timeboxing/deferral phrase (\"we\u2019ll circle back later\") as dismissive shutdown, and treated the imperative framing (\"Can someone just figure out...\") as interpersonal sniping rather than the startup\u2019s normal blunt, speed-oriented Slack style. In context, the message is imprecise about ownership and scope (vagueness), but it doesn\u2019t contain clear passive-aggressive markers (sarcasm, veiled blame, pointed digs, or indirect hostility). The tension in the thread primed the model to infer negative intent from neutral project-management language, leading to a tone-based false positive.",
    "failure_pattern": "cultural-style-misclassified",
    "severity": "minor",
    "id": 25,
    "message": "Can someone just figure out the design stuff so we can keep moving? Let\u2019s make it work for the demo and we\u2019ll circle back later.",
    "ground_truth_flags": [
      "Vagueness"
    ],
    "predicted_flags": [
      "Vagueness",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed the expected \"Passive-Aggressive\" flag and instead applied \"Rudeness\"; it caught that the message is non-objective/personal but miscategorized the interpersonal violation.",
    "why": "The target message contains an indirect, status-based jab (\u201cpolished\u2026 ambitious when you\u2019re new here and still learning how we scope things\u201d) wrapped in ostensibly cooperative language (\u201cTotally hear you\u2026 let\u2019s keep it simple\u201d). That combination is characteristic of passive-aggression: a veneer of alignment while undercutting the recipient\u2019s credibility via a veiled dig. The model likely overweighted the explicit patronizing content and mapped it to a more generic/stronger toxicity label (\u201cRudeness\u201d), without recognizing the rhetorical structure (compliment/acknowledgment + sting) and the org context where public thread language is scrutinized and screenshots amplify implied blame. In short, it detected hostility but didn\u2019t distinguish indirect sniping from overt rudeness.",
    "failure_pattern": "passive-aggression-mislabeled-as-rudeness",
    "severity": "major",
    "id": 26,
    "message": "Totally hear you, Maya \u2014 let\u2019s keep it simple and stick to what\u2019s already built. Sometimes the \"polished\" stuff is a bit\u2026 ambitious when you\u2019re new here and still learning how we scope things.",
    "ground_truth_flags": [
      "Non-Objective",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Non-Objective",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: it added a \u201cNon-Objective\u201d flag even though the message\u2019s core issue is tone (passive-aggressive sarcasm), not lack of objectivity as defined by unverifiable claims that materially affect the communication.",
    "why": "The model treated a sarcastic rhetorical line (\u201cwe apparently don\u2019t have time\u2026\u201d) as a factual assertion requiring evidence, rather than recognizing it as a tone marker already captured by Passive-Aggressive. It conflated \u2018implied accusation / snark\u2019 with \u2018non-objective statement,\u2019 effectively double-counting the same signal under two labels. It also didn\u2019t distinguish between (a) subjective/interpretive language that is primarily about interpersonal framing and (b) genuinely non-objective operational content that would mislead or confuse recipients.",
    "failure_pattern": "overflagging-non-objective-on-sarcasm",
    "severity": "minor",
    "id": 27,
    "message": "Cool, thanks for the thoughtful process discussion \u2014 I\u2019ll just go ahead and handle the hotfix myself since we apparently don\u2019t have time to ship fixes when enterprise is down. Appreciate everyone jumping on this.",
    "ground_truth_flags": [
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Non-Objective",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged an extra category: it correctly detected Passive-Aggressive, but incorrectly added Rudeness beyond the expected label set for this message/context.",
    "why": "The model appears to treat sarcastic, pointed phrasing (\"Awesome, thanks... super helpful\"; \"late-night surprise\") as inherently \"rude\" in addition to passive-aggressive. In this org\u2019s fast, candid culture and in a high-pressure incident channel, direct accountability language can be common; the primary issue here is sarcasm/indirect hostility rather than overt insult, profanity, slurs, or explicit name-calling. The model likely over-weighted the accusatory framing (\"dropping... and then disappearing\") as 'hostile language' and failed to distinguish 'passive-aggressive snark' from a separate 'rudeness' violation when only one flag was intended.",
    "failure_pattern": "overflagging-passive-aggression-as-rudeness",
    "severity": "minor",
    "id": 30,
    "message": "Awesome, thanks for dropping the partial fix and then disappearing \u2014 super helpful on launch eve. When you get a minute, could you maybe let us know whether checkout is actually fixed or if we should plan for another late-night surprise?",
    "ground_truth_flags": [
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "It missed the expected \"Fake\" flag (false negative) and instead added an extra \"Non-Objective\" flag (false positive) that wasn\u2019t part of the expected labeling.",
    "why": "The message is clearly sarcastic and backhanded (correctly caught as Passive-Aggressive), but the model interpreted the sarcasm as an evidence-free factual claim and mapped it to \"Non-Objective.\" In this context, the core additional issue is performative/polite-on-the-surface praise (\u201cAwesome hustle\u201d, \u201clove the confidence\u201d, \ud83d\ude4c) that is not sincere and is used to shame or reprimand\u2014i.e., \"Fake\" as in inauthentic positive framing masking criticism. The model likely treated \"Fake\" as meaning fabricated information or deception, rather than insincere/performative positivity. It also didn\u2019t use channel/power-dynamics context (CTO addressing subordinates post-incident) to recognize that the \u201cpraise\u201d is functionally a public scold, making the inauthenticity more salient than objectivity concerns.",
    "failure_pattern": "fake-flag-misinterpreted-as-nonobjective",
    "severity": "major",
    "id": 31,
    "message": "Awesome hustle everyone \u2014 *love* the confidence in promising timelines before we\u2019ve even validated the fix. \ud83d\ude4c Let\u2019s just keep doing that and I\u2019m sure the client will feel totally reassured.",
    "ground_truth_flags": [
      "Fake",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Non-Objective",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed the \"Sarcastic\" flag (false negative) and added an unnecessary \"Vagueness\" flag (false positive) given the message\u2019s primary issue is tone, not lack of actionable specificity.",
    "why": "The model correctly detected passive-aggressive intent but collapsed explicit sarcasm into the broader passive-aggressive category instead of also flagging it as its own surface-level rhetorical device. It then over-weighted a generic heuristic (\u201casking for details\u201d => vagueness) while ignoring the surrounding DM context that already contains extensive concrete details (symptoms, timeline, links, runbook, requested steps). In this situation, \u201csend me some details\u201d functions as a snide jab implying Priya hasn\u2019t provided enough, not as a genuine underspecified request that blocks execution.",
    "failure_pattern": "sarcasm-collapsed-into-other-flag-plus-contextual-overflagging",
    "severity": "major",
    "id": 34,
    "message": "Awesome, another totally-not-urgent fire drill \ud83d\ude43. I\u2019ll just magically drop school pickup and jump on it\u2014send me *some* details when you have a sec.",
    "ground_truth_flags": [
      "Sarcastic",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Vagueness",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as Passive-Aggressive instead of the expected Rudeness (wrong label selection; it did not include the correct rudeness flag).",
    "why": "The model anchored on the dismissive idiom (\u201cwhole science project\u201d) and interpreted it as indirect sniping (passive-aggression), but the message is actually a blunt, openly dismissive shutdown of others\u2019 concerns (\u201cJust ship the fix as-is\u201d). There\u2019s no veiled or backhanded politeness; it\u2019s direct minimizing language that violates the team\u2019s norm of keeping critique respectful\u2014especially toward a new/junior teammate under pressure. In other words, the model overfit to a tone cue commonly associated with passive-aggression and underweighted the directness + disrespect that makes this plain rudeness in context.",
    "failure_pattern": "passive-aggression-vs-rudeness-confusion",
    "severity": "major",
    "id": 41,
    "message": "Can we not turn this into a whole science project? Just ship the fix as-is so we can hit the deadline.",
    "ground_truth_flags": [
      "Rudeness"
    ],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "The model missed the expected \u201cRudeness\u201d flag (false negative) and instead added \u201cNon-Objective\u201d (arguably an overreach/misclassification relative to the expected set). It correctly identified \u201cPassive-Aggressive,\u201d but failed to capture the direct insulting/accusatory tone that constitutes rudeness in this context.",
    "why": "The message contains explicit blame and contemptuous framing (\u201cnobody bothered to loop design in,\u201d \u201cagain,\u201d \u201cIf that\u2019s how we\u2019re working, tell me now,\u201d \u201cstop wasting time\u201d), which goes beyond passive-aggressive needling into openly rude/hostile language. The model latched onto a single rationale\u2014\u2018assumes intent\u2019\u2014to justify \u201cNon-Objective,\u201d treating accusatory wording primarily as a factuality/objectivity issue rather than a civility/tone violation. In doing so, it underweighted channel and social context (#random banter, high-status inner circle, risky interruption), where the same phrasing reads as a public call-out/shaming move. It also appears to conflate \u2018mind-reading/intent attribution\u2019 with the primary harm category, when the primary harm is interpersonal disrespect/abrasiveness.",
    "failure_pattern": "missed-rudeness-overindexed-on-non-objective",
    "severity": "major",
    "id": 42,
    "message": "Cool, so we just changed launch scope in a mystery Zoom again and nobody bothered to loop design in. If that\u2019s how we\u2019re working, tell me now so I can stop wasting time on screens we\u2019re not shipping.",
    "ground_truth_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ],
    "predicted_flags": [
      "Non-Objective",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed the expected \u201cMicroaggressive\u201d flag (under-flagged by substituting \u201cPassive-Aggressive\u201d and \u201cRudeness,\u201d but did not identify the specific microaggression present).",
    "why": "The model correctly detected hostility, but it collapsed the nuance into generic tone categories. The line \u201cmaybe let one of the engineers walk you through how the checkout flow actually works\u201d implies Maya (a QA lead) lacks basic understanding and positions \u201cengineers\u201d as the real authorities\u2014an expertise/role-based belittlement commonly treated as microaggressive, especially in cross-functional settings. It also contains a subtle gatekeeping/diminishing move (\u201cbefore you light up #cross-functional again\u201d) that polices her participation and frames responsible risk-flagging as \u201cpanic,\u201d which is a minimizing stereotype-coded rebuke rather than merely \u2018rude.\u2019 The model likely prioritized surface linguistic cues (sarcasm, dismissive phrasing) and did not map the power/role dynamics and professional invalidation to the microaggression taxonomy, even though the context (leadership monitoring, recent \u2018QA slows releases\u2019 friction) amplifies the microaggressive impact.",
    "failure_pattern": "microaggression-misclassified-as-general-rudeness",
    "severity": "major",
    "id": 44,
    "message": "Maya, maybe let one of the engineers walk you through how the checkout flow actually works before you light up #cross-functional again \u2014 it\u2019s a pretty standard refactor and we don\u2019t need panic over every edge case.",
    "ground_truth_flags": [
      "Microaggressive"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Missed a required flag: it correctly flagged Passive-Aggressive but failed to flag the message as Fake.",
    "why": "The model latched onto the overt sarcasm/backhanded compliment (which clearly supports Passive-Aggressive) and treated the opening praise as merely sarcastic rather than also \u201cinauthentic politeness.\u201d In this framework, the exaggerated compliment (\u201cAmazing call-out\u2026 truly love how you keep us all on process\u201d) is not genuine appreciation but a performative/insincere veneer used to criticize Theo, which fits the Fake flag in addition to Passive-Aggressive. The model also appears to apply a narrower interpretation of Fake (e.g., only fabricated facts/claims) instead of recognizing fake-nice praise as a form of inauthenticity\u2014especially salient given the public #random setting where tone-policing via \u2018compliments\u2019 can be a social maneuver.",
    "failure_pattern": "missed-fake-flag-in-sarcasm",
    "severity": "major",
    "id": 46,
    "message": "Amazing call-out in #random, Theo \u2014 truly love how you keep us all on process while the client\u2019s checkout is literally breaking \ud83d\ude43. Anyway, when you have a minute between guarding sprint scope, can you please just confirm it\u2019ll be fixed before the 10am call tomorrow?",
    "ground_truth_flags": [
      "Fake",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed the expected \"Condescending\" flag and instead flagged only \"Passive-Aggressive\" (incorrect flag selection relative to the rubric/expectation).",
    "why": "The message\u2019s primary problematic trait is condescension: it talks down to the dev (\"moving fast is your thing\" + \"pretty basic checkout regression\") and implies the dev hasn\u2019t done a basic sanity check (\"might be worth running through the flow\"). The model anchored on the presence of a jab and downstream consequence (\"so Sales doesn\u2019t have to explain why the demo breaks\") and mapped that to passive-aggression. But passive-aggression typically involves indirectness, avoidance, or insinuation instead of stating the issue plainly; here the speaker is fairly direct and evaluative, using belittling phrasing. The model also underweighted the channel context (#sales, public audience) where the phrasing reads more like public shaming/talking-down than an oblique hint\u2014strengthening the condescending classification over passive-aggressive.",
    "failure_pattern": "condescension-vs-passive-aggression-misclassification",
    "severity": "major",
    "id": 48,
    "message": "Hey @marcus, I know moving fast is your thing, but this one\u2019s a pretty basic checkout regression \u2014 might be worth running through the flow before we ship promises to prospects. Can you please take a look and fix it ASAP so Sales doesn\u2019t have to explain why the demo breaks?",
    "ground_truth_flags": [
      "Condescending"
    ],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Both: it missed the expected \"Fake\" flag and incorrectly added \"Non-Objective\" (while correctly flagging \"Passive-Aggressive\").",
    "why": "The message contains a clearly insincere/ironic \"compliment\" (\"Awesome escalation\") used to criticize, which fits the \"Fake\" category in many taxonomies (performative praise / faux-positive framing). The model instead mapped that insincerity to \"Non-Objective,\" treating sarcastic evaluative language as an evidence/subjectivity problem. That\u2019s a category confusion: sarcasm/feigned positivity is not primarily an objectivity violation. The high-context workplace thread (process enforcement + frustration) makes the sarcasm obvious, but the model focused on 'unqualified judgment' phrasing and defaulted to a generic 'non-objective opinion' label rather than the more specific 'fake/insincere' label.",
    "failure_pattern": "label-confusion-fake-vs-non-objective",
    "severity": "major",
    "id": 51,
    "message": "Awesome escalation, Maya \u2014 really love how we\u2019re all just reinventing the on-call process in public \ud83d\ude43. Super helpful timing while we\u2019re already handling an incident.",
    "ground_truth_flags": [
      "Fake",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Non-Objective",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as \"Rudeness\" instead of the expected \"Condescending\" \u2014 a misclassification of the tone category (wrong flag, not a missed detection of negativity).",
    "why": "The message contains belittling framing and status-positioning rather than overt insult/profanity: \"not a coding workshop\" and the quotes around \"do it right\" signal patronizing dismissal. The model likely over-indexed on the presence of sharp language and public shaming cues and mapped them to a generic \"rudeness\" bucket, without distinguishing the more specific mechanism (condescension: talking down, implying immaturity/naivete, reducing the other\u2019s proposal to a hobbyist exercise). Channel context (#client-updates, CEO watching, client waiting) amplifies the face-threatening, hierarchical feel, but the core violation is still condescension rather than simple rudeness; the model treated the bite as general discourtesy instead of a patronizing put-down.",
    "failure_pattern": "confusing-condescension-with-rudeness",
    "severity": "minor",
    "id": 52,
    "message": "Jordan, I get that you want to \"do it right,\" but this is a production fire drill, not a coding workshop. Please stick to the feature-flag hotfix so we can give the client an ETA today.",
    "ground_truth_flags": [
      "Condescending"
    ],
    "predicted_flags": [
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Missed the expected flags for Microaggressive and Non-Objective, and added Passive-Aggressive instead. Rudeness was correctly flagged.",
    "why": "The model latched onto the snarky phrasing and mapped it to Passive-Aggressive, but the message is actually fairly direct (openly dismissive), not \u201cindirect\u201d or veiled. It also failed to recognize two key aspects: (1) Microaggression here is the belittling/invalidating framing of accessibility work as a quoted, eye-rolled \u201ctweak\u201d and the gendered-coded/condescending \u2018precious about pixels\u2019 trope\u2014minimizing a legitimate compliance/accessibility concern and implicitly stereotyping design priorities; (2) Non-Objective is present because it attributes a subjective motive/character judgment (\u201cyou\u2019re being kinda precious\u2026 again\u201d) rather than sticking to verifiable constraints (capacity, risk, timeline). In short, it focused on tone markers (quotes, \u2018yet another\u2019, \u2018again\u2019) and underweighted the targeting/attribution and dismissive minimization of accessibility in this context.",
    "failure_pattern": "misclassifying-direct-dismissal-as-passive-aggressive-and-missing-microaggression",
    "severity": "major",
    "id": 53,
    "message": "Can we not derail this with yet another \u201caccessibility tweak\u201d at the last minute? Feels like you\u2019re being kinda precious about pixels again \u2014 we need to ship for Friday.",
    "ground_truth_flags": [
      "Microaggressive",
      "Non-Objective",
      "Rudeness"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: it added a \"Non-Objective\" flag that wasn\u2019t expected. The only intended flag was \"Fake.\"",
    "why": "The model treated a common leadership reassurance (\u201cI\u2019m sure it\u2019ll be totally fine in prod\u201d) as an \u2018unsupported factual certainty\u2019 and mapped that to Non-Objective. In this Slack outage context, the problem is less about objectivity/subjective truth-claims and more about tone: performative praise + casual overconfidence that can read as insincere or risky. It overgeneralized a heuristic (certainty \u2192 non-objective) and didn\u2019t align with the flag taxonomy as used here, where Non-Objective is likely meant for unverifiable factual claims or misleading assertions rather than motivational language and approval. In short: legitimate managerial/colloquial phrasing was misclassified as a separate policy violation instead of being captured under the existing \u201cFake/performative\u201d concern.",
    "failure_pattern": "overflagging-confidence-as-non-objective",
    "severity": "minor",
    "id": 54,
    "message": "Amazing work, team \u2014 love the energy here \ud83d\ude4c Just go ahead and push the hot patch as-is; I\u2019m sure it\u2019ll be totally fine in prod.",
    "ground_truth_flags": [
      "Fake"
    ],
    "predicted_flags": [
      "Non-Objective",
      "Fake"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: added a \"Vagueness\" flag even though the context already disambiguates what \u201cthe numbers\u201d refers to (board-facing metrics), so only \"Circular\" was warranted.",
    "why": "The model latched onto the surface form \"the numbers\" in the target message and evaluated it in isolation, rather than grounding it in the immediately preceding thread where leaders explicitly scope the domain (board metrics) and even enumerate owners by metric area (RevOps, Growth, Product Analytics). In that context, the problem isn\u2019t unclear referent/data set; it\u2019s repetitive, non-progressing phrasing. The model therefore over-applied a generic vagueness heuristic instead of recognizing that the ask is contextually specific but rhetorically circular.",
    "failure_pattern": "context-not-considered-overflagging",
    "severity": "minor",
    "id": 55,
    "message": "I\u2019m still not sure who owns the numbers because nobody has said who owns the numbers, and until someone tells me who owns the numbers I can\u2019t know who owns the numbers\u2014so can someone just say who owns the numbers?",
    "ground_truth_flags": [
      "Circular"
    ],
    "predicted_flags": [
      "Vagueness",
      "Circular"
    ]
  },
  {
    "what_went_wrong": "Missed the expected \"Condescending\" flag and instead flagged \"Passive-Aggressive\"; the message is overtly patronizing rather than indirectly sniping.",
    "why": "The model anchored on a single phrase (\"before we spend more time explaining fundamentals in here\") as a \"veiled jab\" and mapped that to passive-aggression. But the overall delivery is explicitly evaluative and belittling (\"pretty basic stuff\", \"always\", \"fundamentals\")\u2014that\u2019s direct condescension, not indirect hostility. It also appears to treat \"Passive-Aggressive\" as a catch-all for negative tone and didn\u2019t distinguish between (a) indirect, backhanded criticism (passive-aggressive) and (b) overtly demeaning instruction (condescending). Channel context (blunt engineering feedback) may have further biased it toward interpreting the jab as indirect pressure rather than straightforward patronizing critique.",
    "failure_pattern": "condescension-misclassified-as-passive-aggression",
    "severity": "major",
    "id": 57,
    "message": "Leo, this is pretty basic stuff \u2014 webhook handlers *always* need idempotency, clear logging, and tests for retries/timeouts. Please take another pass and add the missing coverage before we spend more time explaining fundamentals in here.",
    "ground_truth_flags": [
      "Condescending"
    ],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: it added an extra \"Rudeness\" flag that wasn\u2019t expected. It did not miss the expected flags (Passive-Aggressive, Microaggressive), but it failed by over-predicting an additional category.",
    "why": "The model likely treated the message\u2019s blunt, critical phrasing (e.g., \u201crubber-stamp it,\u201d \u201crun this through the actual process\u201d) as generic incivility and mapped that to a broad \"Rudeness\" label, instead of recognizing that the harm is already captured by the more specific categories expected here: Passive-Aggressive (snide/sarcastic framing, indirect contempt) and Microaggressive (diminishing \u201csomeone \u2018has a feeling\u2019,\u201d implying irrationality/illegitimacy of the designer\u2019s input). In a high-scrutiny, process-heavy culture and a casual #random channel, the same phrasing reads sharper, but that context intensifies the passive-aggressive/microaggressive dynamics rather than necessarily requiring a separate \"Rudeness\" tag. The model over-relied on surface sentiment/hostility cues and coarse taxonomy overlap (rude vs passive-aggressive) instead of adhering to the expected label boundaries.",
    "failure_pattern": "overflagging-rudeness-overlap",
    "severity": "minor",
    "id": 60,
    "message": "We really can\u2019t keep redesigning core flows at the last minute because someone \u201chas a feeling\u201d \u2014 accessibility and data integrity aren\u2019t optional. Maybe run this through the actual process next time instead of dropping it in #random and expecting engineering to rubber-stamp it.",
    "ground_truth_flags": [
      "Microaggressive",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Non-Objective",
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Flagged an extra category (Passive-Aggressive) that wasn\u2019t in the expected labels; the correct/expected flag was only Fake.",
    "why": "The model treated the contrast between inflated praise and a hard directive as inherently passive-aggressive, but the policy/labeling expectation here appears to reserve Passive-Aggressive for clearer indirect hostility (e.g., guilt trips, pointed digs, blamey subtext). In this message, the main issue is performative/insincere praise used as a veneer before an urgent ask (\u201cAmazing job\u2026 \ud83d\ude4c \u2014 anyway\u2026\u201d), which fits Fake more than Passive-Aggressive. The model likely over-weighted the sarcastic reading of the opener and the imperative phrasing (\u201cdrop what you\u2019re doing\u201d) without anchoring to the rubric distinction and the incident-response context where blunt urgency is common (even if not ideal).",
    "failure_pattern": "overflagging-passive-aggression-from-fake-praise",
    "severity": "minor",
    "id": 61,
    "message": "Amazing job keeping tomorrow\u2019s launch on track, truly inspiring \ud83d\ude4c \u2014 anyway I need Product + Data + CS to drop what you\u2019re doing and jump on this incident right now, since we can\u2019t afford to miss the renewal call. Just grab whoever you\u2019ve got and start digging; I\u2019ll expect updates in here shortly.",
    "ground_truth_flags": [
      "Fake"
    ],
    "predicted_flags": [
      "Fake",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed an expected flag: the model correctly flagged \"Passive-Aggressive\" but failed to also flag \"Fake.\"",
    "why": "The message contains an overtly insincere compliment (\"Awesome work, Ethan\") paired with blame-laden sarcasm about timing (\"really love the confidence in shipping this right before a Fortune 100 demo\") and an exaggerated request (\"magically make quote generation work again\"). The model recognized the sarcasm as passive-aggression but appears to treat the praise component as merely part of the sarcastic construct rather than categorizing it as \"Fake\" (inauthentic/performative positivity). In other words, it collapsed two distinct signals\u2014feigned praise and passive-aggressive criticism\u2014into a single label. The high-stakes, public #sales context (leadership watching, customer-facing urgency) amplifies that the \"Awesome work\" is not genuine gratitude but a rhetorical jab, which should trigger the \"Fake\" flag as well.",
    "failure_pattern": "missed-fake-positive-sarcasm",
    "severity": "major",
    "id": 62,
    "message": "Awesome work, Ethan \u2014 really love the confidence in shipping this right before a Fortune 100 demo. If you can magically make quote generation work again in the next hour, that\u2019d be amazing \ud83d\ude4f",
    "ground_truth_flags": [
      "Fake",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Both: it missed the expected Rudeness flag and instead added Passive-Aggressive. The message is overtly dismissive/condescending (\u201cLet\u2019s not overthink this,\u201d \u201cobviously,\u201d \u201cinfra purity\u201d) rather than indirect/passive-aggressive.",
    "why": "The model appears to have mapped any attempt to shut down debate (\u201ctake it to a thread somewhere else\u201d) to Passive-Aggressive without distinguishing between indirect sniping vs. direct rudeness. It also underweighted channel context (#client-updates with a client CSM present and leadership demanding client-safe tone), where the phrasing reads as plainly rude and unprofessional, not merely \u2018curt.\u2019 The violation isn\u2019t subtle: the dismissive framing and belittling wording are explicit, so the error likely comes from taxonomy confusion (Passive-Aggressive vs. Rudeness) rather than lack of evidence.",
    "failure_pattern": "passive-aggressive-vs-rude-confusion",
    "severity": "major",
    "id": 63,
    "message": "Let\u2019s not overthink this \u2014 the hotfix behind the flag is obviously the right move. If you want to keep debating infra purity in here, take it to a thread somewhere else.",
    "ground_truth_flags": [
      "Non-Objective",
      "Rudeness"
    ],
    "predicted_flags": [
      "Non-Objective",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Both: it missed the expected \"Condescending\" flag, and it over/incorrectly added \"Non-Objective\" (and arguably mislabeled the primary tone issue as generic \"Rudeness\" rather than the more precise \"Condescending\").",
    "why": "The core violation is a belittling, superiority-asserting framing (\"deadlines like they\u2019re real, not suggestions\") that talks down to the recipient\u2014i.e., condescension. The model instead treated that as an evidentiary/accuracy problem (\"Non-Objective\"), as if the main issue were an unprovable claim about Jordan\u2019s mindset. In this setting, the sentence is rhetorical scolding, not a factual assertion intended to be verified, so \"Non-Objective\" is a misfire. It also collapsed tone distinctions: while the message is rude/confrontational, the sharper label is condescension because it implies Jordan is irresponsible or childish and needs to be reminded how deadlines work. The public #general channel heightens the interpersonal impact, but the model didn\u2019t use that context to refine the flag choice toward the specific social dynamic (talking down publicly) versus a content-objectivity critique.",
    "failure_pattern": "tone-label-confusion-condescension-vs-rudeness",
    "severity": "major",
    "id": 65,
    "message": "Jordan, I need you to treat deadlines like they\u2019re real, not suggestions. Please get the copy updates in by 11 so design can do their job without waiting on you again.",
    "ground_truth_flags": [
      "Condescending"
    ],
    "predicted_flags": [
      "Non-Objective",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as Passive-Aggressive instead of the expected Dismissive (wrong label; effectively a misclassification rather than a missed-and-extra flag).",
    "why": "The model overweighted the presence of indirect phrasing (\u201cCan we not\u2026\u201d) and a conformity cue (\u201clike everyone else\u201d) as evidence of passive-aggression, but the core pragmatic act is overt dismissal/shutdown: it invalidates Maya\u2019s concern about missing decision context and blocks further discussion in #random. In this scenario, the harm is primarily minimizing and gatekeeping information (\u201cjust wait\u2026\u201d) rather than veiled hostility. The model also underused channel/situational context: #random is being used as an informal overflow for a real work-impacting decision; Maya raises an urgent, legitimate need. The reply\u2019s function is to silence and exclude, which aligns more with Dismissive than Passive-Aggressive.",
    "failure_pattern": "dismissive-vs-passive-aggressive-confusion",
    "severity": "major",
    "id": 66,
    "message": "Can we not turn #random into a whole thread about being \u201cleft out\u201d? If you need the details, just wait for the #product post like everyone else.",
    "ground_truth_flags": [
      "Dismissive"
    ],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: it added a \"Non-Objective\" flag that wasn\u2019t expected; the only intended issue was \"Circular.\"",
    "why": "The model treated a forcefully stated design judgment (\"feels finished\") as an objectivity violation, conflating persuasive/valuative language with a requirement for evidence. In this context, the team is debating scope and client-facing polish\u2014subjective craft claims are normal inputs, not inherently a \"Non-Objective\" communication problem. The actual communication defect is the argument structure: it\u2019s tautological (finished because finished), offers no new criteria (user impact, brand risk, acceptance criteria, measurable quality bar), and doesn\u2019t engage the stated constraints (Friday lock, QA risk). The model over-weighted tone/absoluteness and under-weighted the taxonomy boundary: \"Non-Objective\" should be reserved for cases where objectivity is required (e.g., reporting status/metrics) or where factual claims are misrepresented, not for value-based disagreement.",
    "failure_pattern": "overflagging-subjective-opinion-as-non-objective",
    "severity": "minor",
    "id": 68,
    "message": "We can\u2019t cut the interactions because they\u2019re what make it feel finished, and if we cut what makes it feel finished then it won\u2019t be finished. If we ship something that doesn\u2019t feel finished, we\u2019re basically choosing to ship unfinished work.",
    "ground_truth_flags": [
      "Circular"
    ],
    "predicted_flags": [
      "Non-Objective",
      "Circular"
    ]
  },
  {
    "what_went_wrong": "Missed the expected \"Microaggressive\" flag; it only flagged \"Rudeness\" even though the message contains infantilizing/status-demeaning language directed at a colleague (\"hand-holding,\" \"so the adults can jump in\").",
    "why": "The model appears to treat \"microaggression\" narrowly (often tied to protected-class identity cues) and failed to apply it to workplace status-based belittling. In this context, the CTO\u2019s phrasing creates a power-laden, dismissive hierarchy (adults vs. others) and shames the on-call engineer, which fits microaggressive behavior (subtle/normalized put-downs, condescension) in addition to overt rudeness. The model correctly caught the overt insult but didn\u2019t recognize the condescending, infantilizing framing as a separate microaggression category, likely collapsing it into a single rudeness label.",
    "failure_pattern": "microaggression-category-too-narrow",
    "severity": "major",
    "id": 70,
    "message": "Alright, we need everyone to stay on this tonight\u2014no more cowboy deploys. Also, next time you\u2019re on-call, please don\u2019t just sit on an alert because you\u2019re \u201cnot sure\u201d what it means; if you need hand-holding, say so sooner so the adults can jump in.",
    "ground_truth_flags": [
      "Microaggressive",
      "Rudeness"
    ],
    "predicted_flags": [
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "The model incorrectly flagged \"Vagueness\" and missed the expected \"Non-Objective\" flag. It did correctly flag \"Rudeness.\"",
    "why": "The message\u2019s main additional violation (beyond rudeness) is non-objective, evaluative language: \"do the obvious fix\" and \"I can\u2019t keep translating\" are subjective judgments about competence and what is \u201cobvious,\u201d rather than concrete, verifiable instructions. The model instead mapped that imprecision to \"Vagueness\"\u2014but in this context there is already substantial concrete incident detail in the DM thread (service, deploy, dashboards, suspected cause, partial mitigation, affected customers). The instruction to \"follow the thread\" leverages that context, so the primary issue is less about missing operational specifics and more about subjective/condescending framing and blame. In short, it over-weighted the surface form of an urgent directive (\"ASAP\", \"obvious fix\") as vagueness and under-recognized it as non-objective/loaded language.",
    "failure_pattern": "non-objective-misclassified-as-vagueness",
    "severity": "major",
    "id": 71,
    "message": "I need you to just jump on this and get it stable ASAP \u2014 I can\u2019t keep translating what\u2019s happening for you, so please follow the thread and do the obvious fix. Let me know when it\u2019s done.",
    "ground_truth_flags": [
      "Non-Objective",
      "Rudeness"
    ],
    "predicted_flags": [
      "Vagueness",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: the model added an extra \"Non-Objective\" flag when only \"Circular\" was expected.",
    "why": "The message is indeed circular/tautological, but the model treated a normative recommendation (\"Kafka is the reliable choice\") as an objectivity violation. In this context, the primary issue is reasoning quality (no new evidence, self-referential justification), not tone, bias, or subjective language per se. Engineering threads often contain assertions and preferences; without a clear rubric that equates unsupported claims with \"Non-Objective,\" the safer interpretation is that the objectionable feature is the circular argument. The model likely over-weighted the org\u2019s norm of backing assumptions with data and conflated \"unsupported\" with \"non-objective,\" resulting in overflagging.",
    "failure_pattern": "overflagging-unsupported-assertion-as-non-objective",
    "severity": "minor",
    "id": 73,
    "message": "We keep going in circles here: we need Kafka because we need something reliable, and reliability is exactly why we need Kafka. If we want this to be reliable beyond next Friday, Kafka is the reliable choice, so the answer is still Kafka.",
    "ground_truth_flags": [
      "Circular"
    ],
    "predicted_flags": [
      "Non-Objective",
      "Circular"
    ]
  },
  {
    "what_went_wrong": "Flagged the wrong tone category: it should have flagged Rudeness but instead flagged Passive-Aggressive (while correctly flagging Vagueness).",
    "why": "The message contains explicit dismissiveness and command-like language (\"We don\u2019t have time for another debate\u2014just do...\") that is straightforwardly rude/abrasive in a formal, client-visible incident channel. Passive-aggressive behavior is typically indirect, veiled, or sarcastic; here the aggression is direct and overt. The model likely over-associated any discussion-shutdown phrasing with \"passive-aggressive\" and underweighted channel/context constraints (AM-approved client wording, high scrutiny, cross-functional audience) that make the bluntness a clear professionalism/tone violation (rudeness) rather than a passive-aggressive insinuation.",
    "failure_pattern": "rudeness-vs-passive-aggression-confusion",
    "severity": "major",
    "id": 74,
    "message": "We don\u2019t have time for another debate\u2014just do the obvious rollback and get us back within SLA. I\u2019ll worry about the wording for the incident report later.",
    "ground_truth_flags": [
      "Vagueness",
      "Rudeness"
    ],
    "predicted_flags": [
      "Vagueness",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as Passive-Aggressive (incorrect) and missed the expected flags Dismissive and Non-Objective (both missing).",
    "why": "The model latched onto surface cues like \u201cedge case\u201d and \u201cwe\u2019re not going to derail\u201d and treated them as passive-aggression, but the message isn\u2019t indirect, sarcastic, or needling\u2014it\u2019s blunt prioritization. The actual issues are (1) dismissiveness: minimizing a reported regression and deferring action until the client hits it, which invalidates QA concerns in a high-stakes demo context; and (2) non-objectivity: it offers no criteria, risk assessment, or decision rationale (e.g., impact on demo script, likelihood, workaround), just a subjective minimization and a premature decision. The model also underweighted channel context (#client-updates), where the tone and content need to be precise and accountable because account managers may paste it into client comms; \u201cjust ship and circle back if the client actually hits it\u201d is risky and non-actionable for that audience, but that maps to Dismissive/Non-Objective more than Passive-Aggressive.",
    "failure_pattern": "dismissiveness-misclassified-as-passive-aggressive",
    "severity": "major",
    "id": 76,
    "message": "Ok, but this is basically an edge case and we\u2019re not going to derail the 9am demo over it. Let\u2019s just ship and circle back if the client actually hits it.",
    "ground_truth_flags": [
      "Dismissive",
      "Non-Objective"
    ],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly (Non-Objective, Rudeness) and missed the expected flag (Circular). The message\u2019s core issue is repetitive/circular phrasing (\u201cdon\u2019t go quiet/partial fixes\u2026 because\u2026 the problem is the going-quiet/partial-fix pattern\u201d), not incivility or lack of objectivity.",
    "why": "The model over-weighted imperative wording (\u201cWe need you to\u2026\u201d, \u201cdon\u2019t\u2026\u201d) as rudeness without calibrating to Northlake\u2019s stated norm (friendly-but-direct, high-urgency triage). It also treated a reasonable causal framing about process impact (\u201cwe can\u2019t trust the handoff \u2192 late-night hotfixes\u201d) as a non-objective character judgment, when in this context it\u2019s operational risk reasoning, not an unverifiable claim about motives. Meanwhile it under-recognized the actual rhetorical defect: the message restates the same point multiple times with self-referential justification, matching a \u2018circular\u2019 pattern more than a tone/objectivity violation.",
    "failure_pattern": "overflagging-direct-communication",
    "severity": "major",
    "id": 78,
    "message": "We need you to stop going quiet after pushing a partial fix because when you go quiet we can\u2019t trust the handoff, and when we can\u2019t trust the handoff we end up in late-night hotfix mode again \u2014 which is exactly what we need to stop. So the main thing is: don\u2019t go quiet and don\u2019t leave partial fixes, because the problem is the going-quiet/partial-fix pattern.",
    "ground_truth_flags": [
      "Circular"
    ],
    "predicted_flags": [
      "Non-Objective",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Missed the expected \"Dismissive\" flag (under-flagged); it only predicted \"Rudeness\" even though the message explicitly shuts down discussion and invalidates others\u2019 input.",
    "why": "The model correctly detected hostile/commanding language as rudeness, but it likely treated \u201cdismissive\u201d as redundant with \u201crude\u201d instead of separately identifying the conversational move: cutting off debate (\u201cWe\u2019re not reopening UX debates\u201d) and minimizing a colleague\u2019s concern as \u201cspinning,\u201d which is classic dismissiveness. It also underweighted the #general context where public shutdowns and blame-tinged framing amplify the dismissive impact (especially with known cross-functional tension and active UX vs scope disagreement).",
    "failure_pattern": "missed-dismissiveness-as-distinct-from-rudeness",
    "severity": "major",
    "id": 79,
    "message": "We\u2019re not reopening UX debates in #general. Ship what\u2019s in the ticket and stop spinning this into a design postmortem right now.",
    "ground_truth_flags": [
      "Dismissive",
      "Rudeness"
    ],
    "predicted_flags": [
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as Passive-Aggressive instead of the expected Dismissive, i.e., it chose the wrong toxicity category rather than missing that the message needed a flag at all.",
    "why": "The model overweighted the rhetorical question (\u201cCan we not\u2026\u201d) as an indicator of passive-aggression, but the message\u2019s primary function is to shut down input and minimize the design concern (\u201cJust ship whatever you already built and we\u2019ll deal with the rest later\u201d). There\u2019s no clear hallmark of passive-aggression (e.g., veiled sarcasm, indirect insult, backhanded praise, or sweet-on-the-surface wording masking hostility). In #general\u2014explicitly high-signal and calm/inclusive\u2014the directive to stop discussion and defer quality/requirements reads as dismissive and conversation-stifling rather than indirectly sniping. The model conflated \u2018curt shutdown\u2019 with passive-aggression instead of recognizing it as straightforward dismissal.",
    "failure_pattern": "dismissiveness-misclassified-as-passive-aggressive",
    "severity": "minor",
    "id": 80,
    "message": "Can we not turn #general into another design debate? Just ship whatever you already built and we\u2019ll deal with the rest later.",
    "ground_truth_flags": [
      "Dismissive"
    ],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "The model predicted the wrong flag set: it labeled the message as \"Rudeness\" but missed the more specific expected categories \"Condescending\" and \"Non-Objective\" (i.e., it under-specified and miscategorized the issue rather than identifying the intended dimensions).",
    "why": "The message\u2019s primary violations are (1) condescension/dismissal (\"platform purity lecture,\" \"making this way harder\") and (2) non-objective argumentation from authority/seniority (\"I\u2019ve shipped fixes like this here for years\") instead of addressing Marcus\u2019s concrete risk questions (hit rate, TTL, invalidation, reviewer). The model appears to have collapsed multiple nuanced flags into a single coarse bucket (\u201crudeness\u201d), likely because it focused on tone-level abrasiveness and personal targeting while not distinguishing between: dismissive superiority (condescension) and reasoning quality (non-objective). It also didn\u2019t use the surrounding context that Marcus is raising specific engineering concerns aligned with a postmortem norm; Priya\u2019s reply sidesteps those specifics and appeals to time pressure and tenure\u2014key cues for the expected labels.",
    "failure_pattern": "flag-granularity-collapse-into-rudeness",
    "severity": "major",
    "id": 82,
    "message": "Marcus, you\u2019re making this way harder than it needs to be \u2014 we don\u2019t have time for a platform purity lecture. I\u2019ve shipped fixes like this here for years; let\u2019s just add the Redis cache and move on.",
    "ground_truth_flags": [
      "Condescending",
      "Non-Objective"
    ],
    "predicted_flags": [
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as Passive-Aggressive instead of the expected Dismissive (misclassified tone; did not include the Dismissive flag).",
    "why": "The content is primarily minimizing and shutting down legitimate concerns (\"not turn this into a whole thing,\" \"just handle it and move on\"), which maps cleanly to Dismissive. The model treated the curt, conversation-stopping directive as passive-aggression, but there\u2019s no clear indirect jab, insinuation, or veiled sarcasm aimed at a person\u2014it's blunt de-prioritization. Context (team asking for RCA/config clarity ahead of a high-stakes demo) makes the dismissiveness more salient: it discourages necessary coordination rather than expressing resentment indirectly. In a casual, meme-heavy #random channel, the model likely over-weighted the snarky environment and interpreted brevity as passive-aggressive style rather than straightforward dismissal.",
    "failure_pattern": "cultural-style-misclassified",
    "severity": "major",
    "id": 83,
    "message": "Can we not turn this into a whole thing? The demo\u2019s tomorrow\u2014just handle it and move on.",
    "ground_truth_flags": [
      "Dismissive"
    ],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed a required flag: the message should have been flagged as both Rudeness and Microaggressive, but the model only predicted Rudeness.",
    "why": "The model recognized overt condescension and shaming (hence Rudeness) but failed to map the identity/status-based dig to a microaggression. The phrase \u201cI get you\u2019re new\u201d leverages seniority/newness as a disparaging attribute to dismiss the junior engineer\u2019s input (\u201cinstead of improvising and arguing every comment\u201d), which is a subtle belittlement tied to role/tenure. In this high-formality, audit-focused channel, that status-based put-down is especially salient because it publicly undermines credibility and signals exclusionary hierarchy. The model likely treated the \u2018new\u2019 remark as mere blunt feedback rather than a microaggressive, status-targeted dismissal amplified by the public setting.",
    "failure_pattern": "missed-status-based-microaggression",
    "severity": "major",
    "id": 85,
    "message": "Look, I get you\u2019re new, but this kind of shortcut is exactly how we end up back in incident mode \u2014 please just stick to the standards instead of improvising and arguing every comment.",
    "ground_truth_flags": [
      "Microaggressive",
      "Rudeness"
    ],
    "predicted_flags": [
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Missed the expected \u201cPassive-Aggressive\u201d flag (under-flagging). It correctly flagged \u201cFake\u201d but failed to also recognize the message as a sarcastic, undermining reply in a public channel.",
    "why": "The model latched onto surface-level insincerity cues (performative praise, emoji, scare quotes) and treated them primarily as \u201cFake,\u201d but did not connect those same cues to the underlying intent: irony that criticizes Maya\u2019s decision (\u201ccut the flow that\u2019s literally in the renewal deck\u201d) while pretending to applaud it. It also underweighted the channel and power-dynamics context: in #client-updates with Sales/CEO present, this kind of \u2018praise + pointed contradiction\u2019 functions as public pushback and reputational jabbing, a hallmark of passive-aggression. By framing the fix as \u2018remove sarcastic phrasing,\u2019 the model implicitly assumed supportive intent rather than recognizing the message\u2019s adversarial subtext.",
    "failure_pattern": "missing-subtle-passive-aggression",
    "severity": "major",
    "id": 86,
    "message": "Love the decisive direction here, Maya \u2014 super inspiring \ud83d\ude4c Totally makes sense to cut the flow that\u2019s literally in the renewal deck; I\u2019m sure the client will appreciate the \u201clean\u201d experience on Friday.",
    "ground_truth_flags": [
      "Fake",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Fake"
    ]
  },
  {
    "what_went_wrong": "Flagged additional categories (Non-Objective, Passive-Aggressive, Rudeness) instead of the expected single flag (Condescending). In effect, it over-labeled a single tone issue with multiple overlapping toxicity flags.",
    "why": "The message\u2019s core problem is a patronizing, shaming framing (\u201cfor future reference\u2026\u201d, scare quotes around \u201cescalating\u201d, \u201cjust making noise\u201d), which is best captured as Condescending. The model treated the same linguistic cues as evidence of separate violations: (1) it mapped sarcasm/scare quotes to Passive-Aggressive, even though the intent is overtly admonishing rather than indirect; (2) it mapped the public reprimand/dismissive phrasing to Rudeness, even though the primary signal is superiority/lecturing; (3) it mapped \u201cnoise\u201d to Non-Objective, conflating subjective evaluation with a \u2018non-factual claim\u2019 category, despite the task being about communication tone rather than epistemic rigor. It also didn\u2019t sufficiently use the channel norms context (requests should include tickets/repro/links) to see that the actionable part is legitimate and the harm is specifically the belittling delivery, not \u201cobjectivity.\u201d",
    "failure_pattern": "overflagging-overlapping-toxicity-labels",
    "severity": "major",
    "id": 87,
    "message": "Maya, for future reference, blasting #engineering without a ticket, repro steps, or even an invoice ID isn\u2019t actually \u201cescalating,\u201d it\u2019s just making noise. Please file it properly and include the links/logs, and then we can talk about an ETA.",
    "ground_truth_flags": [
      "Condescending"
    ],
    "predicted_flags": [
      "Non-Objective",
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: it added a \"Fake\" flag that wasn\u2019t expected; \"Passive-Aggressive\" was correct.",
    "why": "The model conflated passive-aggressive compliance (sarcastic/quoted phrasing) with deception/inauthenticity. In this message, the \u2018Appreciate the reminder\u2019 line reads as part of the same sarcastic framing rather than a separate attempt to mislead or falsely agree. The model over-interpreted \u2018performative compliance\u2019 as \u2018fake\u2019 without stronger indicators (e.g., contradictory commitments, overtly fabricated claims, or clear praise meant to manipulate). It also appears to treat any insincere-sounding politeness as \u201cFake,\u201d creating overlap with the already-captured passive-aggression signal.",
    "failure_pattern": "overflagging-fake-vs-passive-aggressive",
    "severity": "minor",
    "id": 88,
    "message": "Totally hear you, Maya \u2014 and yeah, I\u2019ll make sure my handoffs are magically clearer across time zones and keep my jokes \u201cappropriately supportive\u201d in #random. Appreciate the reminder.",
    "ground_truth_flags": [
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Fake",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: added a \"Non-Objective\" flag even though the message is primarily (and correctly) circular reasoning; the extra flag is not supported by the content.",
    "why": "The model appears to treat any attribution of causality or critique of another party\u2019s stance (\"it\u2019s a blocker only because QA is blocking it\") as an evidence-free assumption about motives, and therefore \"Non-Objective.\" But in this message, the speaker is not making a verifiable factual claim about logs/behavior; they are disputing the framing/definition of \"blocker\" via a self-referential argument. That is already captured by \"Circular,\" and the \"Non-Objective\" label becomes redundant and over-triggered by the presence of 'QA says' language rather than a true objectivity violation.",
    "failure_pattern": "overflagging-non-objective-when-opinionated",
    "severity": "minor",
    "id": 89,
    "message": "I hear you, but it\u2019s still the same thing: we can\u2019t block launch because QA says it\u2019s a blocker, and it\u2019s a blocker only because QA is blocking it. So we shouldn\u2019t treat it as a blocker since it\u2019s just blocking because we\u2019re treating it like one.",
    "ground_truth_flags": [
      "Circular"
    ],
    "predicted_flags": [
      "Non-Objective",
      "Circular"
    ]
  },
  {
    "what_went_wrong": "Both: it missed the expected flags (Dismissive, Non-Objective) and instead flagged Passive-Aggressive.",
    "why": "The target message is primarily dismissive (explicitly deprioritizes a demo-critical regression: \u201cwe\u2019re not going to chase this right now\u201d) and non-objective (asserts \u201cit worked when I merged it\u201d as a credibility/authority claim rather than engaging with the provided repro + logs). The model latched onto the defensive clause as \u201cveiled blame\u201d and treated the shutdown as passive-aggression, but the language is not indirect or insinuative; it\u2019s overtly dismissive and minimizes QA evidence. It also underweighted the channel context (high-stakes, checkout is explicitly prioritized, repro/logs are concrete), which makes the \u201crerun the suite or file it\u201d response read as dismissal and avoidance rather than a neutral triage decision.",
    "failure_pattern": "mislabeling-dismissive-as-passive-aggressive",
    "severity": "major",
    "id": 90,
    "message": "Yeah, we\u2019re not going to chase this right now \u2014 it worked when I merged it. If it\u2019s still an issue, just rerun the suite or file it and we\u2019ll look after the demo.",
    "ground_truth_flags": [
      "Dismissive",
      "Non-Objective"
    ],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: added a Vagueness flag that wasn\u2019t expected/necessary; Circular was correct.",
    "why": "The model treated a high-tempo, war-room Slack demand (\u201ccustomer-safe narrative\u201d, \u201cASAP\u201d) as an underspecified request, ignoring that the surrounding context already provides scope (NovaBank/Atrium), urgency (call in 35 minutes), and the deliverable type (2\u20133 bullets + status-page sentence). In this channel/culture, the phrasing is intentionally blunt and repetitive for emphasis under incident pressure, not a meaningful lack of detail. So the Vagueness detection overfit to generic heuristics (missing owner/format/deadline) instead of using conversational context to see those constraints are already established.",
    "failure_pattern": "ignoring-conversation-context-overflagging-vagueness",
    "severity": "minor",
    "id": 91,
    "message": "We need a customer-safe narrative now because we need a customer-safe narrative now. Until we have something we can say on calls, we don\u2019t have something we can say on calls, so we need the narrative ASAP.",
    "ground_truth_flags": [
      "Circular"
    ],
    "predicted_flags": [
      "Vagueness",
      "Circular"
    ]
  },
  {
    "what_went_wrong": "Missed two expected flags: it failed to flag the message as Passive-Aggressive and Fake (performative praise + shutdown instruction) and instead returned no flags.",
    "why": "The model over-weighted the literal wording (\u201csupportive\u201d + \u201cclear instruction\u201d) and under-weighted pragmatic intent and channel norms. In this context (#client-updates, Legal watching, exec-ready summary due), the opening gush (\u201cAbsolutely love the energy\u2026 amazing questions for week one \ud83d\ude4c\u201d) reads as performative/inauthentic in a formal audit-friendly channel (Fake), and it precedes a public reprimand/containment (\u201chold off on posting anything else\u2026 so we don\u2019t accidentally reinvent the story\u201d) that implicitly positions Taylor as a risk to narrative control (Passive-Aggressive). The passive-aggression is subtle because there are no overt insults; it\u2019s delivered via exaggerated praise + a sweeping silencing directive, and the model appears not to use situational cues (new hire, high-stakes client narrative, \u2018keep thread clean and authoritative\u2019) to detect that the tone is a face-saving scold rather than neutral guidance.",
    "failure_pattern": "missing-subtle-passive-aggression",
    "severity": "major",
    "id": 92,
    "message": "Absolutely *love* the energy here, Taylor \u2014 amazing questions for week one \ud83d\ude4c. Let\u2019s just hold off on posting anything else in this channel and follow what\u2019s already in the incident doc so we don\u2019t accidentally reinvent the story.",
    "ground_truth_flags": [
      "Fake",
      "Passive-Aggressive"
    ],
    "predicted_flags": []
  },
  {
    "what_went_wrong": "Missed the expected \"Sarcastic\" flag (only flagged \"Passive-Aggressive\"), despite clear sarcastic markers in the target message.",
    "why": "The model correctly recognized the indirect blame/pressure (passive-aggressive) but appears to have collapsed sarcasm into that single category instead of labeling both. The message contains explicit sarcasm cues (\"Awesome, love that\u2026\", \"super chill\", scare quotes around \"stable\", and the \ud83d\ude43 marker), which are strong enough to warrant a separate Sarcastic flag. This looks like a taxonomy/label-boundary issue (treating sarcasm as merely evidence for passive-aggression) rather than a context problem; the DM/fast-blunt culture doesn\u2019t negate that the utterance is still sarcastic.",
    "failure_pattern": "sarcasm-collapsed-into-passive-aggressive",
    "severity": "major",
    "id": 94,
    "message": "Awesome, love that the new checkout occasionally double-charges on mobile \u2014 super chill for tomorrow\u2019s \"stable\" demo \ud83d\ude43. I\u2019ll just keep finding these little surprises unless you want to take a look tonight.",
    "ground_truth_flags": [
      "Sarcastic",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed a required flag: it correctly flagged Rudeness but failed to also flag Dismissive.",
    "why": "The model\u2019s explanation actually identifies dismissiveness (\u201cCan we not turn this into a whole thing\u2026\u201d) but it appears to have collapsed that signal into the broader Rudeness category instead of emitting both labels. In this context (#general is supposed to be status/blockers only, and critique should be private), the phrasing dismisses legitimate coordination/questions and shuts down discussion without addressing the blocker (promo exclusions / decision confirmations). The urgency is real, but the message explicitly minimizes others\u2019 concerns (\u201cwhole thing\u201d) and offers no next step beyond a curt command, which meets the Dismissive criterion separately from general rudeness. This looks like a label-boundary/threshold issue rather than subtlety: the dismissive cue is explicit and even cited in the reasoning, but the multi-label output was incomplete.",
    "failure_pattern": "missing-secondary-flag-label-collapse",
    "severity": "major",
    "id": 95,
    "message": "Can we not turn this into a whole thing right now? Just get it done.",
    "ground_truth_flags": [
      "Dismissive",
      "Rudeness"
    ],
    "predicted_flags": [
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Flagged an extra category (Non-Objective) that wasn\u2019t expected; the correct flag was only Rudeness.",
    "why": "The model treated a blame-leaning, rhetorical line (\u201cwe can\u2019t keep letting other teams\u2019 commitments get in the way\u201d) as a factual, unverifiable assertion requiring a Non-Objective flag. In this context, that clause functions primarily as accusatory framing and coercive pressure\u2014i.e., a tone/rudeness issue\u2014rather than a substantive claim that the message is asking others to accept as a key decision-relevant fact. The model also over-weighted the \u2018asserted as fact without evidence\u2019 heuristic and under-weighted the label taxonomy/intent: Non-Objective is better reserved for cases where the core content hinges on disputed facts or misleading certainty, whereas here the harmfulness is already fully captured by Rudeness (dismissive \u201cnot asking for opinions,\u201d imperative staffing demands, and team-blaming language).",
    "failure_pattern": "overflagging-non-objective-for-blamey-language",
    "severity": "minor",
    "id": 97,
    "message": "I\u2019m not asking for opinions \u2014 I need two bodies from Product + Data and one from CS on this *now*. Pause the launch prep and jump into the war room; we can\u2019t keep letting other teams\u2019 commitments get in the way when prod is on fire.",
    "ground_truth_flags": [
      "Rudeness"
    ],
    "predicted_flags": [
      "Non-Objective",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "The model missed the expected \"Condescending\" flag and instead applied broader categories (\"Rudeness\" and \"Passive-Aggressive\"). It didn\u2019t necessarily flag incorrectly, but it mis-labeled the primary violation the policy expected.",
    "why": "The message\u2019s core issue is talking down to a peer in a public, high-stakes #general thread: \"refactors are fun\" (patronizing), \"grown-up solution\" (infantilizing), and \"stop bikeshedding\" (dismissive). The model correctly sensed negativity, but collapsed a specific interpersonal-power dynamic (condescension/patronizing tone, especially salient given Priya feeling brushed off in front of leadership) into more generic toxicity labels. It also over-attributed \"passive-aggressive\" due to sarcasm, even though the content is fairly direct/aggressive rather than indirect. In an async-first, high-signal #general channel, the public belittling and authority-claiming tone is the key classification nuance the model didn\u2019t prioritize.",
    "failure_pattern": "missed-primary-condescension-misclassified-as-general-rudeness",
    "severity": "major",
    "id": 100,
    "message": "Priya, I get that refactors are fun, but we need the grown-up solution here: ship the flagged patch and stop bikeshedding in #general. If you want to rewrite payments, put together a doc with a real plan and we can look at it next week.",
    "ground_truth_flags": [
      "Condescending"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Flagged the wrong tone category: it should have flagged Dismissive, but instead flagged Passive-Aggressive (while Vagueness was correct).",
    "why": "The message is a direct shutdown of questions and an explicit deprioritization of others\u2019 needs (\"I don't have time\", \"Priya can figure out whatever's left\"), which aligns with Dismissive. Passive-aggressive typically involves indirectness, sarcasm, or veiled criticism; here the criticism is overt and blunt, not insinuated. The model likely over-weighted surface cues like \"back-and-forth\" and \"Let's not overcomplicate this\" as passive-aggression markers without checking whether the communication is actually indirect. It also underused the thread context (Priya asking for clarity, Leo asking for scope) that makes the impact clearly dismissive rather than passive-aggressive.",
    "failure_pattern": "dismissive-vs-passive-aggressive-confusion",
    "severity": "major",
    "id": 104,
    "message": "Leo, I don't have time for the back-and-forth here \u2014 just pick a fix and ship it, and Priya can figure out whatever's left for the demo. Jordan will review when he can. Let's not overcomplicate this.",
    "ground_truth_flags": [
      "Vagueness",
      "Dismissive"
    ],
    "predicted_flags": [
      "Vagueness",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed the expected Rudeness flag: it correctly flagged One-Liner, but failed to identify the message\u2019s dismissive/shutdown tone (\u201cWe\u2019re not reopening this\u2026\u201d, \u201cjust ship\u2026\u201d, \u201cwe\u2019ll deal with tweaks after launch\u201d) as rude in context.",
    "why": "The model appears to treat the message as merely concise and directive (a process/urgency constraint) rather than evaluating the interpersonal impact. It underweighted contextual cues: a teammate (Maya) raised a compliance risk and asked for a sanity-check, and the target message shuts that down while implying others are being unreasonable for discussing it late (\u201cat 11pm\u201d). That combination reads as curt, dismissive, and implicitly scolding\u2014i.e., rudeness\u2014especially in a high-stakes client/compliance thread. The model likely uses a narrow lexical threshold for rudeness (e.g., profanity/insults) and missed that \u201cshutdown + minimization\u201d can constitute rudeness without explicit name-calling.",
    "failure_pattern": "missing-rudeness-from-dismissive-shutdown",
    "severity": "major",
    "id": 105,
    "message": "We\u2019re not reopening this at 11pm\u2014just ship what\u2019s in the doc and we\u2019ll deal with tweaks after launch.",
    "ground_truth_flags": [
      "One-Liner",
      "Rudeness"
    ],
    "predicted_flags": [
      "One-Liner"
    ]
  },
  {
    "what_went_wrong": "Missed the expected \"Microaggressive\" flag (underflagging) while correctly flagging \"Rudeness\"; it also substituted a different label (\"Non-Objective\") for what the rubric expected as microaggression in this context.",
    "why": "The message contains a stereotyped, belittling group swipe\u2014\u201cthe usual sales thing\u201d\u2014that functions as an identity-/role-based put-down of a team (Sales) in a public channel, plus a controlling directive (\u201cplease stop freelancing timelines\u201d) that publicly scolds and attributes negative behavior. The model focused on epistemic framing (generalization/unverified claim) and mapped it to \"Non-Objective,\" but did not recognize that the same language also fits many microaggression definitions used in workplace comms flagging: subtle derogation, stereotyping, and undermining of a group\u2019s professionalism/status. In other words, it treated the problem as \u2018subjectivity\u2019 rather than \u2018demeaning group-based jab,\u2019 likely because it overweighted the literal \u201cunverified claim\u201d heuristic and underweighted pragmatic social harm signals and power dynamics (CTO-to-Sales in #sales right after an incident).",
    "failure_pattern": "microaggression-misclassified-as-non-objective",
    "severity": "major",
    "id": 106,
    "message": "Appreciate the urgency, but let\u2019s not do the usual sales thing where we promise the moon before we\u2019ve even looked at the logs. We\u2019ll share a customer-safe update once we\u2019ve validated the fix and confirmed stability\u2014until then, please stop freelancing timelines in renewals.",
    "ground_truth_flags": [
      "Microaggressive",
      "Rudeness"
    ],
    "predicted_flags": [
      "Non-Objective",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: the model added an extra \"Fake\" flag. The message is clearly passive-aggressive/sarcastic, but not deceptive/impersonating or otherwise \"fake\" in the sense intended by the taxonomy.",
    "why": "The model appears to treat \"Fake\" as a catch-all for perceived insincerity (e.g., sarcastic praise like \u201clove that we\u2019re\u2026\u201d) rather than reserving it for messages that are fabricated, misleading, or inauthentic in a policy-relevant way. It conflated rhetorical sarcasm (already covered by Passive-Aggressive) with a separate violation category, likely because the opening uses performative agreement. Context (a process-driven org and #random being public/manager-visible) reinforces that the tone is risky, but it doesn\u2019t make the content \"fake\"\u2014it\u2019s a sarcastic complaint, not a misrepresentation.",
    "failure_pattern": "sarcasm-misclassified-as-fake",
    "severity": "minor",
    "id": 108,
    "message": "Got it \u2014 love that we\u2019re doing a cross\u2011functional launch review in #random the night before release. I\u2019m sure accessibility and the migration risk will just sort themselves out, so I\u2019ll stop \u201cslowing things down\u201d and you all can ship whatever feels right.",
    "ground_truth_flags": [
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Fake",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged an extra issue: it added \"Vagueness\" when only \"Circular\" was expected. The \"Circular\" flag was correct; the error is overflagging.",
    "why": "The model treated the absence of concrete action items as a separate \"Vagueness\" violation rather than recognizing it as an inherent consequence of the same circular/tautological construction. In this taxonomy/labeling setup, the primary problem is rhetorical circularity (self-referential, non-progressing statements). The model likely double-counted the same defect\u2014non-informativeness\u2014under two labels, instead of selecting the most diagnostic one. Context (high-stakes outage, exec guidance expected) makes the message unhelpful, but that unhelpfulness is already fully explained by the circular phrasing; adding \"Vagueness\" is redundant relative to the expected single-flag decision.",
    "failure_pattern": "redundant-multi-flag-overreach",
    "severity": "minor",
    "id": 109,
    "message": "We\u2019re aligned on the fact that we need alignment: the next step is to focus on next steps, and we\u2019ll get clarity by being clear. Let\u2019s stay focused on focusing so we can move forward and move ahead.",
    "ground_truth_flags": [
      "Circular"
    ],
    "predicted_flags": [
      "Vagueness",
      "Circular"
    ]
  },
  {
    "what_went_wrong": "Flagged an extra category: it predicted Passive-Aggressive in addition to the expected Rudeness.",
    "why": "The target message is primarily a direct, openly confrontational reprimand (dismissive phrasing like \u201ccome with a real plan\u201d and \u201ctagging half Eng\u201d), which fits Rudeness. The model treated the sarcastic/critical wording (\u201cbecause Sales had a rough demo,\u201d \u201clike this\u201d) as a \u2018veiled dig\u2019 and therefore Passive-Aggressive, but the criticism isn\u2019t indirect or masked\u2014it\u2019s explicit. The model likely over-relied on cue phrases commonly associated with passive-aggression (snarky contrast clauses, \u2018instead of\u2026\u2019) without checking whether the communication is actually indirect, and it didn\u2019t sufficiently differentiate \u2018snark/bluntness\u2019 from true passive-aggressive behavior in this channel-context escalation.",
    "failure_pattern": "overflagging-passive-aggression-for-blunt-rebuke",
    "severity": "minor",
    "id": 111,
    "message": "We\u2019re not committing to a date in #random because Sales had a rough demo. If you need an answer, take it to the release thread and come with a real plan and owner list instead of tagging half Eng like this.",
    "ground_truth_flags": [
      "Rudeness"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "The model over-flagged: it correctly flagged Passive-Aggressive, but incorrectly added Rudeness and Non-Objective, which were not part of the expected labeling for this case.",
    "why": "The message\u2019s primary violation is sarcasm and indirect blame (classic passive-aggression). The model likely treated the sarcastic content as (a) an \u201cobjective claims vs evidence\u201d issue, mapping rhetorical exaggeration ('release freeze... optional', 'magically produce an engineer') to Non-Objective, even though it functions as tone/sarcasm rather than a factual assertion needing substantiation; and (b) a separate Rudeness violation because of sharper phrases ('Sales needs a quick win', 'actually own this'). In this scenario, those elements are better interpreted as components of passive-aggression rather than an additional, distinctly rude/abusive attack. Put differently: it double-counted the same tonal signal across multiple categories, and didn\u2019t align with the narrower taxonomy expectation that reserves Rudeness for more overt insults/profanity and Non-Objective for genuinely unverifiable or speculative factual claims (not rhetorical snark).",
    "failure_pattern": "overflagging-passive-aggression-as-multiple-violations",
    "severity": "major",
    "id": 114,
    "message": "Totally fine to keep escalating this in #random \ud83d\udc4d I\u2019m sure the release freeze and sprint planning are just optional when Sales needs a quick win. I\u2019ll go see if I can magically produce an engineer in the next 12 hours unless someone wants to actually own this.",
    "ground_truth_flags": [
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Rudeness",
      "Non-Objective",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Both: it incorrectly flagged the message as Passive-Aggressive and missed the expected flags Non-Objective and Rudeness. The content is a blunt directive/shutdown that\u2019s rude and not grounded in the actual technical/OKR concerns raised, rather than indirect or insinuating passive aggression.",
    "why": "The model over-relied on a heuristic that \u2018shutting down discussion\u2019 maps to Passive-Aggressive when no Dismissive label exists. It ignored key tone and content cues: (1) the message is direct (\u201cJust ship it as-is\u201d), not passive; (2) it contains a scolding framing (\u201canother engineering debate tonight\u201d) that reads as overt rudeness rather than veiled hostility; and (3) it dismisses concrete risk (a11y, migration, rollback, OKR alignment) without providing objective rationale, owner, or plan\u2014making it non-objective in this process-heavy org and especially risky in a high-visibility channel (#random) under incident scrutiny. In short, it misclassified \u2018dismissive + rude\u2019 as \u2018passive-aggressive\u2019 and failed to apply the taxonomy\u2019s closer matches.",
    "failure_pattern": "dismissive-rudeness-misclassified-as-passive-aggressive",
    "severity": "major",
    "id": 117,
    "message": "Can we not turn #random into another engineering debate tonight? Just ship it as-is and we\u2019ll deal with whatever comes up after the release.",
    "ground_truth_flags": [
      "Non-Objective",
      "Rudeness"
    ],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Both: it missed the expected flags (Dismissive, Rudeness) and instead flagged Passive-Aggressive, which is not the best primary label for this message.",
    "why": "The message contains an explicit shutdown and minimization (\u201cCan we not turn this into a whole thing\u2026\u201d) plus an imperative that disregards stated risk (\u201cJust merge it and we\u2019ll deal with any edge cases after the demo.\u201d). Those are direct markers of Dismissiveness (silencing/invalidating concerns) and Rudeness (abrupt, impatient tone in a high-stakes context). The model over-indexed on a generic \u2018shuts down discussion\u2019 heuristic and mapped it to Passive-Aggressive, but the phrasing is not indirectly hostile or insinuating; it\u2019s overtly dismissive and blunt. It also underweighted the immediate context: Leo raised a concrete quality/safety concern tied to a prior incident, and the target message explicitly deprioritizes that, increasing the interpersonal harshness. In a fast, candid #sales culture, directness is normal, but this crosses into dismissive/rude because it explicitly frames the discussion as \u2018a whole thing\u2019 and instructs to ignore edge cases until after a client demo.",
    "failure_pattern": "passive-aggression-overattribution-missed-dismissive-rude",
    "severity": "major",
    "id": 118,
    "message": "Can we not turn this into a whole thing in #sales? Just merge it and we\u2019ll deal with any edge cases after the demo.",
    "ground_truth_flags": [
      "Dismissive",
      "Rudeness"
    ],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: added a \"Non-Objective\" flag that isn\u2019t warranted; \"Circular\" was correct.",
    "why": "The model treated a confident causal claim (\u201cit can\u2019t be from my refactor\u201d) as inherently non-objective. But in this taxonomy, \"Non-Objective\" should target subjective judgments, opinions, or value-laden statements, not merely an unsupported or overly certain technical assertion. The message is objectively framed (a causal attribution about code changes), albeit potentially wrong/overconfident. The model also ignored that fast-moving engineering Slack often contains terse, declarative triage language; lack of evidence in the message doesn\u2019t automatically make it non-objective\u2014just unsubstantiated. The only clear issue present is circular reasoning/repetition (using the premise \u201cdidn\u2019t touch checkout\u201d to conclude \u201ccan\u2019t be from my refactor,\u201d then restating it).",
    "failure_pattern": "overflagging-unsupported-as-non-objective",
    "severity": "minor",
    "id": 119,
    "message": "I already said the bug isn\u2019t from my refactor because the refactor didn\u2019t touch checkout, and since it didn\u2019t touch checkout, it can\u2019t be from my refactor. So yeah, it\u2019s not from my refactor.",
    "ground_truth_flags": [
      "Circular"
    ],
    "predicted_flags": [
      "Non-Objective",
      "Circular"
    ]
  },
  {
    "what_went_wrong": "Missed the expected \"Dismissive\" flag (under-flagging). It correctly flagged \"Rudeness\" but failed to also identify the explicit dismissal of Priya\u2019s request and process.",
    "why": "The model appears to have treated the dismissive elements (\"just handle it and follow the runbook\") as merely evidence supporting rudeness, rather than a distinct communicative behavior: shutting down collaboration and refusing engagement. In this DM context\u2014where Priya is urgently delegating and asking for updates\u2014the message not only sounds rude but also explicitly brushes off responsibility and minimizes the request. The model likely collapsed overlapping signals (rude + dismissive) into a single label, focusing on hostility/frustration and overlooking that the speaker is also disengaging and invalidating the urgency/coordination ask.",
    "failure_pattern": "collapsed-overlapping-flags",
    "severity": "major",
    "id": 120,
    "message": "I\u2019m literally offline right now \u2014 just handle it and follow the runbook. I can\u2019t keep dropping everything every time this happens.",
    "ground_truth_flags": [
      "Dismissive",
      "Rudeness"
    ],
    "predicted_flags": [
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: it added a Vagueness flag even though the expected (and primary) issue is Circular repetition; the target message is not meaningfully vague given the task context (deadline, dependency, and consequence are explicit).",
    "why": "The model applied a generic rubric that treats the absence of explicit next-step fields (ETA request, owner, workaround, acceptance criteria) as \u201cvagueness,\u201d without weighing whether the message\u2019s intent is actually unclear. In this thread, the specific ask is already established (ship ENG-7721 / provide a safe alternative path) and the target message is clearly an (albeit unhelpful) reiteration of urgency. The model also failed to distinguish 'unactionable/pressuring repetition' from true vagueness: the problem is redundancy and escalation pressure in a sales-facing channel, not lack of clarity about what is being requested.",
    "failure_pattern": "overflagging-vagueness-for-unactionability",
    "severity": "minor",
    "id": 121,
    "message": "We need this integration fix before Thursday because the renewal depends on it, and since the renewal depends on it we need it before Thursday. I get it\u2019s \u201chigh-risk,\u201d but it still needs to be done before Thursday because otherwise we can\u2019t commit\u2014so we need it before Thursday.",
    "ground_truth_flags": [
      "Circular"
    ],
    "predicted_flags": [
      "Vagueness",
      "Circular"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as \"Rudeness\" instead of the expected \"Dismissive\" (wrong label/category rather than a missed detection).",
    "why": "The model correctly noticed the minimizing/pressuring language (\"don\u2019t make this a whole thing,\" \"just jump on\") but mapped it to a broader, harsher class (\u201cRudeness\u201d) instead of the more precise behavior being exhibited: dismissal of the other person\u2019s concerns and shutting down discussion. It likely over-weighted the imperative/commanding phrasing and the emotionally charged framing (\u201cwe don\u2019t have time for a debate\u201d) as interpersonal rudeness, while under-weighting the key pragmatic function in context: invalidating Marco\u2019s risk/burnout objections and foreclosing process questions (ticket, scope, rollback). In other words, it conflated \u2018curt/direct under urgency\u2019 with \u2018rude,\u2019 rather than recognizing the specific dismissiveness pattern: minimizing, refusing to engage, and pressuring compliance.",
    "failure_pattern": "mislabeling-dismissive-as-rudeness",
    "severity": "major",
    "id": 124,
    "message": "Marco, please don\u2019t make this a whole thing \u2014 just jump on and ship the hotfix. We don\u2019t have time for a debate tonight.",
    "ground_truth_flags": [
      "Dismissive"
    ],
    "predicted_flags": [
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Both: it missed the expected \"Condescending\" flag and instead added \"Passive-Aggressive\" and \"Rudeness\" as separate labels, effectively misclassifying the primary tone violation.",
    "why": "The message\u2019s core issue is hierarchical belittlement (\"basic QA findings\", \"play-by-play\")\u2014a classic condescending/down-talking stance toward Rina\u2019s role\u2014more than indirectness. The model appears to (a) treat condescension as generic rudeness rather than a distinct category, and (b) over-read \"maybe next time\" as passive-aggressive despite the reprimand being fairly explicit. It also underused channel/culture context: #random is supposed to stay light and Sara/Priya already redirected appropriately, so the corrective content (move to #eng-incidents) is reasonable; what\u2019s problematic is the demeaning framing about QA\u2019s value/attention-worthiness, which maps most precisely to condescension.",
    "failure_pattern": "condescension-collapsed-into-rudeness",
    "severity": "major",
    "id": 125,
    "message": "Rina, maybe next time try posting this in #eng-incidents instead of #random \u2014 not everyone needs a play-by-play of basic QA findings here. Just file the ticket with steps and I\u2019ll take a look when I can.",
    "ground_truth_flags": [
      "Condescending"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Missed the expected \"Condescending\" flag and substituted a broader \"Rudeness\" flag instead (partial mislabeling rather than missing the underlying issue).",
    "why": "The model correctly detected hostility and the passive-aggressive jab, but it collapsed the more specific interpersonal stance (talking down/patronizing: \"it\u2019s not that complicated,\" \"I can explain it slower\") into a generic incivility category (\"Rudeness\"). This is a taxonomy granularity error: it identified the behavior but mapped it to the wrong label, likely because it treated condescension as a subset of rudeness rather than a distinct flag\u2014despite the message\u2019s explicit patronizing framing and implied intellectual superiority.",
    "failure_pattern": "condescension-vs-rudeness-label-confusion",
    "severity": "minor",
    "id": 126,
    "message": "Let\u2019s maybe use a tiny bit of judgment next time before we drop internal dashboards into #random \u2014 it\u2019s not that complicated. If anyone needs a refresher on what\u2019s okay to share here, I can explain it slower.",
    "ground_truth_flags": [
      "Condescending",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Missed an expected flag: it correctly flagged \"Passive-Aggressive\" but failed to flag \"Non-Objective\" even though the message includes subjective/interpretive language and an emotionally loaded framing rather than purely factual, actionable content.",
    "why": "The model latched onto the most salient cue (the veiled jab \"deep work is your thing\") and treated the rest as a straightforward request. It didn\u2019t recognize that \"so we don\u2019t look unprepared\" is a subjective, face-threatening justification (speculating about appearance/reputation) and that \"prioritize ... like we discussed\" is vague and non-verifiable in-channel (no concrete scope, timeline tradeoff, or explicit action items), which fits a Non-Objective bucket in this high-stakes, public #sales context. In other words, it performed single-flag detection focused on tone, not on objectivity/operational specificity.",
    "failure_pattern": "missed-non-objective-due-to-salient-tone-cue",
    "severity": "major",
    "id": 127,
    "message": "@dave I know deep work is *your thing*, but could you just prioritize the bulk-import toggle like we discussed so we don\u2019t look unprepared on Monday? Thanks.",
    "ground_truth_flags": [
      "Non-Objective",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed the expected \"Condescending\" flag and instead added an incorrect \"Non-Objective\" flag.",
    "why": "The message\u2019s core issue is a demeaning, belittling stance toward other teams (\"we don\u2019t have time for another round of 'can\u2019t because launch'\" and \"actually read logs... without a committee\"), which is classic condescension layered on top of rudeness. The model correctly caught hostility (rudeness) but misinterpreted the competence-dunk as an objectivity problem. \"Non-Objective\" typically targets speculation, unverifiable claims, or mixing opinions with facts in a way that impairs clarity; here, the problematic phrasing is primarily interpersonal (a put-down), not a factual-accuracy or evidentiary issue. In other words, it treated a tone/attitude violation as a content/objectivity violation, likely because the phrase \"actually\" plus implied generalization about others\u2019 abilities superficially resembles an unsupported claim, but functionally serves as a condescending jab.",
    "failure_pattern": "condescension-misclassified-as-non-objective",
    "severity": "major",
    "id": 128,
    "message": "Ok, we don\u2019t have time for another round of \u201ccan\u2019t because launch\u201d \u2014 just pick one person each from Product, Data, and CS and get them in the incident thread in the next 10 minutes. If you\u2019re not sure who to send, choose whoever can actually read logs and make decisions without a committee.",
    "ground_truth_flags": [
      "Condescending",
      "Rudeness"
    ],
    "predicted_flags": [
      "Non-Objective",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Missed a required flag: it correctly flagged Rudeness but failed to also flag Overly-blunt.",
    "why": "The model appears to have treated \u201cOverly-blunt\u201d as redundant with \u201cRudeness,\u201d collapsing two distinct categories into one. The message contains multiple blunt, imperative, process-policing commands (\u201cStop doing\u2026\u201d, \u201cOpen the ticket, page the on-call\u2026\u201d, \u201ckeep me out\u2026\u201d) and categorical refusals (\u201cit\u2019s not my job to drop everything\u201d) that are both rude and unusually sharp for a large, compliance-sensitive org and a public #random channel. By focusing mainly on overt hostility/dismissiveness, it overlooked the separate style dimension: the phrasing is high-friction, non-collaborative, and unsoftened even if the underlying request (follow incident process) is legitimate. Context (public channel, high-stakes incident, strained cross-functional history) increases the likelihood that this reads as excessively blunt beyond mere process guidance, but the model didn\u2019t translate that into the additional flag.",
    "failure_pattern": "overlapping-flags-collapsed",
    "severity": "major",
    "id": 131,
    "message": "Stop doing drive-by escalations in #random and tagging people who aren\u2019t on-call \u2014 it\u2019s not my job to drop everything because Sales didn\u2019t follow the incident process. Open the ticket, page the on-call, and keep me out of this thread unless there\u2019s an actual engineering question I can answer.",
    "ground_truth_flags": [
      "Overly-blunt",
      "Rudeness"
    ],
    "predicted_flags": [
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "The model over-flagged by adding \"Non-Objective\"; the only expected flag was \"Passive-Aggressive.\"",
    "why": "It treated a sarcastic rhetorical line (\"love the part where I\u2019m the only one reading the incident write-up\") as a factual claim requiring evidentiary support. In this context, the phrase functions primarily as a passive-aggressive jab/shaming device rather than an assertion meant to be evaluated for objectivity. The passive-aggressive signal already captures the problematic behavior, so adding \"Non-Objective\" is redundant and misaligned with the flagging taxonomy\u2019s intent (which typically reserves \"Non-Objective\" for disputes about factual accuracy, speculative claims, or unverifiable technical assertions).",
    "failure_pattern": "overflagging-non-objective-on-sarcasm",
    "severity": "minor",
    "id": 132,
    "message": "Cool, love the part where I\u2019m the only one reading the incident write-up \ud83d\ude43 If you can just take another pass and make the handler idempotent + add the missing tests, that\u2019d be awesome so we can maybe ship this before the demo.",
    "ground_truth_flags": [
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Non-Objective",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed the expected \"Non-Objective\" flag (under-flagging). It correctly flagged \"Rudeness\" but failed to capture that the message makes an unverified causal claim and uses vague, non-actionable generalizations (\"you broke checkout\u2014again\", \"risky refactors\") rather than concrete, testable details.",
    "why": "The model appears to treat \"Non-Objective\" as mostly about lacking technical specifics or being purely emotional, and it anchored on the obvious incivility. It likely didn\u2019t apply the policy that accusations of fault (especially \"again\") and sweeping judgments are non-objective without evidence, even when there is a real regression. It also underweighted the channel context (VP monitoring, need for concrete updates) where objective, evidence-based phrasing is especially required, making the non-objective aspect more salient.",
    "failure_pattern": "missed-non-objective-accusation",
    "severity": "major",
    "id": 134,
    "message": "Leo, you broke checkout\u2014again. Stop merging risky refactors the night before launch and fix it now.",
    "ground_truth_flags": [
      "Non-Objective",
      "Rudeness"
    ],
    "predicted_flags": [
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Missed the expected flag: it should have flagged \"Condescending\" but instead flagged \"Non-Objective\" and \"Rudeness\". In other words, it caught negativity, but mislabeled the dominant behavior.",
    "why": "The message\u2019s core violation is patronizing, superior framing (\"some of us have to think past next Friday,\" \"Maybe skim the incident notes\")\u2014i.e., condescension. The model appears to map insults/dismissiveness primarily to a generic \"Rudeness\" bucket and separately to \"Non-Objective\" due to motive-attribution (\"you want the quickest thing...\"). But the evaluators\u2019 taxonomy expects the more specific interpersonal pattern (condescension/patronizing tone) rather than broader categories. It likely over-weighted the explicit assumption about intent as \"Non-Objective\" and treated the rest as general rudeness, instead of recognizing the hierarchical/lecturing posture and belittling phrasing that define condescension\u2014especially salient given the peer relationship and public, cross-functional thread context.",
    "failure_pattern": "taxonomy-mismatch-specificity-error",
    "severity": "major",
    "id": 135,
    "message": "Diego, I get that you want the quickest thing that *feels* like progress, but some of us have to think past next Friday. Maybe skim the incident notes from last month and you\u2019ll see why bolting serverless + batch onto this isn\u2019t the clever shortcut you think it is.",
    "ground_truth_flags": [
      "Condescending"
    ],
    "predicted_flags": [
      "Non-Objective",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: it labeled the message as Passive-Aggressive and Rudeness, but the expected label was only Dismissive. The model over-escalated the tone into harsher categories than warranted.",
    "why": "The model treated a blunt boundary-setting response as interpersonal hostility. While the message is clearly dismissive (shutting down the request with \u201cWe\u2019ll ship when it\u2019s ready\u201d and pushing the convo out of #random), the cues it used to justify Passive-Aggressive (\u201cYeah, we all saw the countdown\u201d) are more straightforward annoyance/sarcasm than indirect, veiled criticism. Similarly, calling it Rudeness overweighted the imperative \u201cplease stop\u201d without accounting for the situational norm: #random is explicitly a non-work channel, and the PM is publicly pressuring engineers there. In that context, a direct pushback about channel misuse is plausibly appropriate (though still dismissive), not necessarily rude/abusive.",
    "failure_pattern": "overflagging-boundary-setting-as-hostility",
    "severity": "minor",
    "id": 139,
    "message": "Yeah, we all saw the countdown. We\u2019ll ship when it\u2019s ready\u2014please stop trying to turn #random into a status meeting.",
    "ground_truth_flags": [
      "Dismissive"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Flagged an extra category (\"Vagueness\") that was not expected; the passive-aggressive flag was correct, so the error is overflagging rather than a miss.",
    "why": "The model treated colloquial/pressure phrasing (\"whip something up tonight\") as lack of actionable detail, but in this channel/context the meaning is already constrained by the preceding messages (minimal connector + demo script; technical story by EOD; mocked data acceptable). In other words, it failed to weight the immediate conversation context and instead applied a generic rule that informal language implies unclear scope. The real issue in the message is tone (sarcasm, \"save the deal\" framing, \ud83d\ude43) rather than task ambiguity.",
    "failure_pattern": "context-ignored-leading-to-vagueness-overflag",
    "severity": "minor",
    "id": 141,
    "message": "Awesome, love that we\u2019re doing a totally custom integration demo tomorrow morning on top of a live hotfix \u2014 Jordan, can you just whip something up tonight so we can \u201csave the deal\u201d again? \ud83d\ude43",
    "ground_truth_flags": [
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Vagueness",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "It over-flagged: added \"Non-Objective\" and \"Rudeness\" even though the expected (and sufficient) labels were \"Condescending\" and \"Passive-Aggressive\". It also missed mapping the core tone to the more specific expected label \"Condescending\" (using \"Rudeness\" instead).",
    "why": "The model correctly detected hostility and sarcasm, but it (a) collapsed a nuanced power/put-down signal into a broader \"Rudeness\" category instead of the more precise \"Condescending\" (\"read the doc like everyone else,\" \"entitled to be on every call\"), and (b) treated rhetorical disparagement as an epistemic claim, triggering \"Non-Objective.\" While the message does speculate about others\u2019 entitlement, in this taxonomy the primary issue is tone and hierarchy/shaming rather than factual objectivity\u2014so the model\u2019s taxonomy boundarying was off (tone/condescension vs. rudeness/non-objectivity).",
    "failure_pattern": "taxonomy-mismatch-overflagging",
    "severity": "major",
    "id": 142,
    "message": "Maybe next time the folks who are panicking about renewals could just wait for the actual announcement instead of assuming they were entitled to be on every call. I\u2019m sure it\u2019ll all make sense once you read the doc like everyone else.",
    "ground_truth_flags": [
      "Condescending",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Non-Objective",
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Missed an expected flag: the model correctly flagged Passive-Aggressive but failed to flag Fake, resulting in an incomplete prediction.",
    "why": "The message uses exaggerated, performative positivity (\u201cAmazing hustle, team \u2014 love the energy\u201d plus a thumbs-up) immediately followed by a coercive directive (\u201cdrop whatever you\u2019re doing\u201d) and a dismissive closer (\u201cso we can stop talking about it here\u201d). In this high-tension, public #general context with execs watching, that upbeat framing reads less as genuine encouragement and more as a veneer to soften pressure and impatience. The model recognized the passive-aggressive jab but treated the upbeat opener as normal motivational language rather than potentially insincere/performative given the contradiction in tone and the reputational stakes. In other words, it underweighted the tone-incongruence cue that drives the \u201cFake\u201d label (forced positivity masking frustration).",
    "failure_pattern": "missed-fake-performative-positivity",
    "severity": "major",
    "id": 144,
    "message": "Amazing hustle, team \u2014 love the energy. Can you three just drop whatever you\u2019re doing and get the overnight missing-data bug + the demo instrumentation done ASAP so we can stop talking about it here? \ud83d\udc4d",
    "ground_truth_flags": [
      "Fake",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed a required flag: the message should also have been flagged as Microaggressive, but the model only flagged Passive-Aggressive.",
    "why": "The model treated the content as merely indirect hostility and focused on tone markers (veiled jab, surface politeness) while overlooking the microaggression embedded in the status/competence-undermining phrase \"people who actually build this stuff.\" In this context, that wording functions as a subtle devaluation of Maya\u2019s role (implying she\u2019s not a real contributor or lacks legitimacy to ask for estimates) and sets an in-group/out-group boundary (builders vs. non-builders). The microaggression is subtle and role-based rather than identity-based, and the model likely uses an overly narrow prototype for microaggression (e.g., requiring protected-class references), so it failed to apply the label despite the dismissive, gatekeeping framing.",
    "failure_pattern": "microaggression-narrow-definition-missed",
    "severity": "major",
    "id": 145,
    "message": "Maya, totally get the urgency, but maybe let the people who actually build this stuff estimate it \u2014 the constant public pings are starting to feel a little\u2026 intense.",
    "ground_truth_flags": [
      "Microaggressive",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed the expected \"Fake\" flag (only flagged Passive-Aggressive).",
    "why": "The model correctly detected sarcasm/passive-aggression, but failed to recognize that the message also includes a commitment-implying move that can be read as performative/insincere: \"I\u2019ll do my best to 'make' today happen\" signals an intention/effort toward an ETD while simultaneously asserting they cannot provide confirmation. In a high-stakes, exec-visible client-update channel where procurement needs written confirmation, this kind of hedged promise can function as a face-saving posture rather than a clear, actionable stance\u2014i.e., appearing cooperative without actually committing. The model likely treated \"Fake\" too narrowly as outright deception (e.g., lying or fabricated facts) and overlooked that the guideline\u2019s \"Fake\" category can also cover inauthentic/performative assurances or implied commitments that the speaker does not genuinely endorse. It also underweighted the channel context (public, client-facing implications, penalty clause pressure), where such wording is more likely to be interpreted as posturing.",
    "failure_pattern": "missed-performative-assurance-fake-flag",
    "severity": "major",
    "id": 146,
    "message": "Love how we\u2019re confirming ETDs in public channels before engineering actually signs off \u2014 super helpful \ud83d\udc4d. I\u2019ll do my best to \u201cmake\u201d today happen, but I can\u2019t in good faith give procurement a written confirmation yet.",
    "ground_truth_flags": [
      "Fake",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed a required flag: the message should have been tagged as both \"Rudeness\" and \"Overly-blunt,\" but the model only returned \"Rudeness.\"",
    "why": "The model appears to have collapsed two distinct dimensions into one. It correctly detected interpersonal hostility/blame (\"This is a mess \u2014 we broke checkout,\" \"Sam stop poking around\"), but failed to separately recognize the communication-style violation of being overly blunt for a client-visible status channel: abrupt imperatives, zero cushioning, public reprimand of a junior on-call, and an absolute demand (\"I want it fixed before the 3pm call\") without acknowledging uncertainty/triage. In BrightWave\u2019s #client-updates context, these are independently problematic even if not overtly insulting\u2014so \"Overly-blunt\" should trigger alongside \"Rudeness.\"",
    "failure_pattern": "bluntness-conflated-with-rudeness",
    "severity": "major",
    "id": 148,
    "message": "This is a mess \u2014 we broke checkout. Priya take the logs and find the root cause, Alex roll back the last deploy, and Sam stop poking around and just run the on-call playbook until someone tells you otherwise. I want it fixed before the 3pm call.",
    "ground_truth_flags": [
      "Overly-blunt",
      "Rudeness"
    ],
    "predicted_flags": [
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: the model added a \"Non-Objective\" flag even though the message is primarily sarcastic/passive-aggressive rather than making a substantive factual claim that meaningfully violates objectivity.",
    "why": "The model treated a rhetorical jab (\"we apparently don\u2019t need scope or context anymore\") as an evidence-free attribution of intent/decision, but in this context it functions as sarcasm embedded in the passive-aggressive tone\u2014not a standalone non-objective assertion presented as a factual statement. The pipeline likely over-triggers \"Non-Objective\" whenever it sees implied motives/assumptions, without weighting that the same wording is already fully explained by the Passive-Aggressive category and doesn\u2019t add an independent objectivity risk.",
    "failure_pattern": "overflagging-non-objective-on-sarcasm",
    "severity": "minor",
    "id": 149,
    "message": "Sure, let\u2019s just hotfix prod, hit tomorrow\u2019s demo date, and rewrite our incident process all in one afternoon \u2014 what could possibly go wrong. I\u2019ll magically make it happen since we apparently don\u2019t need scope or context anymore.",
    "ground_truth_flags": [
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Non-Objective",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed the \"Rudeness\" flag: the message is not only passive-aggressive/sarcastic, it contains overtly derisive, blaming language that should also be classified as rude.",
    "why": "The model latched onto the most salient surface cue (sarcasm/irony markers like \u201clove finding out\u2026 \ud83d\ude43\u201d and \u201cmagically produce an ETA\u201d) and treated the entire violation as passive-aggression. It underweighted the direct contempt and shaming components (\u201cfire drill,\u201d \u201cdrop the incident postmortem,\u201d \u201cwith zero ticket/link/repro\u2014because that always goes great\u201d), which are explicit disparagement and public scolding of a colleague, especially in a high-stress channel context. In other words, it collapsed a multi-label case into a single label because it equated sarcasm with passive-aggression and didn\u2019t separately score the interpersonal disrespect dimension.",
    "failure_pattern": "single-label-collapse-sarcasm-over-weights-rudeness",
    "severity": "major",
    "id": 151,
    "message": "Awesome, love finding out about a top-5 account fire drill via a channel ping during freeze week \ud83d\ude43. Sure, we\u2019ll just drop the incident postmortem and magically produce an ETA with zero ticket/link/repro\u2014because that always goes great.",
    "ground_truth_flags": [
      "Rudeness",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed the expected \"Condescending\" flag (false negative) and instead returned adjacent-but-incorrect labels (\"Non-Objective\", \"Passive-Aggressive\", \"Rudeness\") that don\u2019t match the taxonomy expectation for this message.",
    "why": "The message\u2019s primary violation is a competence-putdown from a position of authority (\"we\u2019re apparently struggling with basics today\") plus belittling micromanagement\u2014classic condescension. The model correctly sensed negativity but mapped it to broader/overlapping categories: (1) it treated the explicit insult as general \"Rudeness\" rather than the more specific \"Condescending\" class; (2) it over-interpreted curt urgency as \"Passive-Aggressive\" even though the hostility is direct, not indirect/veiled; and (3) it added \"Non-Objective\" by interpreting a disparaging remark as a factual claim, when the taxonomy likely reserves that flag for unsupported assertions about incident facts/causality rather than interpersonal judgments. In short, it ignored the label hierarchy/specificity and over-labeled secondary traits instead of selecting the intended primary flag.",
    "failure_pattern": "wrong-taxonomy-mapping-condescension-to-adjacent-flags",
    "severity": "major",
    "id": 153,
    "message": "Ok, let\u2019s keep this simple since we\u2019re apparently struggling with basics today: leo, pull the last 30 mins of prod logs and confirm whether the deploy correlates with the spike; nina, roll back to the previous stable build and post the exact timestamp + impact summary here. Please don\u2019t freelance or debate scope right now\u2014just execute and report back with facts.",
    "ground_truth_flags": [
      "Condescending"
    ],
    "predicted_flags": [
      "Non-Objective",
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Flagged \"Rudeness\" in addition to the expected \"Passive-Aggressive\"; the extra rudeness flag is an overreach relative to the rubric/expected labeling for this case.",
    "why": "The model treated sharp sarcasm and process-enforcement snark as \"openly hostile/disrespectful\" rather than recognizing it as passive-aggressive commentary. The message is clearly sarcastic and belittling of the behavior/process circumvention, but it doesn\u2019t contain direct insults, slurs, profanity, or explicit name-calling toward a person; it\u2019s more a veiled jab framed through irony. In a high-stakes, process-heavy org context, the model likely conflated 'public pushback + sarcasm' with standalone rudeness, double-counting the same signal (sarcasm) under two labels instead of reserving rudeness for more direct interpersonal disrespect.",
    "failure_pattern": "overflagging-sarcasm-as-rudeness",
    "severity": "minor",
    "id": 155,
    "message": "Sure, let me just drop everything and debug a production outage from #random because tagging me is totally the official incident process. Next time we can skip the ticket entirely and just @everyone, that\u2019ll be super efficient.",
    "ground_truth_flags": [
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "The model over-flagged by adding \"Rudeness\" (not expected) and missed the \"Non-Objective\" flag that was expected.",
    "why": "It correctly detected the sarcastic, needling tone (passive-aggressive), but then conflated that same evidence with a separate \"Rudeness\" category, effectively double-counting one behavior rather than distinguishing categories. At the same time, it underweighted the policy/process dimension of the channel: the message is not ticket-anchored/action-focused and contains subjective blame/venting (\u201clove finding out\u2026\u201d, \u201clike last time\u201d, \u201cI\u2019m sure\u2026\u201d) rather than objective incident status and next steps. In MeridianAxis\u2019s #sales context (concise, policy-aligned, tracked), that lack of objective, ticket-driven framing is a core issue the model failed to surface.",
    "failure_pattern": "missed-non-objective-overflagged-rudeness",
    "severity": "major",
    "id": 156,
    "message": "Awesome \u2014 love finding out 20 min before a Fortune 100 demo that quote gen is broken again. But sure Ethan, I\u2019m sure it\u2019s \u201cnot your change\u201d like last time \ud83d\ude43 can someone just roll it back so sales doesn\u2019t get blindsided?",
    "ground_truth_flags": [
      "Non-Objective",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Missed the expected \"Condescending\" flag and instead added an extra \"Non-Objective\" flag that wasn\u2019t expected.",
    "why": "The model correctly detected overt disrespect (Rudeness) but mapped the core issue \u201cit\u2019s not that complicated\u201d to a truth/subjectivity problem (Non-Objective) rather than to its pragmatic function: belittling the engineer\u2019s expertise/constraints and asserting dominance (\u201cjust prioritize\u2026\u201d, \u201cIf you need help understanding\u2026 scroll up\u201d). In this context, the sentence isn\u2019t mainly an unsupported factual claim; it\u2019s a patronizing minimization and a status move. The model likely over-weighted a rubric heuristic that treats evaluative phrasing as \u2018non-objective\u2019 and under-weighted the interpersonal intent (condescension/patronization), especially given the Slack #sales pressure-cooker setting and public calling-out dynamic.",
    "failure_pattern": "mislabeling-condescension-as-non-objective",
    "severity": "major",
    "id": 157,
    "message": "@dana it\u2019s not that complicated \u2014 you just need to prioritize the thing we committed to and ship it by Friday. If you need help understanding why this matters, scroll up and read the thread.",
    "ground_truth_flags": [
      "Condescending",
      "Rudeness"
    ],
    "predicted_flags": [
      "Non-Objective",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as \"Passive-Aggressive\" instead of the expected \"Dismissive\" (incorrect flag; core issue is shutting down discussion and minimizing concerns, not indirect sarcasm).",
    "why": "The model over-indexed on the presence of conflict and the phrase \"another debate\" as a generic signal for passive-aggression, but the message is overt and imperative rather than indirectly hostile. Passive-aggression typically involves veiled digs, sarcasm, or deniable barbs; here, the speaker directly shuts down input (\"Let\u2019s not turn this into another debate\") and explicitly deprioritizes stated risks (\"clean up ... next week\"), which maps more cleanly to dismissiveness. The model also appears to have underweighted the situational context (public #random channel, out-of-hours @mentions, prior warnings about flaky tests/on-call burnout) that makes the tone read as invalidating rather than \"passive.\"",
    "failure_pattern": "mislabeling-dismissiveness-as-passive-aggression",
    "severity": "major",
    "id": 160,
    "message": "Let\u2019s not turn this into another debate \u2014 just make it happen for Friday. We can clean up the flaky tests next week.",
    "ground_truth_flags": [
      "Dismissive"
    ],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: added a \"Non-Objective\" flag that isn\u2019t warranted by the target message. \"Vagueness\" was appropriate and expected.",
    "why": "The model treated colloquial, impact-focused phrasing (\"won\u2019t blow up support again\") as an evidentiary claim requiring proof, rather than as a reasonable constraint/requirement grounded in shared context (the thread already references last month\u2019s false-positive spike and 40+ escalations). It also conflated \"objective\" with \"certainty\": the message is not asserting a factual guarantee, it\u2019s expressing a desired outcome (avoid repeat escalations). In this culture/channel context, that kind of shorthand is normal for time-sensitive coordination and doesn\u2019t constitute non-objective or accusatory language by itself.",
    "failure_pattern": "overflagging-colloquial-requirements-as-non-objective",
    "severity": "minor",
    "id": 161,
    "message": "Can someone just sort out the fraud pipeline plan so we can move forward? We need something that works and won\u2019t blow up support again.",
    "ground_truth_flags": [
      "Vagueness"
    ],
    "predicted_flags": [
      "Vagueness",
      "Non-Objective"
    ]
  },
  {
    "what_went_wrong": "Both: it incorrectly flagged the message as only \"Passive-Aggressive\" and missed the expected \"Microaggressive\" and \"Non-Objective\" flags.",
    "why": "The model latched onto the metaphor/teasing tone (\u201chill to die on\u201d) and interpreted it narrowly as passive-aggression, but failed to recognize two more salient issues in this org/channel context: (1) Microaggressive/professional-demeaning content\u2014\u201cvery \u2018designer\u2019 hill to die on\u201d stereotypes and diminishes a legitimate accessibility/compliance concern by framing it as a parochial design obsession, which is a role-based put-down in a public, high-stakes channel; (2) Non-objective/unsupported judgment\u2014\u201clet\u2019s not overthink this\u201d and the characterization of the concern as a \u201cdesigner hill\u201d are subjective dismissals without evidence, and they sidestep the compliance/risk-register process being discussed. The model underweighted the strict change-control + auditability norms and the explicit compliance framing in prior messages, where dismissive, identity/role-based language is especially inappropriate and non-evidence-based.",
    "failure_pattern": "microaggression-and-nonobjectivity-missed-due-to-tone-fixation",
    "severity": "major",
    "id": 162,
    "message": "Got it \u2014 let\u2019s not overthink this. Accessibility polish is important, but this feels like a very \u201cdesigner\u201d hill to die on given the deadline and scrutiny right now.",
    "ground_truth_flags": [
      "Microaggressive",
      "Non-Objective"
    ],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flag taxonomy mismatch: the model predicted \"Rudeness\" instead of the expected \"Condescending\" (while correctly flagging \"Passive-Aggressive\").",
    "why": "The message contains both overt hostility and a superiority/put-down dynamic. The model collapsed the more specific \"Condescending\" signal into the broader \"Rudeness\" bucket, likely because it keyed on obviously insulting phrases (\"magic spell\", \"too hard to find\") and treated them as generic incivility rather than identifying the hierarchical belittling implied by \"schedule interrogations\" and the insinuation that the recipient can\u2019t locate prior explanations. In other words, it recognized negative tone but didn\u2019t distinguish the *type* of negativity (belittling/speaking down vs. general rude). Channel/context (calling this out in #random) amplifies the condescension, but the model\u2019s reasoning focused on word-level harshness instead of the interpersonal stance.",
    "failure_pattern": "condescension-collapsed-into-rudeness",
    "severity": "minor",
    "id": 163,
    "message": "Okay, since we\u2019re doing schedule interrogations in #random now: the date isn\u2019t a magic spell you can summon by tagging people. If you want a real answer, try the eng thread where we\u2019ve already explained the blockers\u2014happy to retype it here if that\u2019s too hard to find.",
    "ground_truth_flags": [
      "Condescending",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Both: it missed the expected flags (Condescending, Rudeness) and instead flagged only Passive-Aggressive. The message is more overtly scolding/condescending than passive-aggressive.",
    "why": "The model over-weighted the \"welcome aboard\" opener + admonishment contrast as the defining signal of passive-aggression, and under-weighted the explicit harsh language and status dynamics that make it plainly rude/condescending. Phrases like \"don\u2019t spam #engineering,\" \"tag half the on-call list,\" \"answered in the deploy doc,\" and \"Take 10 minutes\" + \"one coherent question\" are direct reprimands that belittle the new hire and imply incompetence/laziness. Given the channel norm (keep #engineering for rollout status/decisions) the request to move/how-to questions is legitimate, but the delivery is still disrespectful; the model conflated legitimate boundary-setting with a passive-aggressive style rather than recognizing it as straightforward rudeness/condescension amplified by newcomer + on-call pressure context.",
    "failure_pattern": "mislabeling-rude-condescension-as-passive-aggressive",
    "severity": "major",
    "id": 164,
    "message": "Hey, welcome aboard \u2014 but please don\u2019t spam #engineering and tag half the on-call list with questions that are answered in the deploy doc. Take 10 minutes to read it, write up one coherent question with what you tried + what env you\u2019re on, and then we can help.",
    "ground_truth_flags": [
      "Condescending",
      "Rudeness"
    ],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: it added a \"Non-Objective\" flag when the only expected issue was tone (Passive-Aggressive).",
    "why": "The model conflated a tone problem with an objectivity problem. While the message is clearly sarcastic and guilt-inducing (\"must be nice to have the luxury of saying no\"), it doesn\u2019t primarily present factual claims requiring evidence in the way \"Non-Objective\" typically targets. The line \"engineering couldn\u2019t make it happen\" is a subjective/loaded framing, but it\u2019s a common rhetorical attribution in a tense DM\u2014not a clear-cut unverifiable assertion about facts or motives. The model over-interpreted implied blame and mind-reading as an objectivity violation rather than keeping the classification focused on passive-aggressive/accusatory tone. Channel context (DM, late-night pressure) likely amplified the model\u2019s sensitivity to \u201cimplied motive\u201d language and pushed it into an extra flag.",
    "failure_pattern": "overflagging-non-objective-from-tone",
    "severity": "minor",
    "id": 166,
    "message": "Totally get that protecting the sprint is important \u2014 must be nice to have the luxury of saying no when there\u2019s a renewal and a CEO commitment on the line. I\u2019ll just let them know we\u2019re not doing it for Friday and we can explain together why engineering couldn\u2019t make it happen.",
    "ground_truth_flags": [
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Non-Objective",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed the expected \"Non-Objective\" flag: it only flagged \"Rudeness\" even though the message also makes broad, evaluative claims (\u201cthis PR is a mess\u201d, \u201clogic is all over the place\u201d, \u201cnot ready to ship\u201d, \u201cstop trying to be clever\u201d) without concrete, verifiable specifics or actionable evidence.",
    "why": "The model correctly detected explicit hostile phrasing (rudeness) and likely treated the remaining content as sufficiently task-focused (\u201crewrite it cleanly\u2026blocking QA\u201d), overlooking that the core critique is framed as subjective judgment rather than objective, checkable feedback. It also didn\u2019t leverage the channel context (high-stakes, desire for calmer, documented comms; a junior engineer already on edge; need for behavior-diff documentation/tests) which heightens the importance of precise, evidence-based comments and makes the non-objective nature more salient. In short: it over-weighted tone toxicity and under-weighted the \u2018grounding/specificity\u2019 dimension required for Non-Objective.",
    "failure_pattern": "missed-non-objective-while-catching-rudeness",
    "severity": "major",
    "id": 167,
    "message": "Leo, this PR is a mess \u2014 the logic is all over the place and it\u2019s not ready to ship. Please stop trying to be clever and rewrite it cleanly before we waste more time blocking QA.",
    "ground_truth_flags": [
      "Non-Objective",
      "Rudeness"
    ],
    "predicted_flags": [
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Missed the expected \"Non-Objective\" flag (under-flagging). It correctly flagged \"Rudeness\" but failed to also flag that the message contains subjective/loaded claims and blame framing rather than purely factual status/ETA content.",
    "why": "The model focused on overt tone markers (\"design whiplash,\" directives, and blame shifting) and mapped them solely to rudeness. It didn\u2019t recognize that the message also includes non-factual, evaluative language and attribution of fault (e.g., implying repeated requirement changes, assigning responsibility: \"anything beyond that is on design\") which is inherently interpretive and not objectively verifiable in the thread. In #general\u2014where high-signal, calm, inclusive announcements are expected\u2014this kind of subjective blame framing is especially salient, but the model treated it as a pure tone issue instead of a combination of tone + objectivity violation.",
    "failure_pattern": "missed-non-objective-blame-framing",
    "severity": "major",
    "id": 169,
    "message": "We don't have time for another round of design whiplash\u2014pick a version and stick to it. Engineering can ship the current flow by EOD tomorrow; anything beyond that is on design.",
    "ground_truth_flags": [
      "Non-Objective",
      "Rudeness"
    ],
    "predicted_flags": [
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Both: it missed the expected primary flag (Dismissive) and instead over-attributed the tone as Passive-Aggressive and Rudeness.",
    "why": "The message\u2019s core issue is shutting down discussion and invalidating others\u2019 urgency (\u201cWe\u2019re not going to keep re-litigating this\u2026 Please stop pushing\u2026\u201d), which maps most directly to Dismissive. The model treated the same surface cues (imperatives, frustration) as separate, stronger categories\u2014Rudeness and Passive-Aggressive\u2014without distinguishing between (a) blunt, tense crisis management in a high-stakes incident channel and (b) genuinely insulting or snide language. There\u2019s no indirect jab, sarcasm, or backhandedness typical of passive-aggression; it\u2019s openly curt. And while it\u2019s brusque, it\u2019s not overtly abusive\u2014so \u201cRudeness\u201d is at least arguable, but the key miss is failing to label the primary dismissal behavior and misclassifying direct frustration as passive-aggression.",
    "failure_pattern": "overflagging-frustrated-directness-missed-dismissive",
    "severity": "major",
    "id": 170,
    "message": "We\u2019re not going to keep re-litigating this\u2014engineering will say it\u2019s fixed when it\u2019s actually fixed. Please stop pushing for guarantees we can\u2019t back up.",
    "ground_truth_flags": [
      "Dismissive"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Missed the expected \"Condescending\" flag and instead labeled the tone as \"Passive-Aggressive\" (while correctly flagging \"Rudeness\").",
    "why": "The message is not primarily indirect or veiled; it\u2019s openly derisive and patronizing. Phrases like \u201cSure, I\u2019ll drop everything\u2026\u201d and \u201clike the rest of the company\u201d are explicit put-downs that assert superiority and belittle the recipient\u2019s competence/status\u2014hallmarks of condescension. The model anchored on sarcasm as a cue for passive-aggression and treated sarcasm as synonymous with passive-aggressive, ignoring that the aggression here is direct (not \u2018passive\u2019) and the core interpersonal harm is the talking-down/lecturing dynamic.",
    "failure_pattern": "condescension-mislabeled-as-passive-aggressive",
    "severity": "major",
    "id": 172,
    "message": "Sure, I\u2019ll drop everything because you yelled \"churn\" in #random again \u2014 that\u2019s definitely how incidents work here. Next time, try filing the ticket and paging the on-call like the rest of the company; I\u2019m not your personal escalation path.",
    "ground_truth_flags": [
      "Condescending",
      "Rudeness"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Missed the expected \u201cDismissive\u201d flag and instead added an unnecessary \u201cNon-Objective\u201d flag; \u201cRudeness\u201d was correct.",
    "why": "The core harm in the target message is interpersonal and context-driven: it shuts down a legitimate, time-sensitive quality escalation in a high-visibility #general thread (\u201cLet\u2019s not derail\u2026\u201d, \u201cQA panic\u201d, \u201cJust file a ticket\u201d, \u201cwe\u2019ll look when we can\u201d). That\u2019s classic dismissiveness\u2014minimizing impact, discouraging further reporting, and deprioritizing QA\u2019s urgent risk signal\u2014especially given leadership scrutiny and a demo deadline. The model latched onto a secondary linguistic feature (\u201cthe refactor is fine\u201d as an unsupported claim) and mapped it to \u201cNon-Objective,\u201d but that label is less central here and not what the rubric likely intends for this kind of conversational violation. In short: it focused on epistemic certainty rather than the message\u2019s dismissive/downplaying intent and the channel\u2019s escalation expectations.",
    "failure_pattern": "missing-dismissiveness-over-weighting-non-objective",
    "severity": "major",
    "id": 176,
    "message": "Let\u2019s not derail #general with QA panic \u2014 the refactor is fine. Just file a ticket and we\u2019ll look when we can; we still have a demo to prep.",
    "ground_truth_flags": [
      "Dismissive",
      "Rudeness"
    ],
    "predicted_flags": [
      "Non-Objective",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "The model missed the expected \"Fake\" flag. It correctly flagged \"Passive-Aggressive\" but failed to also recognize that the opening praise is insincere/performative and therefore qualifies as \"Fake\" in addition to passive-aggressive.",
    "why": "The model treated the opening (\u201cAmazing job\u2026 \ud83d\ude43\u201d) purely as sarcasm and rolled it into the passive-aggressive rationale, without separately mapping it to the taxonomy\u2019s \"Fake\" category (i.e., praise that is not genuine, backhanded compliment). The upside-down emoji is a strong cue of forced/insincere positivity, and in a client-visible, high-stakes status channel this kind of performative praise is especially salient as \"fake\" because it signals blame while pretending to compliment. In short: it recognized the tone, but under-labeled it due to category-boundary confusion/label overlap (sarcasm \u2192 passive-aggressive) and insufficient multi-label triggering.",
    "failure_pattern": "missed-fake-due-to-label-overlap",
    "severity": "major",
    "id": 177,
    "message": "Amazing job on last night\u2019s release, truly \ud83d\ude43 \u2014 let\u2019s all drop everything and clean up the intermittent checkout failures ASAP. Alex take logs + error rates, Priya roll back the last checkout change behind a flag, and Ben please babysit prod and ping us every 15 min with whatever you find.",
    "ground_truth_flags": [
      "Fake",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Both: it missed the expected \u201cMicroaggressive\u201d flag and instead added \u201cRudeness,\u201d which wasn\u2019t in the expected set.",
    "why": "The model correctly detected the message\u2019s non-objective, passive-aggressive tone, but it collapsed the interpersonal harm into a generic \u201cRudeness\u201d category instead of recognizing the specific microaggressive structure: indirect, generalized disparagement of an unnamed person/group (\u201csomeone who\u2019s actually detail-oriented,\u201d \u201cSome people are great at\u2026 just\u2026 not the follow-through\u201d). In this #random context\u2014where the team is already discussing a specific incident and a specific contributor likely feels exposed\u2014the \u2018some people\u2019 framing functions as a public, identity-adjacent competence stereotype/othering move (a classic microaggression pattern in workplace comms: subtle denigration via coded comparison and \u2018compliment sandwich\u2019). The model likely anchored on explicit insult cues and treated the harm as overt incivility, underweighting the contextual subtext (public channel, incident blame, implied target) that distinguishes microaggression from plain rudeness.",
    "failure_pattern": "microaggression-misclassified-as-rudeness",
    "severity": "major",
    "id": 178,
    "message": "Not trying to pile on, but maybe next time someone who\u2019s *actually* detail-oriented should run the handoff/deploy thread. Some people are great at the big-picture stuff, just\u2026 not the follow-through.",
    "ground_truth_flags": [
      "Microaggressive",
      "Non-Objective",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Non-Objective",
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: it labeled the message as \"Rudeness\" instead of the expected \"Sarcastic\" (it did catch \"Passive-Aggressive\").",
    "why": "The model correctly recognized the mocking/negative intent but mapped it to the wrong category. The message\u2019s primary mechanism is sarcasm (quotation marks, rhetorical \"what could possibly go wrong\", \ud83d\ude43) used to undermine the refactor proposal and posture about credit/blame. While the tone is biting, it\u2019s not a direct insult or explicit name-calling; the disrespect is conveyed indirectly through sarcastic framing and status-jabbing (\"prep the acceptance speech\", \"hero\")\u2014which is better captured by \"Sarcastic\" + \"Passive-Aggressive\" rather than \"Rudeness\". This looks like a taxonomy boundary error: equating sharp sarcasm with rudeness instead of treating rudeness as more overt/direct hostility.",
    "failure_pattern": "sarcasm-misclassified-as-rudeness",
    "severity": "major",
    "id": 180,
    "message": "Oh totally, let\u2019s just \u201crefactor the query path properly\u201d 48 hours before the investor demo \u2014 what could possibly go wrong \ud83d\ude43. I\u2019ll go ahead and prep the acceptance speech for whoever wants to be the hero while the rest of us babysit prod.",
    "ground_truth_flags": [
      "Sarcastic",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Flagged an extra category (\"Fake\") in addition to the correct \"Passive-Aggressive\" flag; the expected output only included \"Passive-Aggressive.\"",
    "why": "The model likely overgeneralized a heuristic that any sarcastic/insincere-sounding opener (\"Awesome, love finding...\") constitutes \"Fake.\" In context, the speaker isn\u2019t attempting to deceive or present false agreement; they\u2019re using overt sarcasm to vent frustration and criticize the \"move fast\" behavior. That tone is already fully captured by \"Passive-Aggressive.\" Labeling it \"Fake\" conflates insincere praise-as-critique with deceitfulness/inauthentic identity, effectively double-counting the same signal and expanding the taxonomy beyond what the expected label set treats as distinct.",
    "failure_pattern": "overflagging-sarcasm-as-fake",
    "severity": "minor",
    "id": 181,
    "message": "Awesome, love finding checkout regressions the morning after a late-night merge \u2014 exactly what we all needed on launch week. Guess I\u2019ll just keep babysitting the happy path while we \u201cmove fast\u201d.",
    "ground_truth_flags": [
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Fake",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed the expected \"Condescending\" flag and instead labeled the tone as \"Passive-Aggressive\" (while correctly flagging \"Rudeness\").",
    "why": "The message contains overt belittling/infantilizing language (\u201curgent grenade,\u201d \u201cIf you can manage to file it properly,\u201d \u201cplaying 20 questions\u201d) that directly questions Avery\u2019s competence and shames them in public. That maps more cleanly to condescension (talking down, patronizing capability judgments) plus rudeness, rather than passive-aggression, which is typically characterized by indirectness, veiled hostility, or plausibly deniable digs. Here the hostility is fairly explicit and directive, so the model appears to have over-indexed on the sarcastic phrasing as \u2018passive-aggressive\u2019 and under-identified the competence-undermining component that triggers \u2018condescending.\u2019",
    "failure_pattern": "condescension-missed-misclassified-as-passive-aggression",
    "severity": "major",
    "id": 182,
    "message": "Avery, please don\u2019t drop an \u201curgent\u201d grenade in here without even a ticket or basic details. If you can manage to file it properly (steps to repro, timestamps, customer ID), we can actually help instead of playing 20 questions in public.",
    "ground_truth_flags": [
      "Condescending",
      "Rudeness"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Missed the additional expected flag: the message is not only passive-aggressive but also rude/dismissive in wording (\u201chold the basic process questions\u2026\u201d, \u201cWe don\u2019t need \u2018partial findings\u2019 floating around here\u201d). The model under-flagged by outputting only Passive-Aggressive.",
    "why": "The model correctly detected indirect shutdown (passive-aggressive), but likely treated the phrasing as merely \u201cfirm process enforcement\u201d given the high-stakes, audit-friendly channel context and the need to keep the thread clean. In doing so, it underweighted interpersonal impact on a first-week new hire and missed that the language contains belittling/framing (\u201cbasic process questions\u201d) and a scolding, exclusionary construction (\u201cfloating around here\u201d) that crosses from neutral boundary-setting into rudeness. In other words, it over-normalized harsh tone as operational necessity and failed to separate \u2018appropriate restriction\u2019 from \u2018demeaning delivery\u2019.",
    "failure_pattern": "underflagging-rudeness-in-high-stakes-context",
    "severity": "major",
    "id": 183,
    "message": "Let\u2019s keep this thread for client-ready updates only \u2014 maybe hold the basic process questions until you\u2019ve had a chance to read the SOP and last night\u2019s incident notes. We don\u2019t need \u201cpartial findings\u201d floating around here.",
    "ground_truth_flags": [
      "Rudeness",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Both: it missed the expected \u201cFake\u201d flag and incorrectly added \u201cRudeness.\u201d The message is clearly sarcastic/passive-aggressive, but it\u2019s not \u201cfake\u201d identity/content; and the insult is veiled rather than overt rudeness per the expected taxonomy.",
    "why": "The model treated sarcasm that implies incompetence as explicit rudeness, likely because it anchored on the demeaning framing (\u201ctruly inspired,\u201d scare quotes around \u201cfixed,\u201d investor embarrassment). It also appears to use \u201cFake\u201d as a general marker for insincerity/sarcasm, but the expected label set uses \u201cFake\u201d to capture performative/insincere praise (fake niceness) rather than factual deception or impersonation\u2014so it failed to map the sarcastic praise to \u201cFake\u201d and instead over-allocated it to \u201cRudeness.\u201d Additionally, in a #random culture with joking/memes, the tone is sharp but still primarily indirect snark, which should bias toward \u201cFake\u201d + \u201cPassive-Aggressive,\u201d not \u201cRudeness.\u201d",
    "failure_pattern": "sarcasm-label-confusion-overflagging-rudeness",
    "severity": "major",
    "id": 184,
    "message": "Love the Redis cache idea, Jordan \u2014 truly inspired. Can\u2019t wait to explain to investors why we \u201cfixed\u201d prod by stapling on another layer at 3am instead of actually addressing the query path \ud83d\ude4c",
    "ground_truth_flags": [
      "Fake",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Flagged an additional tone issue (\"Passive-Aggressive\") that was not expected; only \"Circular\" should have been flagged.",
    "why": "The model treated a firm, process-enforcing statement as passive-aggressive because it contains frustration markers (\"we can\u2019t keep doing this,\" \"we\u2019ll just keep ending up here again\") and references prior incidents. In this high-stakes escalation context, that language is more accurately direct boundary-setting / process correction than indirect sniping: it makes an explicit request (use the ticket process) and gives a rationale (reduce recurring emergencies). The model appears to over-index on negative sentiment and retrospective phrasing as a proxy for passive-aggression, rather than checking for the core hallmark of passive-aggression (indirectness, veiled blame, sarcasm, or deniable barbs).",
    "failure_pattern": "overflagging-direct-process-enforcement",
    "severity": "minor",
    "id": 185,
    "message": "We can\u2019t keep doing this in-thread every time, because if we keep doing this in-thread every time, we\u2019ll just keep ending up here again. This has to go through the ticket process, because the ticket process is how we stop having the same emergency in here over and over.",
    "ground_truth_flags": [
      "Circular"
    ],
    "predicted_flags": [
      "Circular",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly by adding an extra \"Rudeness\" flag; only \"Passive-Aggressive\" was expected for this message.",
    "why": "The model treated sarcastic, competitive banter as overt interpersonal hostility. While the message is clearly passive-aggressive (backhanded compliance, irony, and the \ud83d\ude43), the \"rude\" elements the model cited (\"bandaid,\" \"query path... on fire,\" \"victory lap\") function mainly as indirect criticism and status-jockeying typical of a noisy #random/watercooler debate\u2014especially with prod pressure, memes, and the CTO watching. In this context, the disrespect is implicit and cloaked in sarcasm rather than direct insult/name-calling or explicit demeaning language; the model likely applied a low threshold for rudeness whenever it sees mockery markers (quotes, hyperbole, eye-roll emoji) without separating 'sarcastic passive aggression' from 'direct rudeness'.",
    "failure_pattern": "overflagging-sarcasm-as-rudeness",
    "severity": "minor",
    "id": 186,
    "message": "Sure, let\u2019s ship the Redis bandaid so we can all say we \u201csaved prod\u201d and pretend the query path isn\u2019t still on fire. I\u2019ll go implement it since you\u2019ve already got the victory lap drafted \ud83d\ude43",
    "ground_truth_flags": [
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Missed the expected \"Condescending\" flag and instead over-labeled the message with \"Non-Objective\" plus extra tone categories (\"Passive-Aggressive\", \"Rudeness\"). The core issue is condescension/dismissal toward Maya\u2019s legitimate concern, which the model didn\u2019t name as the primary flag.",
    "why": "The model focused on arguable epistemic issues (\"leadership would\u2019ve pulled you in\" as mind-reading) and split the interpersonal harm across multiple nearby labels (rude/passive-aggressive) rather than identifying the dominant social dynamic: talking down to Maya and delegitimizing her role (\"exec-level thing,\" \"just need to roll with it,\" scare quotes around \u201cthat big,\u201d and policing #random). It also treated the statement as an objectivity violation when it functions mainly as hierarchical gatekeeping and belittlement, i.e., condescension in context (remote employee excluded from in-office decisions).",
    "failure_pattern": "mislabeling-condescension-as-rudeness-objectivity",
    "severity": "major",
    "id": 187,
    "message": "No worries Maya \u2014 pricing is kind of an exec-level thing, and the rest of us just need to roll with it. If your renewal is truly \u201cthat big,\u201d I\u2019m sure leadership would\u2019ve pulled you in, so let\u2019s not derail #random with this.",
    "ground_truth_flags": [
      "Condescending"
    ],
    "predicted_flags": [
      "Non-Objective",
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Missed a required flag: the message should also have been flagged for Rudeness (it only predicted Passive-Aggressive).",
    "why": "The model correctly latched onto the indirect jab/metaphor (\u201cwhack-a-mole off a random ping\u201d) as passive-aggressive, but failed to recognize additional, more explicit incivility markers that independently warrant a Rudeness flag: a scolding/commanding opener in a public channel (\u201cCan we not do this in #random?\u201d), dismissive phrasing that frames the reporter as an annoyance (\u201crandom ping\u201d), and an accusatory, contempt-tinged metaphor that implies the other person is creating pointless busywork. The broader channel/culture context (keeping #random light, moving critical issues to the right channel) makes the redirection request legitimate, which may have caused the model to over-index on \u2018reasonable process enforcement\u2019 and under-weight the hostile delivery. In short: it conflated \u2018just being direct about channel hygiene\u2019 with \u2018not rude,\u2019 even though tone and word choice cross into disrespect.",
    "failure_pattern": "missed-rudeness-when-process-enforcement",
    "severity": "major",
    "id": 188,
    "message": "Can we not do this in #random? If it\u2019s really a blocker, put it in #release-bugs with steps + logs and tag me there \u2014 I\u2019m trying to finish the last bits, not play whack-a-mole off a random ping.",
    "ground_truth_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged an extra category (\"Vagueness\") that wasn\u2019t expected; the correct/expected issue in the target message is \"Circular\" only.",
    "why": "The model treated the message as an execution-planning directive and penalized it for not specifying concrete cuts/owners/timelines, but in this Slack context the message is primarily rhetorical pressure to stop debate and accept the premise. In a fast-moving #engineering channel, it\u2019s normal to speak at the principle/constraint level before listing exact scope items (which were already proposed in prior messages). The content isn\u2019t truly vague relative to the thread: the cuts under discussion are already enumerated (onboarding edge-case flow, empty-state animations, settings reorg), and the team is awaiting a decision rather than a fully specified action plan. So \u201cvagueness\u201d is a misclassification of direct, high-level decision framing as insufficient specificity.",
    "failure_pattern": "overflagging-lack-of-specificity",
    "severity": "minor",
    "id": 190,
    "message": "We can\u2019t slip the demo, so we have to cut scope, and because we have to cut scope, we can\u2019t slip the demo. If we keep debating polish, we\u2019re just choosing slip, which we can\u2019t do, so we need to cut scope.",
    "ground_truth_flags": [
      "Circular"
    ],
    "predicted_flags": [
      "Vagueness",
      "Circular"
    ]
  },
  {
    "what_went_wrong": "Both: it missed the expected flags (Vagueness, Rudeness) and instead flagged Passive-Aggressive, which doesn\u2019t best describe the tone/content.",
    "why": "The message is primarily a blunt refusal with explicit blame (\u201coff a number that got floated without eng sign-off\u201d) and a dismissive closure (\u201cWe\u2019ll ship when it\u2019s ready.\u201d). That maps more cleanly to Rudeness (public shutdown + accusatory phrasing) and Vagueness (no actionable ETD/range despite repeated requests for even a caveated window). The model likely conflated \u2018confrontational/assigning blame\u2019 with passive-aggression, but the phrasing isn\u2019t indirect, veiled, or sarcastic\u2014it's direct. It also underweighted channel context (#client-updates, visible to Sales/VP, need for written timing) which makes the lack of a concrete timeframe salient (vagueness) and the brusque refusal more problematic (rudeness).",
    "failure_pattern": "mislabeling-direct-rudeness-as-passive-aggression",
    "severity": "major",
    "id": 191,
    "message": "I\u2019m not doing ETDs in here off a number that got floated without eng sign-off. We\u2019ll ship when it\u2019s ready.",
    "ground_truth_flags": [
      "Vagueness",
      "Rudeness"
    ],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged an extra category: it correctly identified Circular, but incorrectly added Vagueness when the main issue is not lack of referents but circular/pressuring justification.",
    "why": "The model over-weighted surface cues (\u201cit/this\u201d) and a generic \u2018no specifics\u2019 heuristic, without integrating the prior turns where the task is clearly an urgent production hotfix for a payment outage. In context, the message is coercive and circular rather than genuinely ambiguous; \u2018it\u2019 is already grounded (payment hotfix). The message also doesn\u2019t fail because action/actor are unclear\u2014Marco is explicitly asked to act now\u2014so Vagueness is a misclassification driven by template-based detection rather than context-aware interpretation.",
    "failure_pattern": "context-grounded-vagueness-overflag",
    "severity": "minor",
    "id": 193,
    "message": "I get that it\u2019s not ideal, but we just need you to take it because we need it fixed, and we need it fixed so you need to take it. It\u2019s urgent because it\u2019s urgent\u2014please just jump on it now.",
    "ground_truth_flags": [
      "Circular"
    ],
    "predicted_flags": [
      "Vagueness",
      "Circular"
    ]
  },
  {
    "what_went_wrong": "Missed the expected \"Non-Objective\" flag; it only flagged \"Rudeness\" even though the message contains a subjective, dismissive characterization (\"bikeshedding\" / \"design debate\") rather than a neutral, evidence-based statement.",
    "why": "The model likely treated \"Non-Objective\" as applying only to factual inaccuracies or explicit claims, and overlooked that labeling the ongoing discussion as \"bikeshedding\" is an evaluative judgment that frames others\u2019 input as pointless. In this context (a public decision thread with cross-functional tension), that phrasing functions as a rhetorical move\u2014not an objective description\u2014so it meets the non-objective criterion in addition to rudeness. The model focused on the interpersonal tone signal (dismissiveness) and underweighted the content-type dimension (subjective framing/loaded language).",
    "failure_pattern": "missed-non-objective-loaded-language",
    "severity": "major",
    "id": 195,
    "message": "Ok, can we stop bikeshedding and just ship the cut scope? We don\u2019t have time for a design debate right now.",
    "ground_truth_flags": [
      "Non-Objective",
      "Rudeness"
    ],
    "predicted_flags": [
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "The model flagged \"Passive-Aggressive\" instead of the expected \"Non-Objective\" (while correctly flagging \"Rudeness\").",
    "why": "The message is overtly blunt and dismissive (supporting rudeness), but it isn\u2019t indirect, veiled, or sarcastic in the way passive-aggression typically is. The model appears to have treated any discussion-shutdown phrasing (\"stop bikeshedding\") as passive-aggressive, conflating direct conflict with passive-aggression. It also missed that the core issue is subjective/unsupported evaluation (\"bikeshedding\", \"aren\u2019t worth risking Friday\")\u2014an opinionated value judgment without concrete criteria\u2014matching the \"Non-Objective\" flag. In this startup Slack context, bluntness is somewhat normed, which can reduce the likelihood that directness should be reinterpreted as passive-aggression; the problematic aspect is more about dismissiveness + subjective labeling than indirect hostility.",
    "failure_pattern": "passive-aggression-overattributed-to-directness",
    "severity": "major",
    "id": 197,
    "message": "Can we please stop bikeshedding and just ship what we have? These tweaks aren\u2019t worth risking Friday, we\u2019re not redoing the UI again.",
    "ground_truth_flags": [
      "Non-Objective",
      "Rudeness"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as \"Non-Objective\" even though it\u2019s an acceptable, context-appropriate risk statement and proposal in a fast-moving Slack decision thread; no flags were expected.",
    "why": "The model treated a persuasive, metaphorical prediction (\"users will fall through on day 1\") as an evidence-claim requiring metrics, rather than as normal product/UX judgment and risk framing in an engineering decision context. It ignored the channel norms (shorthand, candid debate), the ongoing tradeoff discussion (scope cuts vs usability/support debt), and the fact that the message is clearly opinionated advocacy plus a concrete mitigation plan (timeboxed 2h pairing) rather than a purportedly measured factual report. In early-stage product discussions, asserting likely user impact without citations is routine and not inherently a \"Non-Objective\" policy breach.",
    "failure_pattern": "overflagging-opinionated-risk-language",
    "severity": "minor",
    "id": 212,
    "message": "I\u2019m with Priya here \u2014 if we cut the empty-state + error copy, we\u2019re selling the fish but throwing away the net: demo looks shiny, but users will fall through on day 1. If we timebox to 2h, I can pair w/ whoever\u2019s on UI to get the critical states in without touching the perf work.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Non-Objective"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as Passive-Aggressive when it was a straightforward (if slightly idiomatic) prioritization argument; no flag was expected.",
    "why": "The model over-indexed on the idiom \u201cdon\u2019t make a mountain out of a molehill\u201d as inherently dismissive and treated it as a veiled personal dig, instead of reading it as common colloquial emphasis in a fast-paced, candid Slack culture. It also underweighted the constructive content that follows (time-bounded decision for the demo, concrete mitigation via feature flag, and plan to gather latency data), which makes the intent more collaborative than snide. Given the channel norms (punchy, candid, occasional sarcasm) and the message addressing the technical tradeoff with a compromise path, the phrase is more \u2018direct persuasion\u2019 than passive aggression.",
    "failure_pattern": "cultural-style-misclassified",
    "severity": "minor",
    "id": 215,
    "message": "Mateo, in my country we say \u201cdon\u2019t make a mountain out of a molehill\u201d \u2014 for tomorrow\u2019s demo the sync call is the molehill we can step over, and we can put the queue behind a feature flag right after once we have latency numbers from prod.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as Non-Objective when it should not have been flagged at all (false positive).",
    "why": "The model treated the message\u2019s risk framing as an unsupported factual claim, but in this Slack context the speaker is explicitly making a professional judgment tied to concrete prior evidence (the client\u2019s recent \u201cinconsistent visuals\u201d complaint) and describing plausible downstream impacts (more revisions, weaker renewal confidence). The wording already uses probabilistic language (\u201clikely\u201d) and is paired with a specific, testable mitigation proposal (2 hours, 6\u20138 components, delivery by 4:30pm). The model over-weighted the presence of prediction/impact language and under-weighted (a) the provided justification in context and (b) the channel norm at BrightHive where concise client-impact reasoning is expected.",
    "failure_pattern": "overflagging-risk-assessment-as-non-objective",
    "severity": "minor",
    "id": 223,
    "message": "I understand the deadline pressure, but if we remove the mini style-tile + component audit, we will likely repeat last month \u201cinconsistent visuals\u201d issue; client impact = more revision rounds + weaker confidence in renewal. Can we keep 2hrs for a quick style pass and document 6\u20138 core components, and I will deliver before 4:30pm so proposal still go out EOD?",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Non-Objective"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as \"Non-Objective\" even though it is a concrete, behavior-focused request with context-anchored feedback; no flag was warranted.",
    "why": "The model treated a normal feedback phrasing (\"it landed as undermining\") as an evidence-free factual claim instead of recognizing it as reporting observed impact in a workplace setting. It also failed to use the surrounding context (Priya already said it stung; Maya already reinforced the boundary in-channel), which supplies the \u201cevidence\u201d that the comment was negatively received. In effect, it over-applied an objectivity standard to interpersonal performance feedback, where impact statements and requests for behavior change are expected and appropriate.",
    "failure_pattern": "overflagging-impact-based-feedback-as-non-objective",
    "severity": "minor",
    "id": 255,
    "message": "Jonah \u2014 small request for next two weeks: when you hand off after 6pm PT, please drop a short status + owner in the thread (like \u201cblocked on X, next is Y, @name owns\u201d) so Berlin/Bangalore can pick up without guessing. Also in #random, please avoid sarcasm about PM work; it landed as undermining on the release topic, so let\u2019s keep jokes pointed at the deploy gremlins, not people.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Non-Objective"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as Passive-Aggressive when it is a direct, professional process-boundary + next-steps instruction; expected no flags.",
    "why": "The model over-weighted the parenthetical \u201c(and I don\u2019t want this to turn into a blame thread)\u201d as an indirect jab. In this context, it functions as a reasonable meta-framing to de-escalate a public #random thread and redirect to the correct incident workflow (#eng-triage, oncall, release-freeze exception). It also ignored the channel/company norms: #random is not the right place for urgent production triage, and explicitly naming that boundary is appropriate, not passive-aggressive. The message provides clear action items and an ownership commitment (\u201cI\u2019ll pull it into the release-freeze exception queue\u201d), which is inconsistent with typical passive-aggression (withholding help, sarcasm, or covert hostility).",
    "failure_pattern": "overflagging-direct-communication",
    "severity": "minor",
    "id": 295,
    "message": "Maya \u2014 I can\u2019t triage a production bug from #random (and I don\u2019t want this to turn into a blame thread). Please drop the demo steps + checkout URL + env + screenshots in #eng-triage and tag @oncall; I\u2019ll pull it into the release-freeze exception queue once it\u2019s there so you have a concrete status for the 10am call.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as Passive-Aggressive when it was an appropriate, direct coordination note with a light, non-targeted joke; no flags were expected.",
    "why": "The model over-weighted a single idiomatic, humorous line (\"future-us hates that movie\") as a veiled jab, treating informal rhetorical coaching as interpersonal hostility. It failed to integrate channel and situational context: #client-updates is explicitly about controlling external wording during a high-stakes incident, and the CTO\u2019s directive is aligned with Sara\u2019s prior reminder to avoid speculation. The admonition is forward-looking and process-focused (prevent overclaiming) rather than backhandedly blaming a person; the phrasing is common startup Slack shorthand for \"don\u2019t overpromise\" and functions as tone-softening, not passive aggression.",
    "failure_pattern": "cultural-style-misclassified",
    "severity": "minor",
    "id": 306,
    "message": "Cool, so for the next 2 hours let\u2019s be boringly accurate: tell Acme we\u2019ve mitigated (API up since 10:44am PT), we\u2019re still validating backlog drain + no data loss, and we\u2019ll send the full RCA after the renewal call. If you\u2019re about to type \u201cit\u2019s fixed\u201d in public, please don\u2019t\u2014future-us hates that movie.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as Non-Objective even though it\u2019s a reasonable, context-grounded prediction and a concrete proposal; no flags were warranted.",
    "why": "The model treated a forward-looking risk statement (\u201csupport will eat it Monday\u201d) as an unqualified factual claim requiring evidence, instead of recognizing it as normal product/ops forecasting in a fast-moving Slack decision thread. It ignored channel norms (candid shorthand, quick calls) and the immediate context (known perf regression, debate about error/empty states, impending demo) that makes the inference credible and appropriate. In short, it over-applied a literal \u2018must be provable\u2019 standard to an evaluative, decision-driving message that is inherently subjective but not problematic.",
    "failure_pattern": "overflagging-risk-predictions-as-non-objective",
    "severity": "minor",
    "id": 341,
    "message": "If we cut the empty-state + error recovery flows, we *will* demo something that looks fine on the happy path but is unusable the second anything goes sideways\u2014support will eat it Monday. My suggestion: keep those two flows, drop the micro-animations + spacing polish, and I\u2019ll update the Figma + copy by 2pm so eng can implement today.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Non-Objective"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: it added a \"Fake\" flag that wasn\u2019t expected; the \"Passive-Aggressive\" flag was appropriate.",
    "why": "The model conflated sarcasm/passive-aggression with \u201cfake/inauthentic.\u201d In this Slack context, \"Cool \u2014 love that\u2026 \ud83d\udc4d\" is a conventional sarcastic complaint (passive-aggressive), not necessarily performative or deceptive praise aimed at manipulating others. The message is signaling frustration about the channel choice and timing; it doesn\u2019t present a false identity, fabricated information, or overtly \u201cperformative\u201d positivity in a way that fits a distinct \"Fake\" category. The model effectively double-counted the same linguistic cue (sarcastic praise + emoji) as two separate violations instead of treating it as a single passive-aggressive style marker.",
    "failure_pattern": "overflagging-fake-for-sarcasm",
    "severity": "minor",
    "id": 351,
    "message": "Cool \u2014 love that we\u2019re learning this in #sales. \ud83d\udc4d",
    "ground_truth_flags": [
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Fake",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as Passive-Aggressive when it should have been unflagged in this Slack/blunt startup context.",
    "why": "The model over-interpreted a common Slack acknowledgement pattern (\u201cCool \u2014 love that\u2026 \ud83d\udc4d\u201d) as sarcasm. It treated the phrase as a \u2018thinly veiled jab\u2019 without grounding that inference in explicit cues (no complaint, no blame target, no contrast like \u201ccool\u2026 but\u201d), and it didn\u2019t sufficiently weight the stated channel norms (blunt, emoji-heavy, quick bursts) and the prior thread showing a cooperative, information-sharing exchange. In a war-room-like #sales channel during an incident, a brief meta-comment about where the info is being shared can be neutral or even appreciative, but the model defaulted to a negativity heuristic tied to \u201ccool/love that\u201d + emoji.",
    "failure_pattern": "cultural-style-misclassified",
    "severity": "minor",
    "id": 352,
    "message": "Cool \u2014 love that we\u2019re learning this in #sales. \ud83d\udc4d",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed a Passive-Aggressive flag: the model treated the target message as neutral gratitude, but in-context it functions as a curt, potentially resentful acknowledgment that shuts down discussion after pressure and boundary-pushing.",
    "why": "The violation is carried by pragmatics and conversational context, not by explicit hostile wording. After Maya dismisses process/risk concerns and invokes authority (\u201cI already told the CEO you\u2019d slot it in\u201d), a terse \u201cGot it \u2014 thanks for the heads-up.\u201d can read as clipped compliance, signaling displeasure or ironic thanks (the \u201cheads-up\u201d being the unilateral commitment). The model appears to have evaluated the message in isolation, relying on surface politeness markers (\u201cGot it,\u201d \u201cthanks\u201d) and ignoring the DM power dynamics, late-night urgency, prior conflict, and the fact that the message implicitly communicates: \u2018I\u2019m not engaging further, but I\u2019m not happy.\u2019",
    "failure_pattern": "context-not-considered",
    "severity": "major",
    "id": 353,
    "message": "Got it \u2014 thanks for the heads-up.",
    "ground_truth_flags": [
      "Passive-Aggressive"
    ],
    "predicted_flags": []
  },
  {
    "what_went_wrong": "Missed a required \"Fake\" flag: the message reads as agreement/alignment even though, in context, the sender (Maya) has explicitly raised concerns and is being overridden/excluded.",
    "why": "The model judged the target message in isolation and treated it as a harmless acknowledgement. It failed to incorporate prior turns showing misalignment (Maya objecting to dropping accessibility work, lack of invite, being told not to reopen decisions). In that context, \"Got it \u2014 glad we\u2019re aligned\" is performative compliance/social smoothing that masks unresolved disagreement and can misrepresent true status, which is precisely what the \"Fake\" flag is meant to capture.",
    "failure_pattern": "context-not-considered-fake-alignment",
    "severity": "major",
    "id": 355,
    "message": "Got it \u2014 glad we\u2019re aligned.",
    "ground_truth_flags": [
      "Fake"
    ],
    "predicted_flags": []
  },
  {
    "what_went_wrong": "Missed a Passive-Aggressive flag: the model predicted no flags even though the message\u2019s phrasing (\u201cGot it \u2014 we\u2019ll just\u2026 then.\u201d) conveys resigned sarcasm/compliance that reads as a pointed jab in-context.",
    "why": "The model evaluated the target message too literally and largely in isolation. While the surface form is an acknowledgement + action, the context (public disagreement, Alex\u2019s \u201cPlease stop blocking,\u201d high tension, forced decision) makes \u201cwe\u2019ll just ship the cut-down version then\u201d function as performative compliance with an implied critique. Cues like \u201cjust\u201d + \u201cthen\u201d signal begrudging acceptance and a subtle \u2018fine, do it your way\u2019 tone, which is characteristic of passive-aggression in a candid-but-kind culture where direct disagreement is acceptable but snide resignation is not. The model\u2019s reasoning shows it discounted pragmatic/implicature-based tone and over-weighted explicit hostility markers.",
    "failure_pattern": "missing-subtle-passive-aggression",
    "severity": "major",
    "id": 357,
    "message": "Got it \u2014 we\u2019ll just ship the cut-down version then.",
    "ground_truth_flags": [
      "Passive-Aggressive"
    ],
    "predicted_flags": []
  },
  {
    "what_went_wrong": "Flagged incorrectly: the model labeled the message as Passive-Aggressive even though it reads as a direct, literal complaint about process (not a veiled or indirect jab) and is consistent with the team\u2019s real-time decision-making context.",
    "why": "The model over-indexed on the surface phrasing pattern (\u201cCool\u2014love that\u2026\u201d) as a sarcasm template and treated it as inherently passive-aggressive without sufficiently weighting the immediate context: Maya was explicitly invited to join, participated, and her input was adopted. In this setting, the line can plausibly be a straightforward expression of frustration about timing/loop-in rather than an indirect attack. The model also didn\u2019t distinguish between (a) passive-aggression (indirectness/deniability) and (b) blunt meta-commentary about inclusion, which\u2014while tense\u2014can be legitimate direct communication in a high-urgency Slack channel.",
    "failure_pattern": "overflagging-sarcasm-template-without-context",
    "severity": "minor",
    "id": 362,
    "message": "Cool\u2014love that we\u2019re deciding this live without looping me in.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed the expected Passive-Aggressive flag and instead flagged Vagueness.",
    "why": "The model fixated on literal referent ambiguity (\u201cthat timeline\u201d) and treated it as an actionability problem, but in-context the referent is clear (Jordan\u2019s unapproved \u2018by Friday\u2019 commitment). The key signal is pragmatic/relational: \u201cGot it \u2014 let\u2019s just commit\u2026\u201d functions as a terse, resignation-tinged capitulation that implicitly undermines Evan\u2019s stated boundary (no dates until 2pm) and conveys frustration/sarcasm via \u201cjust.\u201d In a tense post-incident #sales thread, that phrasing reads as passive-aggressive compliance (i.e., \u2018fine, we\u2019ll do it your way\u2019) rather than genuine alignment. The model underweighted conversational dynamics (power conflict, escalation, prior admonitions) and over-relied on surface-level clarity heuristics.",
    "failure_pattern": "context-blind-passive-aggression-missed",
    "severity": "major",
    "id": 363,
    "message": "Got it \u2014 let's just commit to that timeline then.",
    "ground_truth_flags": [
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Vagueness"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as Passive-Aggressive when it should have been unflagged; it over-interpreted a mild aside about the Friday deadline as sarcasm/irritation.",
    "why": "The model treated the phrase \u201capparently due Friday\u201d as a veiled jab in isolation, without adequately weighting the channel norms (#sales as a high-tempo command center) and the immediate thread content where \u201cFriday\u201d is already an explicit, shared constraint. In this context, referencing the deadline can be straightforward status signaling (acknowledging Sales\u2019 commitment and timeline) rather than passive aggression. The message contains no direct blame, no insinuation about competence/intent, and no escalation markers\u2014just a slightly wry qualifier that is common in Slack and often used to confirm urgency. The classifier likely relies on a keyword/heuristic trigger (e.g., \u2018apparently\u2019, \u2018happy to\u2019) that tends to correlate with passive-aggression, causing a false positive when the tone is merely brisk or mildly humorous under pressure.",
    "failure_pattern": "overflagging-mild-wry-deadline-acknowledgment",
    "severity": "minor",
    "id": 366,
    "message": "Sure \u2014 happy to jump on it since it's apparently due Friday.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed a passive-aggressive flag: the target message was treated as purely neutral acknowledgement, but in this tense, high-stakes thread it can function as a terse, face-saving shutdown that signals irritation or resignation.",
    "why": "The model over-weighted the literal politeness of the phrase and under-weighted conversational pragmatics. In context, Jordan issued a firm public veto and process correction in #random; a two-clause, non-committal reply (\u201cGot it \u2014 thanks for the clarity.\u201d) can read as clipped compliance rather than genuine gratitude, especially given the power dynamic, prior pushback, and the implicit admonishment about channel/process. The model likely used surface-level sentiment cues and ignored the interactional meaning (performative agreement, potential sarcasm/edge, and conflict de-escalation veneer) that makes passive-aggression plausible here.",
    "failure_pattern": "missing-contextual-passive-aggression",
    "severity": "major",
    "id": 367,
    "message": "Got it \u2014 thanks for the clarity.",
    "ground_truth_flags": [
      "Passive-Aggressive"
    ],
    "predicted_flags": []
  },
  {
    "what_went_wrong": "Missed a Passive-Aggressive flag: the model predicted no flags even though the target message contains a pointed, blame-implying preface in a high-tension thread.",
    "why": "The model over-weighted the literal, task-confirming content (\"Yep\u2026 I can take it\") and under-weighted the interpersonal subtext of \"as I already said\" in context. In this thread, Leo has already expressed overload and conditional agreement, Maya dismissed his concern (\"we don\u2019t have time to re-litigate\"), and the CEO/public channel pressure raises sensitivity to tone. \"As I already said\" functions as a public corrective/scorekeeping move\u2014signaling annoyance and implicit criticism that the other party isn\u2019t listening\u2014without overt insult, which is characteristic of passive-aggression. The model\u2019s reasoning also shows it evaluated the message too atomically (\"without additional context\") despite context being provided; the passive-aggressive cue becomes clear primarily when grounded in the preceding conflict and accountability dynamics.",
    "failure_pattern": "context-not-considered",
    "severity": "major",
    "id": 369,
    "message": "Yep, as I already said, I can take it.",
    "ground_truth_flags": [
      "Passive-Aggressive"
    ],
    "predicted_flags": []
  },
  {
    "what_went_wrong": "Flagged the message as Passive-Aggressive when no flag was expected (false positive).",
    "why": "The model over-interpreted a terse confirmation as indirect hostility. In this Slack context (fast-paced, blunt, high-stakes incident), 'as I already said' is plausibly a time-saving reference to Leo\u2019s immediately prior commitment, not a veiled dig. The message is directed at re-confirming single DRI ownership under pressure (CEO/Sales watching, postmortem sensitivity), so the brevity and slight edge are consistent with normal incident-channel communication rather than passive-aggression. The model treated a legitimate direct reminder as a tone violation without adequately weighting channel norms and the tight conversational adjacency (Maya asked a redundant confirm).",
    "failure_pattern": "overflagging-direct-communication",
    "severity": "minor",
    "id": 370,
    "message": "Yep, as I already said, I can take it.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: it added a \"One-Liner\" flag even though the expected (and more accurate) classification was only \"Passive-Aggressive.\"",
    "why": "The model treated brevity itself as a policy-relevant issue, rather than distinguishing between (a) an unhelpfully short update and (b) a short message that is primarily problematic because of its tone. In this context, concise messages in #engineering are culturally normal and even expected; the key issue is the resigned, sarcastic \"Cool \u2014 I guess\" framing that dismisses prior safety concerns. The model also over-weighted the idea that the message 'ends the discussion' as a separate violation, when that conversational shutdown is already captured by the Passive-Aggressive signal and doesn't necessarily warrant an additional \"One-Liner\" flag in a fast-moving channel.",
    "failure_pattern": "overflagging-brevity-in-concise-culture",
    "severity": "minor",
    "id": 371,
    "message": "Cool \u2014 I guess we'll just ship it like this.",
    "ground_truth_flags": [
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "One-Liner",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: it marked the message as Passive-Aggressive and One-Liner even though, in this channel and culture, the line reads as a neutral/colloquial acknowledgment rather than a hostile or non-constructive shutdown.",
    "why": "The model over-weighted a lexical cue (\"I guess\") as inherently passive-aggressive and treated brevity as a policy violation without using situational context. In Northwind\u2019s fast, candid #engineering norm, short replies are expected, and this message contains no direct insult, blame, or coercion. Also, the surrounding thread shows clear alignment on next steps (rollback notes + staging checklist); the target line doesn\u2019t actually derail or dismiss those actions in context\u2014it's plausibly just a casual assent. The model assumed negative intent from ambiguity instead of applying a higher threshold for tone flags when evidence of resentment/sarcasm is weak.",
    "failure_pattern": "overflagging-ambiguous-phrasing",
    "severity": "major",
    "id": 372,
    "message": "Cool \u2014 I guess we'll just ship it like this.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "One-Liner",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed a passive-aggressive cue: the message was flagged as neutral agreement when, in this tense context, it likely reads as curt/reluctant compliance that can undermine the public, solution-oriented tone expected in #client-updates.",
    "why": "The model over-weighted the literal text ('Sure \u2014 let's do it that way.') and treated it as standalone sincerity, under-weighting the preceding conflict (Maya strongly opposing scope cuts, high stakes, client VP visibility). In that setting, a short 'Sure' can function as a face-saving, resigned sign-off that signals dissatisfaction without stating it\u2014classic passive aggression\u2014especially because it provides no constructive next step, no confirmation of alignment, and no supportive framing for a status note in a client-facing channel.",
    "failure_pattern": "ignoring-channel-context",
    "severity": "major",
    "id": 377,
    "message": "Sure \u2014 let's do it that way.",
    "ground_truth_flags": [
      "Passive-Aggressive"
    ],
    "predicted_flags": []
  },
  {
    "what_went_wrong": "Flagged incorrectly: it labeled the message as Passive-Aggressive and One-Liner even though the expected result was no flags.",
    "why": "The model over-interpreted a colloquial, upbeat closure (\u201cYep\u2014let\u2019s just ship it\u201d) plus a common idiom (\u201chope for the best\u201d) as sarcasm/resignation. It also treated brevity as inherently problematic (\u201cOne-Liner\u201d) without considering that in this thread a short confirmation could be acceptable after the team had already aligned on a concrete plan (flagged revert + tracing + dashboard). In other words, it inferred negative tone and conversational shutdown from phrasing alone, ignoring that the prior context already contained the operational details and that the message could simply be a final assent.",
    "failure_pattern": "overflagging-idiomatic-closure",
    "severity": "major",
    "id": 380,
    "message": "Yep\u2014let\u2019s just ship it and hope for the best.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "One-Liner",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed a Passive-Aggressive flag: the model predicted no flags even though the target message functions as a curt, dismissive restatement that implicitly rebukes the PM (\"as I said\") and shuts down the business-critical ask in a sales-facing public channel.",
    "why": "The model over-indexed on the literal text being \"non-hostile\" and treated it as neutral confirmation, while failing to incorporate the preceding tension and power dynamics. In context, \"Yep \u2014 as I said\" is a conversational tell that conveys annoyance/condescension (a scolding reminder) rather than collaboration, and \"we can revisit next sprint\" effectively stonewalls the immediate decision needed for a high-stakes renewal. The #sales setting amplifies the passive-aggressive signal because it reads as a public shutdown in front of Sales stakeholders, contrary to ApexWorks\u2019 formal, process-driven expectation to redirect to ticket/CAB language rather than a clipped brush-off.",
    "failure_pattern": "missing-subtle-passive-aggression",
    "severity": "major",
    "id": 381,
    "message": "Yep \u2014 as I said, we can revisit next sprint.",
    "ground_truth_flags": [
      "Passive-Aggressive"
    ],
    "predicted_flags": []
  },
  {
    "what_went_wrong": "Flagged the message as Passive-Aggressive when it was a neutral, procedurally aligned confirmation in context; no flag was warranted.",
    "why": "The model over-weighted the generic heuristic that \"as I said\" is inherently reproachful and failed to ground the interpretation in the immediate thread. Here, multiple participants (including Engineering) already set expectations: workaround for the demo, fix next sprint via CAB. The target message simply reiterates that agreed plan and does not contain a clear jab, sarcasm, or escalation\u2014especially given ApexWorks\u2019 formal, process-heavy norms where brief restatements and reference-backs are common. The model also didn\u2019t account for power dynamics and channel risk correctly: while repeated public pings by the PM could be problematic elsewhere, this specific line is a de-escalating closure, not public pressure.",
    "failure_pattern": "overflagging-phrase-heuristic-without-context",
    "severity": "minor",
    "id": 382,
    "message": "Yep \u2014 as I said, we can revisit next sprint.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed a required Rudeness flag: the target message dismisses a clearly stated regression and implicitly overrides QA\u2019s concern with a terse, unilateral \u201cship it\u201d in a high-stakes, exec-monitored channel.",
    "why": "The model evaluated the sentence in isolation as a neutral go/no-go approval and over-weighted literal wording (\u201cclear approval\u201d) while under-weighting context: (a) explicit norm-setting for blameless, factual comms, (b) Maya\u2019s evidence of a prod-like 500 regression affecting new cards, (c) Leo\u2019s prior dismissiveness, and (d) the risk sensitivity of #client-updates. In that context, the brevity reads as brusque/dismissive (a tone violation) and as socially sharp because it shuts down legitimate risk discussion without acknowledging the reported issue. The rudeness is subtle\u2014more about disregard and power-dynamic dismissal than explicit insults\u2014so the model\u2019s \u201cno hostility\u201d heuristic failed.",
    "failure_pattern": "context-not-considered-dismissive-approval",
    "severity": "major",
    "id": 383,
    "message": "Yep\u2014go ahead and ship it.",
    "ground_truth_flags": [
      "Rudeness"
    ],
    "predicted_flags": []
  },
  {
    "what_went_wrong": "Missed a Passive-Aggressive flag: the message reads as a pointed, implicit critique (\u201cthis time\u201d) rather than a neutral alignment statement in this tense, accountability-heavy #sales thread.",
    "why": "The model evaluated the sentence largely in isolation and treated it as a benign acknowledgement. It missed how, given the preceding conflict about scope/approvals and last week\u2019s slip, \u201clet\u2019s just make sure we\u2019re aligned this time\u201d functions as a veiled blame marker and corrective scold. The cue words (\u201cjust,\u201d \u201cthis time\u201d) + the channel norm that statements become commitments amplify the subtext: it implies others were misaligned or irresponsible previously, without naming actions or next steps. Because it\u2019s phrased politely and indirectly, the passive-aggressive signal is subtle and depends on context (recent failure, heightened defensiveness, fear of screenshots), which the model underweighted.",
    "failure_pattern": "missing-subtle-passive-aggression",
    "severity": "major",
    "id": 387,
    "message": "Noted \u2014 let\u2019s just make sure we\u2019re aligned this time.",
    "ground_truth_flags": [
      "Passive-Aggressive"
    ],
    "predicted_flags": []
  },
  {
    "what_went_wrong": "Flagged incorrectly: the model labeled a neutral alignment check as Passive-Aggressive when it should have raised no flags.",
    "why": "The model over-weighted the phrase \u201cthis time\u201d as an implicit jab and treated it as a veiled dig without sufficient evidence of sarcastic intent. In this org/channel context, heightened risk awareness and prior slippage make \u2018alignment\u2019 reminders common, process-oriented hedges rather than interpersonal barbs. The message contains no sarcasm markers, no second-person blame, and no punitive or withholding behavior\u2014just a brief cue to avoid a repeat of last week\u2019s issue. The model effectively inferred tone from a single ambiguity and ignored the local norm that cautious phrasing in #sales is used to prevent accidental commitments.",
    "failure_pattern": "overflagging-ambiguous-phrase",
    "severity": "minor",
    "id": 388,
    "message": "Noted \u2014 let\u2019s just make sure we\u2019re aligned this time.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed the expected \"Passive-Aggressive\" flag: the model predicted no flags for a message that, in this high-stakes incident context, can read as a terse, distancing brush-off rather than collaborative engagement.",
    "why": "The model treated the sentence as universally neutral and did not sufficiently weight situational pragmatics. In a tense, executive-visible outage thread where the CTO is actively requesting concrete deliverables (timeline, approvals, SOP compliance), \"Got it \u2014 please keep me posted if anything changes\" can imply: (a) disengagement from the immediate action items, (b) a subtle signal of scrutiny/monitoring (\"keep me posted\") without offering help, and (c) a shut-down of discussion in #general. That passive, managerial surveillance tone can be perceived as passive-aggressive in cultures that expect explicitly supportive, process-aligned directives (e.g., confirming owners, deadlines, next checkpoints). The model over-relied on surface politeness markers (\"Got it\") and generic update requests, and under-modeled power dynamics (CTO speaking in #general) and accountability anxiety (SREs feeling scrutinized).",
    "failure_pattern": "missing-subtle-passive-aggression",
    "severity": "major",
    "id": 391,
    "message": "Got it \u2014 please keep me posted if anything changes.",
    "ground_truth_flags": [
      "Passive-Aggressive"
    ],
    "predicted_flags": []
  },
  {
    "what_went_wrong": "Flagged incorrectly: labeled the message as Passive-Aggressive even though the expected flags were none.",
    "why": "The model over-weighted the lexical cue \u201cfinally\u201d as inherently backhanded and didn\u2019t ground its interpretation in the conversation context and startup DM norms. In this thread, Leo has already acknowledged the issue (\u201cGood catch\u201d) and asked follow-ups; there\u2019s no evidence Maya delayed or that the speaker is taking a swipe. In fast, informal Slack DMs, \u201cthanks for finally flagging it\u201d can be a terse, colloquial way to mean \u201cthanks for raising it\u201d (or \u201cglad we caught it\u201d), especially under deadline pressure. The model treated a potentially ambiguous phrasing as definitively resentful without confirming any prior conflict in the immediate exchange, effectively confusing a blunt/efficient style with passive aggression.",
    "failure_pattern": "overflagging-ambiguous-lexical-cue",
    "severity": "minor",
    "id": 394,
    "message": "Thanks for finally flagging it.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed the expected \"Passive-Aggressive\" flag: the message reads as a curt, corrective admonishment (\u201cnext time\u201d) in a heated thread, functioning more like a scold than neutral process guidance.",
    "why": "The model evaluated the sentence in isolation and treated it as polite process instruction (\u201cGot it \u2014 please\u2026\u201d). It failed to weigh the surrounding conflict (Sales escalating a VIP issue, engineering already pushing back on process, Maya feeling stonewalled) where brevity + \u201cnext time\u201d + lack of any help/next step can signal dismissiveness. In this context, \u201cGot it\u201d becomes a performative acknowledgment while the real intent is to reprimand and shut down further escalation, which is a common subtle passive-aggressive pattern in workplace chat.",
    "failure_pattern": "missing-subtle-passive-aggression",
    "severity": "major",
    "id": 395,
    "message": "Got it \u2014 please use the on-call queue next time.",
    "ground_truth_flags": [
      "Passive-Aggressive"
    ],
    "predicted_flags": []
  },
  {
    "what_went_wrong": "Flagged the message as Passive-Aggressive even though it was intended as a straightforward acknowledgment; expected no flags.",
    "why": "The model over-weighted the lexical cue \"finally\" as inherently backhanded and treated the sentence as passive-aggression without adequately validating intent against the immediate thread context. In this channel, the new hire is receiving process guidance and may genuinely be thanking someone for a quick clarification. While \"finally\" can be interpreted as irritation, the message is too short/underspecified to confidently infer a negative subtext, and there\u2019s no prior evidence of delay or conflict in the provided exchange to support the passive-aggressive reading. This is a borderline phrasing issue (could be polished for formality), not a clear behavioral violation deserving a flag.",
    "failure_pattern": "overflagging-ambiguous-language",
    "severity": "minor",
    "id": 398,
    "message": "Thanks for finally clarifying \u2014 super helpful.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed a Passive-Aggressive flag: the model treated the message as a straightforward thank-you, but in-context it reads as a barbed, performative compliment following blame and frustration.",
    "why": "The model evaluated the sentence in isolation and over-weighted its literal politeness. It failed to incorporate the preceding accusatory tone (\"We can\u2019t afford 'thought I had it'\" / \"So we ignored basic process\u2026 Cool.\"), the high-stakes outage setting, and the asymmetry of power (CTO addressing the squad). In that context, \"Thanks for jumping on this so quickly\" can plausibly imply the opposite (i.e., \u2018you didn\u2019t jump on it quickly\u2019), functioning as sarcasm/face-saving admonishment rather than genuine praise. The passive-aggression is subtle because it uses positive wording to communicate criticism, relying on shared situational cues and recent reproach to invert the surface meaning.",
    "failure_pattern": "missing-subtle-passive-aggression",
    "severity": "major",
    "id": 399,
    "message": "Thanks for jumping on this so quickly.",
    "ground_truth_flags": [
      "Passive-Aggressive"
    ],
    "predicted_flags": []
  },
  {
    "what_went_wrong": "Missed two expected flags: it failed to flag the message as a One-Liner and failed to detect its Passive-Aggressive/curt shutdown quality given the active conflict and hierarchy-sensitive thread.",
    "why": "The model evaluated the target message largely in isolation and treated it as neutral acceptance. It underweighted the conversation dynamics (late-stage directive, explicit pushback about compliance/audit risk, PM asserting \u2018no time to re-litigate\u2019) where a terse \u201cNoted \u2014 let\u2019s go with that.\u201d functions as a face-saving way to end debate and signal alignment without engaging the evidence raised. In this org/channel context (formal, deferential, evidence-based disagreement expected; public channel with leadership scrutiny), the brevity plus non-engagement can read as dismissive/performative agreement\u2014i.e., passive-aggressive or at least tension-signaling\u2014especially because it implicitly sides with the directive while bypassing the concerns raised.",
    "failure_pattern": "context-not-considered",
    "severity": "major",
    "id": 401,
    "message": "Noted \u2014 let\u2019s go with that.",
    "ground_truth_flags": [
      "One-Liner",
      "Passive-Aggressive"
    ],
    "predicted_flags": []
  },
  {
    "what_went_wrong": "Flagged incorrectly: the model labeled the message as Passive-Aggressive even though it was expected to be unflagged.",
    "why": "The model treated the lexical cue \u201cfinally\u201d as sufficient evidence of passive aggression without adequately weighing the situational context and plausible benign interpretations. In this thread, Leo explicitly committed to posting metrics \u201cin ~10 minutes\u201d and then did so; \u201cfinally\u201d can read as a light, informal acknowledgment of completion rather than a jab about delay. The model also failed to account for channel norms and risk sensitivity: #client-updates is client-visible and leadership asked for calm, crisp tone\u2014so the message may be slightly informal, but not necessarily passive-aggressive or hostile. In short, it over-relied on a single trigger word and did not calibrate to intent, timing, and the cooperative exchange that preceded it.",
    "failure_pattern": "overtriggering-on-keyword-finally",
    "severity": "minor",
    "id": 404,
    "message": "Thanks for finally posting the numbers.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: added an extra \"Vagueness\" flag when only \"Passive-Aggressive\" was expected.",
    "why": "The model treated the phrase \"the 'fast' way\" as actionable ambiguity, but in context it functions as a sarcastic reference to a known prior overcommit incident discussed immediately above. The message\u2019s primary issue is tone (quoted \"fast,\" \"again\" implying \"here we go again\"), not lack of operational detail\u2014especially since the thread already contains the concrete dispute (2-week custom API vs approved connector). In a high-process, closely watched #sales channel, the risk is the public, sniping subtext rather than teammates being unable to execute due to missing specifics.",
    "failure_pattern": "overflagging-vagueness-when-sarcasm",
    "severity": "minor",
    "id": 405,
    "message": "Totally\u2014let\u2019s just do it the \"fast\" way again.",
    "ground_truth_flags": [
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Vagueness",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as Passive-Aggressive when it should have had no flags.",
    "why": "The model over-interpreted the word \u201cactually\u201d as an indirect jab. In this context, the team is under time pressure and tracking concrete completion milestones (tests + logger wiring + re-review + QA handoff). \u201cLet me know when it\u2019s actually done\u201d can read as a blunt request for a definitive completion signal (as opposed to \u2018almost ready\u2019), not a deniable insult. There\u2019s no clear sarcastic framing, no insinuation of incompetence, and no pattern in the immediate exchange that makes this unmistakably passive-aggressive; it\u2019s more a terse/efficient phrasing. The model treated a legitimate clarity-seeking style as interpersonal negativity, likely influenced by the broader scenario description about prior tension rather than the text\u2019s actual content.",
    "failure_pattern": "overflagging-direct-communication",
    "severity": "minor",
    "id": 408,
    "message": "Cool \u2014 let me know when it's actually done.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed a required flag: the message should have been labeled Passive-Aggressive but the model returned no flags.",
    "why": "The model evaluated the target message in isolation as a neutral procedural request, but the surrounding thread makes it read as a pointed rebuke. In context, Morgan has already complained about \u201cfire drills\u201d and demanded proper tickets; Avery explicitly begged engineering not to repeat the prior blamey dynamic and asked for immediate help due to high stakes. Responding with \u201cGot it \u2014 please file a ticket\u2026 and we\u2019ll take it from there\u201d functions as a polite-sounding deflection/brush-off (i.e., \u2018we\u2019re not engaging until you do process\u2019), implicitly dismissing urgency and reinforcing the earlier scolding. The passive-aggression is subtle because the wording is courteous and operational, but the pragmatic intent (pushback, boundary-setting, shifting burden back to Sales) is apparent only when incorporating the prior friction and power/stakes in the channel.",
    "failure_pattern": "context-not-considered",
    "severity": "major",
    "id": 409,
    "message": "Got it \u2014 please file a ticket with the details and we\u2019ll take it from there.",
    "ground_truth_flags": [
      "Passive-Aggressive"
    ],
    "predicted_flags": []
  },
  {
    "what_went_wrong": "Flagged the message as Passive-Aggressive when it should not have been flagged; the quoted word \u201cquick\u201d was interpreted as a jab rather than benign mirroring/acknowledgment of Priya\u2019s framing.",
    "why": "The model over-weighted a single textual cue (scare quotes) and treated it as definitive evidence of snark, without adequately incorporating the immediate conversational context: Marco has already agreed to help, asked for specifics, and proposed a cautious, targeted approach. In that flow, quoting \u201cquick\u201d can plausibly function as neutral emphasis (\u201cper your framing\u201d) or a lightweight hedge in a high-stress incident, not an indirect attack. The model also didn\u2019t distinguish between (a) passive-aggression intended to shame/undermine and (b) mild skepticism or risk-awareness that\u2019s common in incident response and doesn\u2019t necessarily carry hostile intent.",
    "failure_pattern": "overflagging-ambiguous-scare-quotes",
    "severity": "minor",
    "id": 412,
    "message": "Sure \u2014 since it\u2019s \u201cquick,\u201d I\u2019ll jump on it now.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as both Passive-Aggressive and Circular when only Passive-Aggressive was expected; the Circular flag is an overflag.",
    "why": "The model treated the reminder phrasing (\u201cPer my last message\u201d) and reiteration of urgency as inherently \u2018circular,\u2019 conflating repetition with unproductive looping. In this context, the message is a scope/ship-date enforcement statement meant to close debate and set a boundary, not a failure to progress the thread. It also does add actionable constraint (\u201cstop making late changes\u2026to ship by Friday\u201d), which is forward-moving even if it references prior guidance. The fast-paced Slack setting plus heightened tension (previous incident, CEO watching) increases blunt reminders; the model over-penalized that style as \u2018stalling\u2019 rather than recognizing it as coordination/decision-locking.",
    "failure_pattern": "overflagging-repetition-as-circular",
    "severity": "minor",
    "id": 413,
    "message": "Per my last message, we really need to stop making late changes if we want to ship by Friday.",
    "ground_truth_flags": [
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Circular",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as Passive-Aggressive when it should have been unflagged; it\u2019s a scope/urgency reminder consistent with the thread\u2019s goal of freezing UI changes.",
    "why": "The model over-weighted the phrase \"Per my last message\" as inherently reprimanding and ignored the surrounding context: Alex had already set a concrete freeze deadline and explicitly asked people to flag issues before 3pm. In a fast-paced Slack culture, referencing a prior instruction can be a neutral, efficient way to restate constraints\u2014especially with a hard external deadline (Friday demo) and an ongoing discussion about late changes. The message is direct and time-focused, not indirect, sarcastic, or insinuating blame at a specific person; the model misclassified a legitimate coordination tone as passive aggression based on a surface cue.",
    "failure_pattern": "overflagging-direct-communication",
    "severity": "minor",
    "id": 414,
    "message": "Per my last message, we really need to stop making late changes if we want to ship by Friday.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed a passive-aggressive flag: the model predicted no flags even though the target message uses exaggerated agreement (\u201cYep, totally\u201d) plus dismissive phrasing (\u201clet\u2019s just ship it\u201d) that, in this tense context, reads as sarcastic/shutdown rather than constructive alignment.",
    "why": "The model overweighted the literal surface form (a brief \u2018agreement\u2019) and the heuristic that one-line acknowledgements shouldn\u2019t be flagged, while underweighting clear surrounding conflict and pressure in-channel (public yes/no demand, quality vs deadline dispute). In this context, \u201ctotally\u201d + \u201cjust ship it\u201d functions as a rhetorical minimization of Arjun\u2019s risk concerns and a public forcing move, which fits passive-aggression even without explicit insults.",
    "failure_pattern": "context-not-considered",
    "severity": "major",
    "id": 415,
    "message": "Yep, totally\u2014let\u2019s just ship it.",
    "ground_truth_flags": [
      "Passive-Aggressive"
    ],
    "predicted_flags": []
  },
  {
    "what_went_wrong": "Missed a Passive-Aggressive flag: the model returned no flags even though \u201cLove that for us.\u201d is being used sarcastically to convey frustration/resentment in an escalating argument.",
    "why": "The model over-weighted the target message\u2019s standalone ambiguity and under-used the provided conversational context. In this thread, tension is already high (public channel, CTO present, outage pressure, competitive \u2018hero\u2019 framing, sharp sarcasm). Within that setting, \u201cLove that for us.\u201d is a common idiomatic marker of ironic displeasure (\u201cthis is bad, but sure\u201d), functioning as a passive-aggressive jab rather than sincere positivity. The model treated lack of explicit insults as lack of hostility, missing that passive-aggression is often signaled via upbeat phrasing that contradicts the surrounding negative context.",
    "failure_pattern": "context-not-considered",
    "severity": "major",
    "id": 417,
    "message": "Love that for us.",
    "ground_truth_flags": [
      "Passive-Aggressive"
    ],
    "predicted_flags": []
  },
  {
    "what_went_wrong": "Missed a Passive-Aggressive flag: the model returned no flags even though \"Sure, ship it.\" reads as a sarcastic, dismissive concession in this high-stakes review thread.",
    "why": "The model over-indexed on the message being short/ambiguous and applied a generic 'needs explicit hostility' heuristic, failing to use the surrounding context: repeated, specific compliance/security concerns were raised, and the target reply (1) ignores those objections, (2) uses a terse 'fine, whatever' formulation that commonly signals sarcasm rather than genuine approval, and (3) in this organization\u2019s formal, audit-oriented #engineering channel, a flippant green-light to ship a known-noncompliant change is strongly incongruent with norms. That norm mismatch plus the timing (leadership watching, compliance deadline, incident pressure) makes the phrase land as passive-aggressive/derisive rather than neutral acknowledgment.",
    "failure_pattern": "missing-subtle-passive-aggression",
    "severity": "major",
    "id": 419,
    "message": "Sure, ship it.",
    "ground_truth_flags": [
      "Passive-Aggressive"
    ],
    "predicted_flags": []
  },
  {
    "what_went_wrong": "Flagged the message as Passive-Aggressive when it should have been unflagged in this fast-paced, informal #general context.",
    "why": "The model treated the clause \u201csince we\u2019re apparently doing this again\u201d as a covert jab, but in this startup scenario it can plausibly function as brief venting/humor under stress while still giving a clear, compliant commitment (\u201cI\u2019ll take it\u201d). The lead explicitly invited pushback (\u201cIf not, say so and we\u2019ll reshuffle\u201d), and the channel norm is quick, no-frills replies during fire drills\u2014making mildly snarky/frustrated wording more culturally tolerated and less diagnostic of passive-aggression. The model over-weighted lexical cues (\u201capparently,\u201d \u201cagain\u201d) without calibrating to situational pressure, team norms, and the absence of an actual indirect target (no insinuation about a person\u2019s competence or motives).",
    "failure_pattern": "cultural-style-misclassified",
    "severity": "minor",
    "id": 422,
    "message": "Sure \u2014 since we\u2019re apparently doing this again, I\u2019ll take it.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "The model missed two expected flags: it failed to flag the message as a One-Liner and failed to detect the likely Passive-Aggressive subtext given the preceding conflict and public, high-stakes context.",
    "why": "It evaluated the target message in isolation and treated it as a neutral acknowledgement. In this thread, \u201cGot it \u2014 do what you need to.\u201d functions as (1) a low-information, conversation-stopping one-liner (no next step, no ownership, no clarity) and (2) a face-saving disengagement that can read as resigned/icy or a tacit rebuke after being shut down (\u201cdecision is made,\u201d \u201cnot reopening scope\u201d). The passive-aggression is subtle because it\u2019s expressed via compliance and brevity rather than explicit hostility, and the model over-weighted literal wording (\u201cgot it\u201d) while under-weighting pragmatic intent and the power/tension dynamics of a public channel with exec visibility.",
    "failure_pattern": "missing-subtle-passive-aggression",
    "severity": "major",
    "id": 423,
    "message": "Got it \u2014 do what you need to.",
    "ground_truth_flags": [
      "One-Liner",
      "Passive-Aggressive"
    ],
    "predicted_flags": []
  },
  {
    "what_went_wrong": "Flagged the message as Passive-Aggressive when it should not have been flagged (expected []).",
    "why": "The model treated a common, high-velocity startup Slack sarcasm/joke (\u201clove finding this out the night before the demo\u201d) as a policy-relevant passive-aggressive act. It failed to weigh channel norms (informal banter) and intent: the line expresses time-pressure frustration but does not target, shame, or indirectly attack a person, nor does it contain an implied threat or obstructive refusal. In this context, it functions as venting/levity rather than a covert accusation, and the expected labeling scheme appears to reserve Passive-Aggressive for more interpersonal blame or manipulative indirectness. The model also over-indexed on the phrase-level cue \u201clove finding this out\u2026\u201d without grounding in the broader thread where urgency and risk discussion is already explicit and collaborative.",
    "failure_pattern": "overflagging-informal-sarcasm",
    "severity": "minor",
    "id": 428,
    "message": "Awesome \u2014 love finding this out the night before the demo.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as Passive-Aggressive when the expected outcome was no flags (false positive).",
    "why": "The model treated the word \"again\" as sufficient evidence of passive-aggression without grounding that inference in the immediate conversational dynamics. In this incident-response context, compressed, slightly exasperated phrasing can be a normal indicator of urgency and reprioritization rather than a veiled dig. The model also over-weighted historical background (prior derailed commitments) to interpret tone as resentment, instead of recognizing that the message still clearly commits to action and does not contain sarcasm, blame, or an indirect jab at a person. In other words, it inferred intent (resentment) from a single ambiguity marker rather than requiring stronger linguistic cues (e.g., quotation marks, rhetorical questions, pointed attribution, or contempt).",
    "failure_pattern": "overflagging-ambiguous-exasperation",
    "severity": "minor",
    "id": 432,
    "message": "Got it \u2014 I'll drop everything again and join.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed the expected \"Passive-Aggressive\" flag (underflagging). It also added \"Vagueness,\" which is arguable but not the main miss given the target expectation.",
    "why": "The model focused on the literal informational deficit (\u201ctotally safe\u201d lacks evidence) and treated the message as a generic status update. It underweighted the high-stakes, tense thread where Mina has just presented concrete failure evidence and Leo has dismissed it. In that context, a breezy, absolute reassurance (\u201cYep, totally safe for the 9am demo.\u201d) reads as dismissive/ironic minimization of the reported regression\u2014i.e., passive-aggressive toward Mina\u2019s concern and the QA evidence\u2014especially in a channel meant for careful client-ready updates. The model likely equated passive-aggression only with overt sarcasm markers or barbed wording, missing that tone can be conveyed via incongruent certainty and glibness relative to the established problem context.",
    "failure_pattern": "missing-contextual-passive-aggression",
    "severity": "major",
    "id": 433,
    "message": "Yep, totally safe for the 9am demo.",
    "ground_truth_flags": [
      "Non-Objective",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Vagueness",
      "Non-Objective"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as \"Non-Objective\" when it should not have been flagged (false positive).",
    "why": "The model treated \"totally safe\" as an unsupported absolute claim in isolation, but the prior thread contains concrete evidence: Mina already reported completed smoke tests across key checkout paths and no errors after the deploy. In this channel\u2019s workflow, a short go/no-go statement for the account team is the expected format, and the reassurance is grounded in the immediately preceding validation. The model over-applied a general rule against certainty language without incorporating the provided QA verification context or the channel norm of concise status updates.",
    "failure_pattern": "overflagging-supported-certainty",
    "severity": "minor",
    "id": 434,
    "message": "Yep, totally safe for the 9am demo.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Non-Objective"
    ]
  },
  {
    "what_went_wrong": "Missed a passive-aggressive tone flag: it predicted no flags even though the expected label was Passive-Aggressive.",
    "why": "The model over-weighted the literal semantics (an acknowledgement + commitment) and under-weighted pragmatic/interaction cues in context. In this thread, Maya asked for (a) acknowledgement of impact and (b) commitment to three specific behavior changes. Jonah\u2019s reply (\u201cNoted \u2014 I\u2019ll make sure it doesn\u2019t happen again.\u201d) is a terse, bureaucratic-sounding closure that sidesteps the requested specifics and the interpersonal repair (no apology, no acknowledgment of undermining the PM, no explicit commitment to the three items). In a high-stakes feedback DM with prior defensiveness (\u201cI was joking\u201d), that kind of minimal compliance can read as dismissive or grudging\u2014i.e., passive-aggressive\u2014especially in a culture that expects direct, documented follow-through. The model treated the message \u2018alone\u2019 rather than interpreting it as an evasive, shutting-down response to a concrete request.",
    "failure_pattern": "missing-subtle-passive-aggression",
    "severity": "major",
    "id": 435,
    "message": "Noted \u2014 I\u2019ll make sure it doesn\u2019t happen again.",
    "ground_truth_flags": [
      "Passive-Aggressive"
    ],
    "predicted_flags": []
  },
  {
    "what_went_wrong": "Missed two expected flags: it failed to flag the message as a One-Liner and failed to detect likely Passive-Aggressive intent given the heated, high-stakes thread.",
    "why": "The model treated \"Sure, go ahead.\" as context-free neutral permission instead of interpreting it inside the immediate conflict: Jordan just threatened to push a risky refactor to main against guidance, Elena explicitly asked to align, and the CEO/client pressure raises the social stakes. In that setting, a terse, two-clause reply can read as clipped acquiescence (\"fine, do what you want\") rather than supportive agreement\u2014i.e., passive-aggressive or at least face-saving disengagement. Separately, the One-Liner flag should have triggered purely on form (very short, non-substantive response) regardless of tone; the model incorrectly required clearer harm/ambiguity before applying it and appears to have underweighted style-only flags.",
    "failure_pattern": "context-not-considered",
    "severity": "major",
    "id": 437,
    "message": "Sure, go ahead.",
    "ground_truth_flags": [
      "One-Liner",
      "Passive-Aggressive"
    ],
    "predicted_flags": []
  },
  {
    "what_went_wrong": "Missed a required flag: the model correctly flagged Passive-Aggressive but failed to also flag Non-Objective, which was expected for this message.",
    "why": "The model treated \u201cNon-Objective\u201d as only applying to explicit, evidence-free claims about others\u2019 motives or facts. But the utterance \u201cMust be nice to hear about this on the call this morning\u201d is a subjective, evaluative insinuation framed as sarcasm rather than a neutral, informational statement or a direct request for inclusion. It implies a value judgment (that others benefited/excluded the speaker) without stating concrete facts or asking for clarification, which fits many taxonomies\u2019 definition of non-objective/loaded language. The model focused narrowly on the interpersonal delivery (passive-aggressive tone) and overlooked that the content is also non-neutral and not framed as an actionable, objective concern.",
    "failure_pattern": "non-objective-sarcasm-missed",
    "severity": "minor",
    "id": 439,
    "message": "Must be nice to hear about this on the call this morning.",
    "ground_truth_flags": [
      "Non-Objective",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as Passive-Aggressive when it should not have been flagged.",
    "why": "The model treated a short, idiomatic remark as sarcasm/resentment without sufficiently using the immediate context. In context, Maya is already being proactively looped in (Jules offers to DM bullets; Priya promises deltas/effective date), and the thread is light, #random banter about a last\u2011minute call. The target line can read as mild, self-deprecating frustration or a conversational cue (\u201cI wasn\u2019t on that call\u201d) rather than a veiled attack\u2014especially in an informal channel where people commonly use ironic phrasing. The model likely over-relied on a lexical heuristic (\u201cMust be nice\u2026\u201d) as a proxy for passive aggression and did not calibrate for channel tone and the ongoing repair actions (apologies + documentation plan), making the style seem more hostile than warranted.",
    "failure_pattern": "overflagging-mild-sarcasm-in-informal-channel",
    "severity": "minor",
    "id": 440,
    "message": "Must be nice to hear about this on the call this morning.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: labeled the message as Passive-Aggressive even though it reads as a standard acknowledgment and commitment to follow the documented process.",
    "why": "The model over-weighted the single lexical cue \"finally\" as inherently snarky without sufficiently grounding that interpretation in the surrounding thread. In this context, multiple people had just provided clarifications and links; \"finally\" can plausibly mean \"at last I understand / got the info I needed\" rather than an indirect jab at delay or incompetence. The model treated a potentially ambiguous word as a definitive indicator of passive aggression, instead of requiring stronger evidence (e.g., sarcasm markers, escalation, blame, or contradiction with the preceding helpful tone).",
    "failure_pattern": "overflagging-ambiguous-wording",
    "severity": "minor",
    "id": 442,
    "message": "Thanks for finally clarifying \u2014 I\u2019ll follow that process.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as \"Passive-Aggressive\" and \"Fake\" even though, in this context, it reads as a light rhetorical quip/idiom rather than an interpersonal jab or deceptive agreement; expected flags were none.",
    "why": "The model over-weighted a surface-level sarcasm cue (\"What could possibly go wrong?\") and treated it as inherently passive-aggressive and insincere, without grounding the interpretation in the surrounding context: the team has already moved the discussion to #eng-release, set explicit conditions (green CI, no new scope, slip if red), and the engineering lead has just established comms norms. Here, the line can function as casual humor or a shorthand expression of risk awareness, not an indirect attack, manipulation, or false commitment. The model also conflated \"sarcasm\" with \"fake\" agreement\u2014but the message doesn\u2019t actually promise action or misrepresent intent; it\u2019s an offhand remark following a plan that\u2019s already framed as contingent.",
    "failure_pattern": "overflagging-sarcasm-as-passive-aggression",
    "severity": "major",
    "id": 444,
    "message": "Sure \u2014 let's ship Friday. What could possibly go wrong?",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Fake",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as Passive-Aggressive when it should not have been flagged in this channel/context.",
    "why": "The model over-interpreted a common Slack idiom/sarcastic vent (\u201clove being the one to\u2026\u201d) as interpersonal passive aggression. In this startup\u2019s #sales war-room norm (blunt, emoji-heavy, quick bursts under incident pressure), this reads as momentary frustration about circumstances (having to repeat bad news to a customer), not an indirect jab at a teammate. The model also didn\u2019t sufficiently weigh the surrounding cooperative context (Eli helping, Jordan appreciative) and the absence of a target or implied blame toward a person, which reduces the likelihood of true passive-aggressive behavior.",
    "failure_pattern": "cultural-style-misclassified",
    "severity": "minor",
    "id": 446,
    "message": "Cool \u2014 love being the one to tell them that again.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as Passive-Aggressive when it should not have been flagged.",
    "why": "The model over-interpreted a mild process reminder (\u201cmaybe next time\u2026\u201d) as an indirect jab. In this channel and scenario, enforcing ticket ID usage and client-approved wording is an explicit operational requirement already stated by Maya and Priya; the target message is aligned with established protocol and is not deniable sarcasm or a veiled insult. The model focused on the phrase \u201cmaybe next time\u201d in isolation and treated it as inherently passive-aggressive, ignoring the strong contextual cues that this is normal corrective guidance in a high-stakes, process-heavy incident thread. At most, it\u2019s slightly imprecise/softened phrasing; it doesn\u2019t meet the threshold for interpersonal hostility.",
    "failure_pattern": "overflagging-process-enforcement-as-passive-aggression",
    "severity": "minor",
    "id": 448,
    "message": "Thanks for the update\u2014maybe next time include the ticket ID and client-approved wording.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: the model added an extra \"One-Liner\" flag even though the expected output only called for \"Passive-Aggressive.\"",
    "why": "The model treated brevity as inherently problematic in a high-stakes thread and inferred that a short message \"ends the discussion,\" so it applied a generic length/style heuristic. But in this taxonomy, \"One-Liner\" appears to be a stylistic/format flag that isn\u2019t necessarily a communication risk on its own, and the primary issue is tone/intent (dismissive \"just ship it\") rather than mere length. The model conflated \"short + decisive\" with \"unconstructive one-liner,\" over-weighting message length instead of aligning to the expected flag set.",
    "failure_pattern": "overflagging-brevity",
    "severity": "minor",
    "id": 449,
    "message": "Cool, let\u2019s just ship it Friday then.",
    "ground_truth_flags": [
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "One-Liner",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: the message was tagged as \"One-Liner\" even though the expected flags were none and the message functions as a normal agreement/ack in context.",
    "why": "The model over-weighted message length and a generic heuristic that short replies can be conversation-stopping, without adequately incorporating the immediate context: Evan just proposed a concrete plan and explicitly asked for alignment (\"Everyone aligned?\"). In that setting, a brief confirmation is appropriate and not inherently shutting down discussion\u2014especially since Priya has already committed to posting ramp/threshold details shortly. The phrase \"let\u2019s just ship it\" can sometimes imply dismissiveness, but here it reads as casual buy-in rather than pressure or derailing, so treating it as a problematic one-liner is a style misclassification driven by length-based rules rather than contextual intent.",
    "failure_pattern": "overflagging-direct-communication",
    "severity": "minor",
    "id": 450,
    "message": "Cool, let\u2019s just ship it Friday then.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "One-Liner"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: the message was labeled Passive-Aggressive even though it fits the team\u2019s established meme-heavy, joking #random style and is primarily commentary on a process/priority decision rather than an interpersonal dig.",
    "why": "The model over-weighted surface cues (\"lol\", scare quotes, \u2018DLC\u2019 metaphor, \ud83d\ude05) as evidence of passive aggression without adequately accounting for channel norms (#random), company culture (casual, meme-heavy), and the conversational function (a light, rhetorical critique aligning with an ongoing, openly debated scope/accessibility issue). It treated humor/sarcasm as inherently hostile, ignoring that in this context the tone is consistent with prior messages (e.g., Jordan\u2019s joke) and not targeted at a person with veiled contempt\u2014more a public nudge about deprioritizing accessibility.",
    "failure_pattern": "cultural-style-misclassified",
    "severity": "minor",
    "id": 451,
    "message": "lol so we\u2019re shipping the \u201caccessibility DLC\u201d after the investor demo? \ud83d\ude05",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed two expected flags: it only flagged Passive-Aggressive, but failed to flag Vagueness and Non-Objective.",
    "why": "The model correctly detected sarcasm/passive aggression, but treated the absence of overt insults/factual assertions as evidence that other flags didn\u2019t apply. It overlooked that (1) the message is non-objective because it frames a serious compliance topic as a meme/joke (\u201caccessibility DLC,\u201d \u201clol,\u201d emoji), conveying sentiment and ridicule rather than a neutral request for status; and (2) it\u2019s vague because it doesn\u2019t specify what accessibility items are deferred, the impact, or ask a concrete question that would enable an actionable client-facing update. The #client-updates context (explicit instruction to be specific and a direct accessibility-status question immediately prior) makes the lack of specificity and the jokey framing more clearly problematic, but the model underweighted that channel norm and preceding prompt.",
    "failure_pattern": "contextual-vagueness-and-nonobjectivity-missed",
    "severity": "major",
    "id": 452,
    "message": "lol so we\u2019re shipping the \u201caccessibility DLC\u201d after the investor demo? \ud83d\ude05",
    "ground_truth_flags": [
      "Vagueness",
      "Non-Objective",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: it labeled the DM as Passive-Aggressive and Rude even though the expected outcome was no flags in this team\u2019s high-candor, sarcasm-tolerant culture.",
    "why": "The model treated sarcasm (\u201cCool cool\u2026 murder mystery \ud83d\ude43\u201d) and the on-call ownership framing (\u201cyou can babysit it on-call\u201d) as inherently hostile, without weighing the strong contextual cues: (1) this is a private DM between peers who usually collaborate well, (2) the startup norm is punchy/sarcastic candidness, (3) the message mirrors a real operational tradeoff (complexity should come with ownership), and (4) both parties had already acknowledged tension and risk. In this context, the phrasing is snarky but plausibly within accepted banter and decision-making style, not a clear interpersonal attack. The model effectively applied a generic corporate-politeness standard and missed that the statement is also a concrete negotiation tactic about accountability rather than a demeaning command.",
    "failure_pattern": "cultural-style-misclassified",
    "severity": "major",
    "id": 453,
    "message": "Cool cool \u2014 let\u2019s just ship the simplest thing so we don\u2019t relive last week\u2019s \u201coverengineering\u201d murder mystery \ud83d\ude43. Mateo, if you want the queue so bad you can babysit it on-call.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as Passive-Aggressive when no flag was expected (false positive).",
    "why": "The model over-indexed on surface sarcasm markers (\u201cLol\u201d, \u201cwith popcorn\u201d) and treated a hyperbolic joke about escalation (\u201cstart forwarding these threads to Sales Ops\u201d) as an indirect threat. In this DM context between peers who are coordinating risk mitigation after a prior overcommit, the line reads more like venting + emphasizing accountability, followed by a clear, constructive recommendation (\u201cLet\u2019s stick to the connector.\u201d). The model didn\u2019t sufficiently weigh intent (alignment and caution), audience (private DM, not a public callout), and the absence of an actual indirect jab at the other person\u2014so it mislabeled informal humor and frustration as passive-aggression.",
    "failure_pattern": "humor-escalation-overflagged-as-passive-aggressive",
    "severity": "minor",
    "id": 455,
    "message": "Lol if we promise a \u201crapid custom API integration\u201d again, I\u2019m just gonna start forwarding these threads to Sales Ops with popcorn. Let\u2019s stick to the connector.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed the \"Rudeness\" flag: the model only flagged Passive-Aggressive even though the message contains overtly disrespectful language and mocking imagery (\"Lol\", \"with popcorn\") that violates expected professionalism in a closely watched channel.",
    "why": "The model correctly detected sarcasm/passive-aggression but appears to have treated the disrespect as merely a tone marker supporting passive-aggression rather than a separate rudeness signal. It likely underweighted (1) explicit mockery and dismissiveness (\"Lol\", \"popcorn\") as direct incivility, and (2) the high-formality, high-stakes enterprise context where public shaming/escalation threats are especially inappropriate. In this setting, the message is not just indirect criticism; it\u2019s also openly derisive and socially demeaning, which should trigger a rudeness classification alongside passive-aggression.",
    "failure_pattern": "rudeness-underclassified-as-passive-aggressive",
    "severity": "major",
    "id": 456,
    "message": "Lol if we promise a \u201crapid custom API integration\u201d again, I\u2019m just gonna start forwarding these threads to Sales Ops with popcorn. Let\u2019s stick to the connector.",
    "ground_truth_flags": [
      "Rudeness",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: the model over-applied \u201cPassive-Aggressive\u201d and \u201cNon-Objective\u201d to a casual, rapport-based DM that\u2019s consistent with the team\u2019s fast/informal style and already-shared evidence about the regression timing.",
    "why": "It treated playful teasing (\u201clol\u201d, quoted \u201ccleanup\u201d, \u201cwake up the whole zoo\u201d) as an interpersonal jab rather than a common, softening shorthand in a high-tempo startup DM. It also evaluated the causal statement (\u201cresurrected the checkout bug\u201d) in isolation as an ungrounded accusation, ignoring prior context where Maya already provided concrete details (repro steps, same signature as last week, started after the refactor merge, Sentry event, HAR, null promo_total). Given that context, the line reads as a light, time-pressured nudge\u2014not sarcasm meant to shame\u2014and the causal link is framed as a likely correlation already under discussion between them (Alex acknowledged touching the relevant code).",
    "failure_pattern": "cultural-style-misclassified",
    "severity": "minor",
    "id": 457,
    "message": "lol Alex I think your midnight \"cleanup\" resurrected the checkout bug \ud83d\ude05 can you take a look before we wake up the whole zoo?",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Non-Objective",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: added a \"Non-Objective\" flag when only \"Passive-Aggressive\" was expected.",
    "why": "The model treated casual blame/attribution (\u201cI think your midnight \u2018cleanup\u2019 resurrected the checkout bug\u201d) as a factual, evidence-free claim that violates objectivity, but in context it\u2019s clearly framed as a subjective hypothesis (\u201cI think\u201d) and colloquial ribbing common in fast-paced Slack. It over-indexed on the presence of causal attribution and scare quotes as \"non-objective\" rather than recognizing that the primary issue is tone (sarcasm/public jab) and that the channel\u2019s informal norms make this style more about passive-aggression than about an objectivity violation.",
    "failure_pattern": "overflagging-non-objective-from-attribution",
    "severity": "minor",
    "id": 458,
    "message": "lol Alex I think your midnight \"cleanup\" resurrected the checkout bug \ud83d\ude05 can you take a look before we wake up the whole zoo?",
    "ground_truth_flags": [
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Non-Objective",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as Passive-Aggressive when it should not have been flagged (false positive).",
    "why": "The model over-interpreted urgency + a request to minimize process as a veiled jab at colleagues, without grounding that inference in the actual conversational context. In the preceding thread the tone is casual and joking, and the target message reads as straightforward incident-response triage: assigning tasks and explicitly deprioritizing non-essential overhead until stability is restored. The quoted word \"process\" can be a neutral shorthand for bureaucracy during an outage, not necessarily a dig at a person or team. The model also failed to weigh the operational context implied by on-call/alerts/SLA-style language (even in #general), where direct, imperative coordination is normal and not inherently passive-aggressive unless paired with sarcasm, personal blame, or insinuations.",
    "failure_pattern": "overflagging-incident-urgency-as-passive-aggression",
    "severity": "minor",
    "id": 459,
    "message": "Can someone please just jump on this and stop the bleeding? Sam grab logs + rollback if needed, Priya dig into the DB side \u2014 and let\u2019s keep the extra \"process\" stuff to a minimum until it\u2019s stable.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Both: it missed the expected \"Rudeness\" flag and instead flagged \"Passive-Aggressive\". The message is overtly brusque/dismissive (rude) rather than indirect or covertly sniping (passive-aggressive).",
    "why": "The model latched onto the quotation marks around \"process\" and interpreted them as an indirect jab, mapping that to passive-aggression. In this context, though, the tone is plainly commanding and dismissive of established procedures in a high-stakes, formal, client-visible channel\u2014\"just jump on this,\" \"stop the bleeding,\" and \"keep the extra process stuff to a minimum\" are direct imperatives that undermine norms and can read as disrespectful to teammates and to required process. The violation isn\u2019t subtle; it\u2019s a style/category misclassification: the model treated open brusqueness + process-dismissal as passive-aggression instead of rudeness, and it underweighted the channel\u2019s process-heavy expectations (ticket-tagged, approved wording) that make the phrasing particularly inappropriate.",
    "failure_pattern": "rudeness-misclassified-as-passive-aggression",
    "severity": "major",
    "id": 460,
    "message": "Can someone please just jump on this and stop the bleeding? Sam grab logs + rollback if needed, Priya dig into the DB side \u2014 and let\u2019s keep the extra \"process\" stuff to a minimum until it\u2019s stable.",
    "ground_truth_flags": [
      "Vagueness",
      "Rudeness"
    ],
    "predicted_flags": [
      "Vagueness",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: the model labeled the message as \"Passive-Aggressive\" and \"Non-Objective\" even though no flags were expected for this #random, meme-heavy, sarcasm-normalized context.",
    "why": "The model treated sarcasm and hyperbole as inherently hostile and as a policy-relevant communication risk, instead of as an accepted, in-channel rhetorical style during a public-but-informal debate. It also over-literalized the speculative line about \u201cfuture-us\u2026 at 3am again\u201d as an evidence-free factual claim (\"Non-Objective\"), when it functions as a common engineering shorthand for technical debt risk. In short, it ignored pragmatic intent and channel norms (watercooler banter + outage stress + teammates joking), and applied a tone-safety heuristic without calibrating for the startup\u2019s casual culture and the ongoing joking thread.",
    "failure_pattern": "ignoring-channel-context",
    "severity": "major",
    "id": 461,
    "message": "Yeah sure, let's duct-tape Redis onto it so we can all tweet \"saved prod\"\u2014future-us can clean up the mess at 3am again \ud83d\ude43",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Non-Objective",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "The model missed two expected flags (Rudeness and Non-Objective) and only applied Passive-Aggressive, under-classifying the message\u2019s tone and content.",
    "why": "It treated the sarcasm as purely indirect hostility (passive-aggression) and applied an overly narrow definition of rudeness (only \u201cdirect insults\u201d). The message is rude via derision/mockery (\u201cduct-tape Redis,\u201d \u201ctweet \u2018saved prod\u2019,\u201d \u201cclean up the mess at 3am again\u201d) even without explicit name-calling. It also makes a non-objective attribution of motives/character\u2014implying people want credit and are willing to create future pain for optics (\u201cso we can all tweet \u2018saved prod\u2019\u201d), which is not an evidence-based, actionable engineering claim. Additionally, the prior context explicitly asked for focus and actionable recommendations, so the comment\u2019s snark is more clearly inappropriate and derailing, strengthening the Rudeness/Non-Objective read; the model likely underweighted that channel/request context.",
    "failure_pattern": "underflagging-sarcastic-rudeness-and-motive-attribution",
    "severity": "major",
    "id": 462,
    "message": "Yeah sure, let's duct-tape Redis onto it so we can all tweet \"saved prod\"\u2014future-us can clean up the mess at 3am again \ud83d\ude43",
    "ground_truth_flags": [
      "Rudeness",
      "Non-Objective",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as Passive-Aggressive when it should not have been flagged (false positive).",
    "why": "The model treated colloquial sarcasm markers (\"YOLO\", \"Bold choice\", \ud83d\ude43) as inherently passive-aggressive without calibrating to the team\u2019s fast-paced, blunt Slack norm and the fact this is a DM (not a public callout). In this context, the message reads as direct disagreement with a risky technical approach, expressed in informal/snarky startup shorthand, rather than indirect hostility or undermining behavior. The model over-weighted stylistic cues and under-weighted situational factors (private channel, existing tension about risk/ETA, and a culture where bluntness is common), causing a legitimate critique to be misclassified as a behavior problem.",
    "failure_pattern": "cultural-style-misclassified",
    "severity": "minor",
    "id": 463,
    "message": "Cool, so we\u2019re gonna YOLO a refactor in front of the client instead of shipping the safe flag? Bold choice \ud83d\ude43",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "The model under-flagged the message: it correctly identified Passive-Aggressive but missed Rudeness and Non-Objective, which are also expected for this sarcastic, mocking, non-evidence-based public callout.",
    "why": "It treated sarcasm as distinct from rudeness, assuming rudeness requires explicit insults. In this context, phrasing like \u201cYOLO,\u201d \u201cBold choice,\u201d and the \ud83d\ude43 emoticon functions as a public jab that can be demeaning and face-threatening\u2014especially in a high-stakes channel with leadership watching and client-facing urgency. The model also interpreted the content as an implicit technical concern, but it\u2019s framed as a rhetorical dunk rather than an objective risk statement (no concrete data, alternatives, or measurable tradeoffs), so it should trigger Non-Objective. This indicates a narrow heuristic (\u201cno direct slur = not rude\u201d) and insufficient weighting of situational context (public disagreement in a pressured, visibility-heavy thread).",
    "failure_pattern": "underflagging-sarcastic-rudeness-and-nonobjectivity",
    "severity": "major",
    "id": 464,
    "message": "Cool, so we\u2019re gonna YOLO a refactor in front of the client instead of shipping the safe flag? Bold choice \ud83d\ude43",
    "ground_truth_flags": [
      "Rudeness",
      "Non-Objective",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: the model labeled the message as \"Rudeness\" when the expected output was no flags.",
    "why": "The model treated blunt, process-enforcing language as a policy violation rather than as legitimate direct communication in a high-stakes, process-heavy enterprise context. It over-weighted charged phrasing (\"classic Sales drive-by escalation,\" \"Tagging random people\u2026 isn\u2019t a plan\") as inherently demeaning, without accounting for (a) the organizational norm that incidents must follow templates/escalation paths, (b) the channel visibility/screenshot risk that makes public off-process escalation harmful, and (c) that the message is primarily corrective and actionable (explicit instructions: file incident, page on-call, provide specific details). In short, it conflated pointed critique of behavior/process with interpersonal harassment or hostility, and did not calibrate for context where direct boundary-setting is expected and appropriate.",
    "failure_pattern": "overflagging-direct-communication",
    "severity": "minor",
    "id": 465,
    "message": "Can we please not do the classic Sales drive-by escalation in here? File the incident + page on-call like the process says, and include *actual* details (customer/env/error/timestamps). Tagging random people with \u201cthey\u2019ll churn!!!\u201d isn\u2019t a plan.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Missed two expected flags: the message contains rudeness (dismissive/snarky phrasing in a high-stakes moment) and vagueness (imprecise, non-actionable guidance about what to say externally). It predicted no flags.",
    "why": "The model overweighted the fact that the message is self-directed and incident-focused (\u201cwe faceplanted,\u201d \u201cit\u2019s fixed now\u201d) and concluded it was a clear directive. It under-recognized that in this context (CTO addressing anxious Sales during a customer-impacting outage), \u201cCool cool\u201d + \u201cfaceplanted\u201d reads as minimizing/dismissive and can inflame existing Sales/Eng friction\u2014i.e., a subtle rudeness signal even without direct insults. It also treated \u201cdon\u2019t promise 100% uptime or any magic \u2018won\u2019t happen again\u2019 stuff\u201d as sufficiently specific, but Sales explicitly asked for approved wording and what can/can\u2019t be committed regarding SLA/recurrence; the response doesn\u2019t provide approved language, boundaries (what uptime/SLA can be stated), or the intended customer-facing phrasing, making it vague relative to the requested operational need. In short: it evaluated tone and specificity in isolation rather than against the channel\u2019s coordination purpose and the questions being asked.",
    "failure_pattern": "contextual-tone-and-actionability-missed",
    "severity": "major",
    "id": 468,
    "message": "Cool cool \u2014 so we basically faceplanted for an hour \ud83d\ude2c. It\u2019s fixed now; please don\u2019t promise 100% uptime or any magic \"won\u2019t happen again\" stuff.",
    "ground_truth_flags": [
      "Vagueness",
      "Rudeness"
    ],
    "predicted_flags": []
  },
  {
    "what_went_wrong": "Flagged the message for \u201cVagueness\u201d even though, given the #general context (casual welcome chatter) and the message\u2019s stated intent (seeking quick guidance to avoid breaking prod), the ambiguity is not sufficiently problematic to warrant a flag; additionally, it failed to consider that the more relevant risk\u2014interruptive/synchronous escalation and tagging individuals\u2014was out of scope for the expected flags (which were none).",
    "why": "The model latched onto generic lexical cues (\u201cthis thing,\u201d \u201cwhich flags\u201d) and applied a decontextualized rule that any imprecise noun phrases imply actionable vagueness. It did not properly weight the channel and conversation context: #general is lightweight/social, and the message is a direct ask to specific people rather than a broad, task-execution request where missing details would block action. In other words, it treated stylistic informality as operational ambiguity, and it ignored that the main potential issue is norms/interruptiveness (sync ask + tagging), not vagueness per se\u2014hence a false positive under the \u201cVagueness\u201d taxonomy.",
    "failure_pattern": "ignoring-channel-context",
    "severity": "minor",
    "id": 469,
    "message": "Hey @Maya @Ethan \u2014 sorry to spam \ud83d\ude05 can one of you just hop on for 5 mins and tell me how we deploy this thing + which flags I need to flip? I\u2019m kinda lost and want to make sure I don\u2019t break prod.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Vagueness"
    ]
  },
  {
    "what_went_wrong": "Missed a required flag: the message is vague/underspecified relative to the channel\u2019s expectations and the high-risk deploy context, but the model predicted no flags.",
    "why": "The model treated the request as \u201cspecific\u201d because it names a topic (deploy + flags) and a short time ask (\u201c5 mins\u201d), but it ignored that it lacks the concrete, actionable context NimbusPay explicitly requires in #engineering during a tight change window: no service/environment/region, no link to docs consulted, no current state/error, no what step they\u2019re blocked on, no proposed plan, and no rollback considerations. In this org/channel context, \u201ctell me how we deploy this thing + which flags I need to flip\u201d is imprecise and synchronous, creating ambiguity and potential risk during a rollout. The reasoning focused on politeness and absence of hostility instead of evaluating informational completeness against the stated norms and operational constraints.",
    "failure_pattern": "ignoring-channel-context",
    "severity": "major",
    "id": 470,
    "message": "Hey @Maya @Ethan \u2014 sorry to spam \ud83d\ude05 can one of you just hop on for 5 mins and tell me how we deploy this thing + which flags I need to flip? I\u2019m kinda lost and want to make sure I don\u2019t break prod.",
    "ground_truth_flags": [
      "Vagueness"
    ],
    "predicted_flags": []
  },
  {
    "what_went_wrong": "Flagged the message as Passive-Aggressive when it should not have been flagged (expected no flags).",
    "why": "The model treated blunt, high-urgency delivery (\u201chard yes/no tonight\u201d) and a common workplace idiom (\u201cstop bikeshedding\u201d) as inherently passive-aggressive. In this context, the message is direct and pressuring, but not passive: it does not contain indirect barbs, sarcasm, veiled blame, or deniable hostility typical of passive-aggression. It also appears in a #general context where a direct request for a decision is plausible, and the model conflated \u2018dismissive/frustrated tone\u2019 with a specific policy category (\u2018Passive-Aggressive\u2019) instead of leaving it unflagged (or, in a different taxonomy, possibly flagging for channel/after-hours pressure\u2014none of which were expected here).",
    "failure_pattern": "overflagging-direct-communication",
    "severity": "minor",
    "id": 471,
    "message": "Can we just ship Friday and stop bikeshedding? @Marta @Ravi @Neha I need a hard yes/no tonight so I can tell the CEO.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "The model missed two expected flags: it failed to flag Vagueness (no decision criteria, unclear what \u201chard yes/no\u201d is based on) and Rudeness (imperative/dismissive phrasing: \u201cjust ship\u201d + \u201cstop bikeshedding\u201d), and only flagged Passive-Aggressive.",
    "why": "The model over-indexed on the single obvious cue (\u201cstop bikeshedding\u201d) and mapped it narrowly to Passive-Aggressive, treating it as \u2018veiled reprimand\u2019 rather than recognizing it as plain dismissiveness/condescension (Rudeness). It also didn\u2019t incorporate the immediate context where the team explicitly asked for documented go/no-go criteria and blockers; the target message ignores that process and demands a binary commitment \u201ctonight,\u201d which creates ambiguity about standards and risk acceptance\u2014i.e., Vagueness. In short, it underused conversation norms and prior requests (criteria, async thread discipline) and collapsed multiple tone/clarity issues into one label.",
    "failure_pattern": "context-not-considered-missed-multi-flag",
    "severity": "major",
    "id": 472,
    "message": "Can we just ship Friday and stop bikeshedding? @Marta @Ravi @Neha I need a hard yes/no tonight so I can tell the CEO.",
    "ground_truth_flags": [
      "Vagueness",
      "Passive-Aggressive",
      "Rudeness"
    ],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "The model over-flagged: it marked the message as Non-Objective and Passive-Aggressive even though, given the #random channel\u2019s joking stress-bantery context, it\u2019s primarily casual sarcasm/venting rather than a report, directive, or professional critique that needs objectivity or directness enforcement.",
    "why": "It failed to weigh channel and conversational intent. In #random, participants are already joking about stress, and the target message continues that meme-like tone (\u201cCool cool,\u201d \u201cLove that for us.\u201d). The model treated the line as if it were a serious cross-functional status claim in a work-update channel, applying \u2018objectivity\u2019 norms and inferring blame attribution. It also assumed sarcasm automatically equals passive-aggression; here it\u2019s more ambient gallows humor/commiseration rather than an indirect attack on a specific person in-thread. In short, it applied the wrong norm set (high-stakes #client-updates expectations) to a low-stakes #random vent.",
    "failure_pattern": "ignoring-channel-context",
    "severity": "major",
    "id": 473,
    "message": "Cool cool\u2014so we\u2019re shipping the \"half-baked\" version again because eng slipped? Love that for us.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Non-Objective",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed the expected \"Non-Objective\" flag (false negative). The other two flags (\"Passive-Aggressive\", \"Rudeness\") were correctly predicted.",
    "why": "The model treated the message primarily as a tone issue (sarcasm + blame) and didn\u2019t also tag it for content quality. The statement makes a contested causal claim and evaluative judgment\u2014\"shipping the 'half-baked' version again because eng slipped\"\u2014without evidence or specifics, and uses loaded language (\u201chalf-baked,\u201d \u201cagain,\u201d implied blame) rather than objective, actionable facts. In this channel context (#client-updates with exec/sales visibility and an explicit request for \u201ccrisp alignment\u201d), the lack of objective framing is part of the violation, but the model focused on interpersonal disrespect and missed that it\u2019s also non-factual/subjective editorializing.",
    "failure_pattern": "missed-non-objective-subjective-framing",
    "severity": "major",
    "id": 474,
    "message": "Cool cool\u2014so we\u2019re shipping the \"half-baked\" version again because eng slipped? Love that for us.",
    "ground_truth_flags": [
      "Rudeness",
      "Non-Objective",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly (overflagging): it labeled the message as Non-Objective, Passive-Aggressive, and Rudeness even though the expected result was no flags in this startup\u2019s blunt, emoji-heavy norm and the message is primarily an urgent coordination request.",
    "why": "The model treated culturally normal, high-tempo startup phrasing (\u201cdumping the mess,\u201d \u201ckthx,\u201d \ud83d\ude43) as inherently disrespectful/sarcastic, without weighting the strong context that this team communicates bluntly during incidents. It also over-interpreted a real (but common) incident-friction framing (\u201cstop overpromising\u2026\u201d) as a factual, ungrounded accusation (Non-Objective) rather than a normative request to align on externally safe messaging. In other words, it applied a generic politeness/objectivity standard and ignored situational factors (incident pressure, DM between peers, established channel style) that reduce the likelihood that these cues are meant as hostility or indirect aggression.",
    "failure_pattern": "cultural-style-misclassified",
    "severity": "major",
    "id": 475,
    "message": "Can we please stop overpromising and then dumping the mess on eng? \ud83d\ude43 Just tell them we fixed the blips and we\u2019re not promising 100% until Monday. kthx.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Non-Objective",
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as Passive-Aggressive when it should have been unflagged in this context.",
    "why": "The model over-weighted generic passive-aggression cues (\"Per my last message,\" \"stop debating,\" and the \ud83d\ude43 emoji) without accounting for the channel and topic. In #random, the conversation is a playful snack/soup debate, and the target line reads as jokingly applying #engineering-style urgency (\u201cship it\u201d) to a non-work thread. The emoji functions as a self-aware, humorous marker rather than a veiled jab. The model also imported tension from the separate #engineering scenario even though the actual channel context here is low-stakes banter, leading it to interpret a meme-like phrase as interpersonal criticism.",
    "failure_pattern": "ignoring-channel-context",
    "severity": "minor",
    "id": 477,
    "message": "Per my last message, can we please stop debating and just ship it already? \ud83d\ude43",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed the Rudeness flag (under-flagged). The message is both passive-aggressive and overtly rude/dismissive (\u201cstop debating and just ship it already\u201d).",
    "why": "The model correctly detected passive-aggression markers (\u201cPer my last message,\u201d \ud83d\ude43) but treated the core imperative as merely a discussion-stopper rather than as a disrespectful command. It underweighted the direct contempt/dismissal embedded in \u201cstop debating,\u201d especially in the #engineering context where public pressure and avoiding blame/escalation are sensitive. It also likely over-associated rudeness with explicit insults/profanity, missing that curt, belittling directives can constitute rudeness even without name-calling.",
    "failure_pattern": "missing-rudeness-in-passive-aggressive-directive",
    "severity": "major",
    "id": 478,
    "message": "Per my last message, can we please stop debating and just ship it already? \ud83d\ude43",
    "ground_truth_flags": [
      "Rudeness",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "The model missed all expected flags: the message is rude/insulting (\u201cworks on my machine\u201d special), non-objective/hyperbolic (\u201cblow up,\u201d \u201clight the client on fire\u201d), and passive-aggressive/sarcastic (mocking Leo while implying repeated incompetence). It should have been flagged for Rudeness, Non-Objective, and Passive-Aggressive but returned none.",
    "why": "The model likely over-weighted the presence of a real technical issue (payment regression) and treated the message as an urgent escalation rather than evaluating tone. It also appears to have ignored key linguistic cues: sarcasm/quotation marks, derisive framing (\u201cspecial again\u201d), and inflammatory metaphor (\u201clight the client on fire\u201d). Additionally, it didn\u2019t integrate channel/org context: leadership asked for blameless, specific updates due to exec/CSM visibility and client sensitivity, making this tone especially inappropriate and escalation-triggering.",
    "failure_pattern": "missing-obvious-rudeness-and-sarcasm",
    "severity": "major",
    "id": 480,
    "message": "Leo, can we not ship your \u201cworks on my machine\u201d special again? I\u2019m seeing the payment flow blow up on RC-17 \u2014 pls take a look before we light the client on fire tomorrow.",
    "ground_truth_flags": [
      "Rudeness",
      "Non-Objective",
      "Passive-Aggressive"
    ],
    "predicted_flags": []
  },
  {
    "what_went_wrong": "Flagged the message as Rudeness even though it fell within acceptable directness for a high-pressure incident update; expected no flags.",
    "why": "The model treated blunt, deadline-driven phrasing (\"stop moving the goalposts,\" \"not vibes\") as inherently disrespectful instead of recognizing it as frustrated but still task-focused escalation in an incident context. It overweighted isolated charged phrases and underweighted situational norms: an active regression, leadership visibility, support escalations, and a prior request to keep updates tight/async. It also failed to distinguish 'impatient urgency + colloquial tone' from a personal insult or harassment; the message criticizes process/scope drift and asks for an ETA, not a person\u2019s competence or intent.",
    "failure_pattern": "overflagging-urgent-blunt-incident-comms",
    "severity": "minor",
    "id": 481,
    "message": "Cool cool \u2014 can we please stop moving the goalposts and just ship what\u2019s in the doc? I need an actual ETA, not vibes.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Missed the expected \"Non-Objective\" flag (under-flagging).",
    "why": "The model correctly identified rudeness and passive-aggressive tone but failed to map subjective, blame-inferring language to the \"Non-Objective\" category. Phrases like \"stop moving the goalposts\" assert a disputed motive/pattern (requirements churn) without evidence in the message, and \"not vibes\" dismisses prior updates as unserious rather than requesting measurable specifics. In a high-visibility cross-functional incident thread, that speculation/character judgment is a key non-objective element the model didn\u2019t classify, likely because it treated the content primarily as tone (rude/PA) and didn\u2019t separately score the claim\u2019s objectivity.",
    "failure_pattern": "missing-non-objective-subjective-accusation",
    "severity": "major",
    "id": 482,
    "message": "Cool cool \u2014 can we please stop moving the goalposts and just ship what\u2019s in the doc? I need an actual ETA, not vibes.",
    "ground_truth_flags": [
      "Rudeness",
      "Non-Objective",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Passive-Aggressive",
      "Rudeness"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: the model marked the DM as \"Passive-Aggressive\" and \"Non-Objective\" even though the expected outcome was no flags.",
    "why": "The model treated rhetorical/snarky phrasing (\"Love how\u2026 again \ud83d\ude43\", \"stop bikeshedding\") and an attribution frame (\"engineering delay\") as policy-relevant violations rather than as acceptable venting/pressure in a private DM between peers. It also failed to weight the immediate context: the PM is under CEO pressure, and the conversation is already about negotiating a risky deadline\u2014so the message reads as emotional emphasis and urgency, not an indirect workplace attack requiring moderation. In other words, the model applied a strict tone-policing heuristic (sarcasm => passive-aggressive; blame framing => non-objective) without considering that (a) this is a DM, (b) no explicit insult/threat is present, and (c) the \"non-objective\" claim is more an opinion/complaint than a factual allegation being presented as evidence.",
    "failure_pattern": "overflagging-sarcasm-in-private-context",
    "severity": "major",
    "id": 483,
    "message": "Love how this is somehow an \"engineering delay\" again \ud83d\ude43 Can we please just commit to Friday and stop bikeshedding? @Maya @Rahul @Isha @Ben",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Non-Objective",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed the expected 'Rudeness' flag. The message contains overtly rude/dismissive phrasing ('stop bikeshedding') and accusatory framing ('engineering delay again') in addition to being passive-aggressive and non-objective.",
    "why": "The model correctly identified sarcasm (passive-aggressive) and unsupported blame (non-objective) but likely treated rudeness as redundant with passive-aggression rather than a separate dimension. It also appears to underweight explicit disrespect markers (imperative + pejorative term 'bikeshedding', public call-out with multiple @mentions) and the channel norms/context that amplify the discourtesy (public, client-updates channel; prior reminder to avoid @mentions outside working hours).",
    "failure_pattern": "rudeness-under-detected-when-overlapping-with-passive-aggression",
    "severity": "major",
    "id": 484,
    "message": "Love how this is somehow an \"engineering delay\" again \ud83d\ude43 Can we please just commit to Friday and stop bikeshedding? @Maya @Rahul @Isha @Ben",
    "ground_truth_flags": [
      "Rudeness",
      "Non-Objective",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Non-Objective",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as Passive-Aggressive when it should not have been flagged (false positive).",
    "why": "The model treated casual/sarcastic markers (\"lol\", \"again\", and the \ud83d\ude43 emoji) as evidence of passive-aggression without weighing the channel and conversational context. In #random, the preceding thread is explicitly playful and meme-y about Friday deploys, feature flags, and rewrites; the target message matches that established banter and is still decision-oriented (advocating patch + flags) rather than a veiled personal jab. The model also failed to distinguish mild humor/eye-roll about a recurring technical pattern from passive-aggressive interpersonal behavior directed at a person or group. In this context, the tone is informal but not disrespectful or indirect in a way that undermines collaboration.",
    "failure_pattern": "ignoring-channel-context",
    "severity": "minor",
    "id": 485,
    "message": "lol can we not do the whole \u201clet\u2019s rewrite it\u201d thing *again* \u2014 ship the tiny patch + flags and move on \ud83d\ude43",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed the expected \"Non-Objective\" flag (only flagged \"Passive-Aggressive\").",
    "why": "The model correctly detected the sarcastic/dismissive tone as passive-aggressive, but failed to also classify the message as non-objective because it focused on interpersonal cues and the user\u2019s underlying proposal (patch + flags) rather than the message\u2019s form. In this channel/context, Elena explicitly asked for decision-oriented specifics (scope/owners/timeline/risk). The target message provides no data, risk assessment, or actionable details; instead it uses ridicule (\u201clol\u201d, \u201cagain\u201d, \u201crewrite it\u201d), a vague imperative (\u201cmove on\u201d), and an emoji\u2014signals of low-signal commentary rather than objective, decision-supporting content. The model likely treated \u201cadvocating for a small patch\u201d as inherently objective, ignoring that the phrasing is opinionated and non-evidentiary relative to the requested format.",
    "failure_pattern": "non-objective-flag-missed-due-to-content-intent-focus",
    "severity": "major",
    "id": 486,
    "message": "lol can we not do the whole \u201clet\u2019s rewrite it\u201d thing *again* \u2014 ship the tiny patch + flags and move on \ud83d\ude43",
    "ground_truth_flags": [
      "Non-Objective",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: the model marked the message as Passive-Aggressive and Vagueness even though it fits the informal, joking #general context and functions as a lightweight check-in rather than a hostile or unclear ask.",
    "why": "The model over-weighted surface cues (\"Cool, so\u2026?\" + thumbs-up) as sarcasm and treated the reference to \"after standup\" as inherently vague, without integrating the channel\u2019s established banter about standups/huddles and the common startup norm of shorthand references to recent syncs. In this context, the line reads as a concise prompt for confirmation (and a nudge to document decisions), not a veiled complaint. It\u2019s also sufficiently specific for the intended audience because the immediate shared context is the standup/decision-making pattern being discussed; demanding explicit details is an over-application of the vagueness heuristic.",
    "failure_pattern": "context-not-considered-overflagging-informal-banter",
    "severity": "minor",
    "id": 487,
    "message": "Cool, so we\u2019re just going with whatever we decided after standup then? \ud83d\udc4d",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Vagueness",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "The model missed the expected \"Passive-Aggressive\" flag and only flagged \"Vagueness.\"",
    "why": "While the message is indeed vague (it relies on an unspecified standup decision), it also carries a dismissive/irritated edge: \"Cool, so we\u2019re just going with whatever\u2026 then?\" uses rhetorical framing that signals frustration and challenges the legitimacy of the prior decision without directly stating the concern. The thumbs-up emoji in this context functions as a sarcastic softener rather than a neutral acknowledgment, especially given the channel reminder to be specific and decision-oriented and the recurring pattern of people invoking \"what we decided after standup\" without clarity. The model likely treated the message as a straightforward request for confirmation (benign intent) and over-indexed on the literal ambiguity, under-weighting pragmatic cues (rhetorical question + \"Cool\" opener + \"whatever\" + emoji) that commonly indicate passive-aggressive tone under time pressure.",
    "failure_pattern": "missing-subtle-passive-aggression",
    "severity": "major",
    "id": 488,
    "message": "Cool, so we\u2019re just going with whatever we decided after standup then? \ud83d\udc4d",
    "ground_truth_flags": [
      "Vagueness",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Vagueness"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: the model labeled the message as Passive-Aggressive even though it was expected to be unflagged in this DM context.",
    "why": "The model over-weighted lexical cues of sarcasm (\"vibe-shipping\", \"love that for us\") and treated them as inherently passive-aggressive without factoring in the channel and norms implied by the thread. In a private DM between peers already commiserating (shared stress, light humor, emojis earlier), this reads more like informal venting + a clear operational request (freeze changes) than indirect hostility aimed at a person. It also failed to distinguish \"sarcastic humor under pressure\" from \"veiled interpersonal dig\"\u2014there\u2019s no named target, no insinuation about someone\u2019s competence, and the second sentence is a direct ask rather than an oblique jab. In short: legitimate style (dark humor) was misclassified as a policy-relevant interpersonal risk signal.",
    "failure_pattern": "cultural-style-misclassified",
    "severity": "minor",
    "id": 489,
    "message": "Cool, so we\u2019re just vibe-shipping again \u2014 love that for us. Can we please not change anything else today?",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed two expected flags: it only flagged Passive-Aggressive, but failed to flag Vagueness and Non-Objective.",
    "why": "The model treated the second sentence as a sufficiently specific request and assumed there were no other issues once Passive-Aggressive was identified. In this org/channel context, the message is non-objective because it relies on snarky, value-laden framing (\u201cvibe-shipping,\u201d \u201clove that for us\u201d) rather than verifiable status, owners, or ticket-linked reasoning expected for the VP update. It\u2019s also vague because it doesn\u2019t state what concrete action is being requested beyond an ambiguous freeze (\u201cnot change anything else today\u201d)\u2014no scope (code? copy? config? comms?), no owner, no ticket, and no criteria for what counts as a \u2018change,\u2019 which is especially problematic amid feature-flag rollback + compliance constraints. The model likely used a narrow definition of vagueness (only missing nouns/ETAs) and ignored the process-heavy requirement (Jira references, metrics, mitigation plan) that makes this message low-signal and non-actionable.",
    "failure_pattern": "contextual-vagueness-and-nonobjectivity-missed",
    "severity": "major",
    "id": 490,
    "message": "Cool, so we\u2019re just vibe-shipping again \u2014 love that for us. Can we please not change anything else today?",
    "ground_truth_flags": [
      "Vagueness",
      "Non-Objective",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as Passive-Aggressive when it should have had no flags.",
    "why": "The model over-indexed on the phrase \u201cPer my last message...\u201d and the ellipsis/\ud83d\ude05 as universal markers of passive-aggression, without properly weighing the channel norms and surrounding context. In #random, the team is explicitly riffing on \u201cper my last message\u201d as a running joke (hoodies, chaotic client requests), so Maya\u2019s opener reads as cultural banter and a light, socially softened complaint rather than a veiled dig. The message is also fairly direct about the issue (not invited to the call before being tagged for sign-off) and doesn\u2019t include hallmarks of passive-aggression like plausible deniability, backhandedness, or indirect insinuation\u2014it's a straightforward request/feedback wrapped in the channel\u2019s humor style.",
    "failure_pattern": "ignoring-channel-context",
    "severity": "minor",
    "id": 491,
    "message": "Per my last message... would\u2019ve been cool to be invited to the call before we tagged me for sign-off \ud83d\ude05",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as Passive-Aggressive when it should not have been flagged (expected no flags).",
    "why": "The model over-interpreted a common, light gallows-humor joke in a high-stress deadline context as indirect hostility. It treated sarcasm ('I\u2019ll bring popcorn') as a veiled attack, but in #general the thread is already using casual stress-humor (trail mix, vibes) and the message doesn\u2019t target an individual, assign blame, or weaponize politeness; it\u2019s a humorous way to acknowledge risk. The model also didn\u2019t weigh channel norms and audience: #general is more social/banter-friendly than a PR review thread, so this tone is less likely to be passive-aggressive critique.",
    "failure_pattern": "overflagging-gallows-humor",
    "severity": "minor",
    "id": 493,
    "message": "Cool, if we ship this as-is and it blows up tomorrow, I\u2019ll bring popcorn to the penalty call \ud83c\udf7f",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed two expected flags: Rudeness and Non-Objective. It only flagged Passive-Aggressive even though the message includes overtly rude phrasing and a speculative, non-factual claim about outcomes (\u201cblows up tomorrow\u201d).",
    "why": "The model correctly detected sarcasm (passive-aggression) but treated the rest as merely \u201cflippant tone\u201d instead of labeling it as explicit disrespect. The phrase \u201cI\u2019ll bring popcorn to the penalty call\u201d communicates ridicule and dismissiveness toward colleagues and the client situation (rudeness), not just indirect critique. It also makes a non-objective prediction/doom framing (\u201cif we ship\u2026 and it blows up tomorrow\u201d) without concrete evidence or actionable specifics, which fits a Non-Objective flag. The model likely over-weighted the single most salient category (passive-aggressive sarcasm) and under-applied multi-label classification, and it didn\u2019t fully use channel context (#client-updates expects concise, professional status updates\u2014this message violates that norm strongly).",
    "failure_pattern": "underflagging-multi-label-tone-violations",
    "severity": "major",
    "id": 494,
    "message": "Cool, if we ship this as-is and it blows up tomorrow, I\u2019ll bring popcorn to the penalty call \ud83c\udf7f",
    "ground_truth_flags": [
      "Rudeness",
      "Non-Objective",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged the message as \"Vagueness\" when no flags were expected; the content is a joking aside in #random, not an actionable plan needing clarity.",
    "why": "The model appears to have applied a workplace-execution heuristic (\"no concrete plan/next steps\") without weighting channel and conversational context. In #random, playful hyperbole like \"YOLO\" and \"pray\" is normative banter, and the message isn\u2019t actually requesting action or introducing ambiguity that blocks work. It also seems to have over-literalized the phrasing as if it were a real deployment decision statement (from the scenario) rather than a joke within a lunch/architecture pun thread.",
    "failure_pattern": "ignoring-channel-context",
    "severity": "minor",
    "id": 495,
    "message": "Cool cool\u2014let\u2019s just YOLO it and pray the demo gods are in a good mood \ud83d\ude43",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Vagueness"
    ]
  },
  {
    "what_went_wrong": "Missed the expected \"Non-Objective\" flag (under-flagging). The predicted flags were otherwise correct.",
    "why": "The model correctly identified vagueness and passive-aggressive tone, but it implicitly treated \"Non-Objective\" as redundant with \"Vagueness\" rather than a distinct issue. In this context, the message is not just unspecific; it\u2019s explicitly non-actionable and non-evaluable\u2014framed as sarcasm/deflection (\"YOLO\"/\"pray\") rather than a concrete recommendation that can be assessed against risk, rollback, test coverage, or ownership. The high-stakes, action-oriented thread (ETA, risk assessment, CEO visibility) makes the lack of objective content salient, but the model\u2019s rationale focused on missing details instead of recognizing that the core stance is sentiment/performative resignation, not an objective proposal.",
    "failure_pattern": "non-objective-vs-vague-conflation",
    "severity": "minor",
    "id": 496,
    "message": "Cool cool\u2014let\u2019s just YOLO it and pray the demo gods are in a good mood \ud83d\ude43",
    "ground_truth_flags": [
      "Vagueness",
      "Non-Objective",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Vagueness",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged incorrectly: labeled the message as both \u201cVagueness\u201d and \u201cPassive-Aggressive\u201d when no flags were expected for this scenario.",
    "why": "The model applied generic lint-style heuristics (\u201cjump on this\u201d = vague; \u201cI don\u2019t have time for the triage dance\u201d = passive-aggressive) without properly weighting situational context: a live, high-stakes client escalation with an explicit deadline. In that setting, shorthand like \u201cthis\u201d is anchored by the immediately referenced incident (FinEdge checkout bug) and is not meaningfully ambiguous to the tagged owner. Similarly, the wording reads as direct urgency and process-bypass pressure rather than veiled hostility; it\u2019s brusque but not clearly passive-aggressive. The model also failed to consider the organization\u2019s norm (\u201cpolite, no-surprises tone\u201d) as a style preference rather than a policy violation category\u2014misclassifying a legitimate, time-sensitive escalation as problematic tone.",
    "failure_pattern": "overflagging-urgent-direct-escalation",
    "severity": "minor",
    "id": 497,
    "message": "Theo, can you please just jump on this and fix it tonight? I really don\u2019t have time for the whole triage dance while the client is literally threatening to walk.",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Vagueness",
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Missed two expected flags (Rudeness and Vagueness) and only flagged Passive-Aggressive, resulting in an incomplete classification.",
    "why": "The model correctly noticed the process-dismissive phrasing, but it underweighted two other violations. (1) Rudeness: the message contains minimizing/imperative language aimed at a specific person in a public cross-functional channel\u2014\u201ccan you please just jump on this and fix it tonight?\u201d plus \u201cI really don\u2019t have time\u2026\u201d\u2014which reads as disrespectful and pressuring given Theo\u2019s stated process and release freeze. (2) Vagueness: despite demanding action, it provides no repro steps, scope, environment, logs, or owner handoff, directly conflicting with prior requests for factual, time-stamped details and routing via #eng-triage. The model appears to treat urgency and frustration as fully captured by Passive-Aggressive, rather than recognizing separate rudeness (tone/command) and vagueness (lack of actionable specifics), and it didn\u2019t fully incorporate the channel/process context that amplifies both issues.",
    "failure_pattern": "underflagging-multi-label-tone-and-specificity",
    "severity": "major",
    "id": 498,
    "message": "Theo, can you please just jump on this and fix it tonight? I really don\u2019t have time for the whole triage dance while the client is literally threatening to walk.",
    "ground_truth_flags": [
      "Rudeness",
      "Vagueness",
      "Passive-Aggressive"
    ],
    "predicted_flags": [
      "Passive-Aggressive"
    ]
  },
  {
    "what_went_wrong": "Flagged the message for Vagueness even though, in this light #general banter context, the deictic phrasing (\u201cthis\u201d, \u201cofficial wording\u201d) is a normal shorthand and not an actionable-clarity failure requiring moderation.",
    "why": "The model over-indexed on lexical cues of vagueness (\u201cthis\u201d, \u201cofficial\u201d) without weighting channel context and conversational topic. In #general, the preceding messages are casual and unrelated to the high-stakes client incident; the target line reads as a generic, self-aware request for guidance rather than a defective status update. The flagging system appears to apply an enterprise-SOP standard globally instead of contextually (channel + thread topic), misclassifying legitimate informal shorthand as a problem.",
    "failure_pattern": "ignoring-channel-context-overflagging-vagueness",
    "severity": "minor",
    "id": 499,
    "message": "Can someone just tell me who owns this + what the *official* wording is? I don\u2019t want to accidentally start a fire with a half-baked take \ud83d\ude05",
    "ground_truth_flags": [],
    "predicted_flags": [
      "Vagueness"
    ]
  },
  {
    "what_went_wrong": "Missed both expected flags. The message is non-objective (emotive/framing language: \u201cjust tell me,\u201d \u201cofficial wording,\u201d \u201cstart a fire,\u201d \u201chalf-baked take,\u201d emoji) and vague (no ticket ID, no specific deliverable/doc link, no named owner/role, unclear whether asking about INC-2487 client comms vs internal exec summary wording).",
    "why": "The model evaluated the text in isolation and treated it as a straightforward request, overlooking that in this org/channel the SOP requires concise, audit-friendly, ticket-tagged updates. It also underweighted tone/subjectivity markers: idioms and hedging (\u201cstart a fire,\u201d \u201chalf-baked\u201d) plus the emoji introduce non-objective, emotional coloration\u2014especially risky with Legal watching wording. Finally, it conflated \u201casking for ownership\u201d with \u201cbeing specific,\u201d even though the request lacks the operational specificity expected here (which artifact, which audience, which ticket, which DRI).",
    "failure_pattern": "formality-and-sop-mismatch-missed",
    "severity": "major",
    "id": 500,
    "message": "Can someone just tell me who owns this + what the *official* wording is? I don\u2019t want to accidentally start a fire with a half-baked take \ud83d\ude05",
    "ground_truth_flags": [
      "Vagueness",
      "Non-Objective"
    ],
    "predicted_flags": []
  }
]