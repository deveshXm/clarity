---
description: Evaluation framework conventions for synthetic dataset generation and prompt optimization
globs: evals/**
alwaysApply: false
---

# Evals Framework

## Overview
Python-based evaluation framework in `evals/` for testing Clarity's communication coaching AI. Two pipelines: **generate** (synthetic data) and **train** (eval loop + prompt optimization). Uses Poetry for dependency management, OpenAI for LLM calls.

## Code Style
- **No types/Pydantic** — plain dicts, lists, and simple functions
- **No unnecessary abstractions** — keep logic flat and readable
- **Single file per pipeline** — `generate.py` and `train.py`
- **Config and prompts are separate** — but logic stays in one file

## Project Structure
```
evals/
├── config/
│   ├── generate.py      # Flags, scenario axes, case type distribution
│   └── train.py         # API URL, concurrency, iterations, judge model
├── prompts/
│   ├── generate.py      # Scenario + message generation prompts
│   └── train.py         # Judge, taxonomy, optimizer prompts
├── data/
│   ├── generate/
│   │   ├── flags.json       # Hardcoded 7 coaching flags (reference)
│   │   ├── scenarios.json   # Generated rich scenarios
│   │   └── dataset.json     # Messages with context + ground truth
│   └── train/
│       └── run_N/
│           ├── results.json   # Per-message predictions
│           ├── metrics.json   # Aggregate scores
│           ├── failures.json  # Judge analysis per failure
│           ├── taxonomy.json  # Clustered error patterns
│           └── prompt.txt     # Prompt used or generated
├── generate.py          # Data generation (--step scenarios|messages|all)
└── train.py             # Training loop (--step evaluate|full)
```

## 7 Coaching Flags (Hardcoded)
Flags are NOT generated — they're hardcoded in `config/generate.py`. All set to `enabled: True` for evals:
- Vagueness, Non-Objective, Circular, Rudeness, Passive-Aggressive, Fake, One-Liner

## Data Generation Pipeline (`generate.py`)

### Step 1: Scenarios (`--step scenarios`)
- Generates ~100 rich story-format scenario descriptions
- Combines random workspace_type + channel + participant_template
- Each scenario: `{ id, scenario, workspace_type, channel, participants }`
- Output: `data/generate/scenarios.json`

### Step 2: Messages (`--step messages`)
- For each case type, picks a random scenario and generates messages with 3-5 context messages
- Four case types with configurable distribution:
  - **true_positive** (40%): Messages that violate flags (passive-aggressive, condescending, dismissive, etc.)
  - **true_negative** (30%): Tricky clean messages (direct-but-not-rude, dry-humor, cultural-idiom, etc.)
  - **context_dependent** (20%): Same message, two contexts, different labels
  - **formality_mismatch** (10%): Same message, casual vs formal channel
- Output: `data/generate/dataset.json`

### Dataset Entry Format
```python
{
    "id": 1,
    "scenario": "A 15-person remote-first SaaS startup...",
    "channel": "#engineering",
    "case_type": "true_positive",
    "category": "passive-aggressive",
    "message": "Thanks for finally doing that",
    "user": "jordan",
    "context": [{"text": "...", "user": "alex"}],
    "ground_truth_flags": ["Passive-Aggressive"]  # [] for true negatives
}
```

## Training Loop (`train.py`)

### Step 1: Evaluate (20 concurrent)
- Load dataset + flags → POST each to `/api/evaluate`
- First run uses default prompt, subsequent runs use optimized prompt

### Step 2: Score (math metrics)
- Per-message binary pass/fail (exact flag match)
- Aggregate: overall pass rate, per-case-type pass rate, per-flag precision/recall, FP rate on negatives

### Step 3: LLM Judge (failures only)
- Different model from flagger (avoid shared biases)
- Analyzes what went wrong, why, categorizes failure pattern

### Step 4: Error Taxonomy
- Clusters all judge analyses into higher-level patterns with counts
- Ranks by frequency, suggests fix directions

### Step 5: Prompt Optimizer
- Feeds current prompt + taxonomy + failures → LLM returns revised prompt

### Step 6: Loop
- Re-run with new prompt until max iterations or pass rate plateaus

## Config Pattern (`config/*.py`)
- `SEED` for reproducibility
- `MODEL` for generation LLM
- `FLAGS` — hardcoded coaching flags with descriptions
- Scenario axes: `WORKSPACE_TYPES`, `CHANNELS`, `PARTICIPANT_TEMPLATES`
- `CASE_TYPES` with percentage distribution
- `TOTAL_MESSAGES`, `SCENARIOS_COUNT`, `CONTEXT_MESSAGES_PER_ENTRY`
- Training: `API_URL`, `CONCURRENCY`, `MAX_ITERATIONS`, `JUDGE_MODEL`

## Evaluate API
- `POST /api/evaluate`
- Accepts: `message`, `coachingFlags[]`, `context[]`, `includeReason`, `prompt`
- Returns: `{ flagged, flags[], rephrasedMessage, reason? }`
- Context: array of `{ text, user, ts }` — passed through to AI analysis

## Hold-Out Hard Set
- Optional `data/generate/hard_set.json` with hand-curated cases
- Same format as dataset.json
- Scored separately, never used for prompt optimization
